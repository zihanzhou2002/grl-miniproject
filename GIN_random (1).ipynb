{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing Randomness: Enhancing the Expressiveness of Graph Isomorphism Networks\n",
        "\n",
        "This is the main notebook used for the Graph Representation Learning mini-project titled above.\n",
        "\n",
        "# Cloning the PR-MPNN repository"
      ],
      "metadata": {
        "id": "6RnUm-Xd3OVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbWQ0SK8ymme",
        "outputId": "1c447b1b-f603-4f9f-c2ed-927cf0c3923a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PR-MPNN'...\n",
            "remote: Enumerating objects: 5892, done.\u001b[K\n",
            "remote: Counting objects: 100% (1734/1734), done.\u001b[K\n",
            "remote: Compressing objects: 100% (512/512), done.\u001b[K\n",
            "remote: Total 5892 (delta 1209), reused 1629 (delta 1123), pack-reused 4158 (from 1)\u001b[K\n",
            "Receiving objects: 100% (5892/5892), 51.77 MiB | 16.09 MiB/s, done.\n",
            "Resolving deltas: 100% (4043/4043), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/chendiqian/PR-MPNN.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup\n",
        "To set up the environment in google colab we need to first intall conda."
      ],
      "metadata": {
        "id": "xof-EDdZ3YeG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCKtHvBJ_6L4",
        "outputId": "2f4fc835-2e1f-4567-c65b-ebc60f1f260a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/conda-forge/miniforge/releases/download/23.11.0-0/Mambaforge-23.11.0-0-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:07\n",
            "🔁 Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nUJt1c66AEGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152a1627-33ba-45fb-a165-1d590a8cd301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conda 23.11.0\n"
          ]
        }
      ],
      "source": [
        "!conda --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiVleVnbAblS",
        "outputId": "5e3daf6c-af07-4760-c0bd-19a9a2fa42a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waO4fM55tZYZ",
        "outputId": "d8a8d1ec-e0ed-4ca7-bc64-cb33204e4a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - pytorch\n",
            " - nvidia\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 23.11.0\n",
            "    latest version: 24.11.2\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pytorch-cuda=12.1\n",
            "    - pytorch==2.1.2\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |       2_kmp_llvm           6 KB  conda-forge\n",
            "    blas-2.116                 |              mkl          13 KB  conda-forge\n",
            "    blas-devel-3.9.0           |   16_linux64_mkl          12 KB  conda-forge\n",
            "    ca-certificates-2024.12.14 |       hbcca054_0         153 KB  conda-forge\n",
            "    certifi-2024.12.14         |     pyhd8ed1ab_0         158 KB  conda-forge\n",
            "    cuda-cudart-12.1.105       |                0         189 KB  nvidia\n",
            "    cuda-cupti-12.1.105        |                0        15.4 MB  nvidia\n",
            "    cuda-libraries-12.1.0      |                0           2 KB  nvidia\n",
            "    cuda-nvrtc-12.1.105        |                0        19.7 MB  nvidia\n",
            "    cuda-nvtx-12.1.105         |                0          57 KB  nvidia\n",
            "    cuda-opencl-12.6.77        |                0          25 KB  nvidia\n",
            "    cuda-runtime-12.1.0        |                0           1 KB  nvidia\n",
            "    cuda-version-12.6          |                3          16 KB  nvidia\n",
            "    filelock-3.16.1            |     pyhd8ed1ab_1          17 KB  conda-forge\n",
            "    gmp-6.3.0                  |       hac33072_2         449 KB  conda-forge\n",
            "    gmpy2-2.1.5                |  py310he8512ff_3         198 KB  conda-forge\n",
            "    jinja2-3.1.5               |     pyhd8ed1ab_0         110 KB  conda-forge\n",
            "    libblas-3.9.0              |   16_linux64_mkl          13 KB  conda-forge\n",
            "    libcblas-3.9.0             |   16_linux64_mkl          12 KB  conda-forge\n",
            "    libcublas-12.1.0.26        |                0       329.0 MB  nvidia\n",
            "    libcufft-11.0.2.4          |                0       102.9 MB  nvidia\n",
            "    libcufile-1.11.1.6         |                0         899 KB  nvidia\n",
            "    libcurand-10.3.7.77        |                0        39.7 MB  nvidia\n",
            "    libcusolver-11.4.4.55      |                0        98.3 MB  nvidia\n",
            "    libcusparse-12.0.2.55      |                0       163.0 MB  nvidia\n",
            "    libgcc-14.2.0              |       h77fa898_1         829 KB  conda-forge\n",
            "    libgcc-ng-14.2.0           |       h69a702a_1          53 KB  conda-forge\n",
            "    libgfortran-14.2.0         |       h69a702a_1          53 KB  conda-forge\n",
            "    libgfortran-ng-14.2.0      |       h69a702a_1          53 KB  conda-forge\n",
            "    libgfortran5-14.2.0        |       hd5240d6_1         1.4 MB  conda-forge\n",
            "    libgomp-14.2.0             |       h77fa898_1         450 KB  conda-forge\n",
            "    libhwloc-2.9.3             |default_h554bfaf_1009         2.5 MB  conda-forge\n",
            "    liblapack-3.9.0            |   16_linux64_mkl          12 KB  conda-forge\n",
            "    liblapacke-3.9.0           |   16_linux64_mkl          12 KB  conda-forge\n",
            "    libnpp-12.0.2.50           |                0       139.8 MB  nvidia\n",
            "    libnvjitlink-12.1.105      |                0        16.9 MB  nvidia\n",
            "    libnvjpeg-12.1.1.14        |                0         2.9 MB  nvidia\n",
            "    llvm-openmp-15.0.7         |       h0cdce71_0         3.1 MB  conda-forge\n",
            "    markupsafe-3.0.2           |  py310h89163eb_1          23 KB  conda-forge\n",
            "    mkl-2022.1.0               |     h84fe81f_915       199.6 MB  conda-forge\n",
            "    mkl-devel-2022.1.0         |     ha770c72_916          25 KB  conda-forge\n",
            "    mkl-include-2022.1.0       |     h84fe81f_915         745 KB  conda-forge\n",
            "    mpc-1.3.1                  |       h24ddda3_1         114 KB  conda-forge\n",
            "    mpfr-4.2.1                 |       h90cbb55_3         620 KB  conda-forge\n",
            "    mpmath-1.3.0               |     pyhd8ed1ab_1         429 KB  conda-forge\n",
            "    networkx-3.4.2             |     pyh267e887_2         1.2 MB  conda-forge\n",
            "    openssl-3.4.0              |       hb9d3cd8_0         2.8 MB  conda-forge\n",
            "    pytorch-2.1.2              |py3.10_cuda12.1_cudnn8.9.2_0        1.28 GB  pytorch\n",
            "    pytorch-cuda-12.1          |       ha16c6d3_6           7 KB  pytorch\n",
            "    pytorch-mutex-1.0          |             cuda           3 KB  pytorch\n",
            "    pyyaml-6.0.2               |  py310ha75aee5_1         178 KB  conda-forge\n",
            "    sympy-1.13.3               | pypyh2585a3b_103         4.4 MB  conda-forge\n",
            "    tbb-2021.11.0              |       h00ab1b0_1         191 KB  conda-forge\n",
            "    torchtriton-2.1.0          |            py310        90.9 MB  pytorch\n",
            "    typing_extensions-4.12.2   |     pyha770c72_1          39 KB  conda-forge\n",
            "    yaml-0.2.5                 |       h7f98852_2          87 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        2.49 GB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               conda-forge/linux-64::blas-2.116-mkl \n",
            "  blas-devel         conda-forge/linux-64::blas-devel-3.9.0-16_linux64_mkl \n",
            "  cuda-cudart        nvidia/linux-64::cuda-cudart-12.1.105-0 \n",
            "  cuda-cupti         nvidia/linux-64::cuda-cupti-12.1.105-0 \n",
            "  cuda-libraries     nvidia/linux-64::cuda-libraries-12.1.0-0 \n",
            "  cuda-nvrtc         nvidia/linux-64::cuda-nvrtc-12.1.105-0 \n",
            "  cuda-nvtx          nvidia/linux-64::cuda-nvtx-12.1.105-0 \n",
            "  cuda-opencl        nvidia/linux-64::cuda-opencl-12.6.77-0 \n",
            "  cuda-runtime       nvidia/linux-64::cuda-runtime-12.1.0-0 \n",
            "  cuda-version       nvidia/noarch::cuda-version-12.6-3 \n",
            "  filelock           conda-forge/noarch::filelock-3.16.1-pyhd8ed1ab_1 \n",
            "  gmp                conda-forge/linux-64::gmp-6.3.0-hac33072_2 \n",
            "  gmpy2              conda-forge/linux-64::gmpy2-2.1.5-py310he8512ff_3 \n",
            "  jinja2             conda-forge/noarch::jinja2-3.1.5-pyhd8ed1ab_0 \n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-16_linux64_mkl \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-16_linux64_mkl \n",
            "  libcublas          nvidia/linux-64::libcublas-12.1.0.26-0 \n",
            "  libcufft           nvidia/linux-64::libcufft-11.0.2.4-0 \n",
            "  libcufile          nvidia/linux-64::libcufile-1.11.1.6-0 \n",
            "  libcurand          nvidia/linux-64::libcurand-10.3.7.77-0 \n",
            "  libcusolver        nvidia/linux-64::libcusolver-11.4.4.55-0 \n",
            "  libcusparse        nvidia/linux-64::libcusparse-12.0.2.55-0 \n",
            "  libgcc             conda-forge/linux-64::libgcc-14.2.0-h77fa898_1 \n",
            "  libgfortran        conda-forge/linux-64::libgfortran-14.2.0-h69a702a_1 \n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-14.2.0-h69a702a_1 \n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-14.2.0-hd5240d6_1 \n",
            "  libhwloc           conda-forge/linux-64::libhwloc-2.9.3-default_h554bfaf_1009 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-16_linux64_mkl \n",
            "  liblapacke         conda-forge/linux-64::liblapacke-3.9.0-16_linux64_mkl \n",
            "  libnpp             nvidia/linux-64::libnpp-12.0.2.50-0 \n",
            "  libnvjitlink       nvidia/linux-64::libnvjitlink-12.1.105-0 \n",
            "  libnvjpeg          nvidia/linux-64::libnvjpeg-12.1.1.14-0 \n",
            "  llvm-openmp        conda-forge/linux-64::llvm-openmp-15.0.7-h0cdce71_0 \n",
            "  markupsafe         conda-forge/linux-64::markupsafe-3.0.2-py310h89163eb_1 \n",
            "  mkl                conda-forge/linux-64::mkl-2022.1.0-h84fe81f_915 \n",
            "  mkl-devel          conda-forge/linux-64::mkl-devel-2022.1.0-ha770c72_916 \n",
            "  mkl-include        conda-forge/linux-64::mkl-include-2022.1.0-h84fe81f_915 \n",
            "  mpc                conda-forge/linux-64::mpc-1.3.1-h24ddda3_1 \n",
            "  mpfr               conda-forge/linux-64::mpfr-4.2.1-h90cbb55_3 \n",
            "  mpmath             conda-forge/noarch::mpmath-1.3.0-pyhd8ed1ab_1 \n",
            "  networkx           conda-forge/noarch::networkx-3.4.2-pyh267e887_2 \n",
            "  pytorch            pytorch/linux-64::pytorch-2.1.2-py3.10_cuda12.1_cudnn8.9.2_0 \n",
            "  pytorch-cuda       pytorch/linux-64::pytorch-cuda-12.1-ha16c6d3_6 \n",
            "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda \n",
            "  pyyaml             conda-forge/linux-64::pyyaml-6.0.2-py310ha75aee5_1 \n",
            "  sympy              conda-forge/noarch::sympy-1.13.3-pypyh2585a3b_103 \n",
            "  tbb                conda-forge/linux-64::tbb-2021.11.0-h00ab1b0_1 \n",
            "  torchtriton        pytorch/linux-64::torchtriton-2.1.0-py310 \n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.12.2-pyha770c72_1 \n",
            "  yaml               conda-forge/linux-64::yaml-0.2.5-h7f98852_2 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                     2023.11.17-hbcca054_0 --> 2024.12.14-hbcca054_0 \n",
            "  certifi                           2023.11.17-pyhd8ed1ab_0 --> 2024.12.14-pyhd8ed1ab_0 \n",
            "  libgcc-ng                               13.2.0-h807b86a_3 --> 14.2.0-h69a702a_1 \n",
            "  libgomp                                 13.2.0-h807b86a_3 --> 14.2.0-h77fa898_1 \n",
            "  openssl                                  3.2.0-hd590300_1 --> 3.4.0-hb9d3cd8_0 \n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  _openmp_mutex                                   4.5-2_gnu --> 4.5-2_kmp_llvm \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "pytorch-2.1.2        | 1.28 GB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.4.5 | 98.3 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 90.9 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.7.77  | 39.7 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-12.1.105  | 19.7 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnvjitlink-12.1.10 | 16.9 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-12.1.105  | 15.4 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.13.3         | 4.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnvjpeg-12.1.1.14  | 2.9 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.4.0        | 2.8 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.9.3       | 2.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.4.2       | 1.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   0% 0.0007634115325035855/1 [00:00<02:11, 131.31s/it]\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :   1% 0.007646956213854052/1 [00:00<00:12, 13.09s/it]\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :   2% 0.01946436569808182/1 [00:00<00:05,  5.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :   1% 0.014645694773221737/1 [00:00<00:06,  6.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   0% 0.004890605130101094/1 [00:00<00:36, 36.58s/it]  \n",
            "libcublas-12.1.0.26  | 329.0 MB  | :   2% 0.02379580784559553/1 [00:00<00:07,  7.93s/it] \u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :   5% 0.05244831545246677/1 [00:00<00:03,  3.65s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :   5% 0.05399901202645877/1 [00:00<00:03,  3.43s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   1% 0.008099319227655227/1 [00:00<00:33, 33.95s/it]\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :   8% 0.07987101786454263/1 [00:00<00:03,  3.70s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :   9% 0.08586178309797171/1 [00:00<00:03,  3.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :   4% 0.03642990941631092/1 [00:00<00:08,  8.37s/it]\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   1% 0.011761308922633364/1 [00:00<00:30, 31.01s/it]\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  11% 0.1072937202766185/1 [00:00<00:03,  3.68s/it] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  12% 0.12264371882613928/1 [00:00<00:02,  3.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :   5% 0.052436271180713506/1 [00:00<00:07,  7.41s/it]\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   2% 0.015160875903313392/1 [00:00<00:29, 30.42s/it]\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  13% 0.13462053911382696/1 [00:00<00:03,  3.67s/it]\u001b[A\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :   7% 0.06678025115949564/1 [00:00<00:06,  7.26s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  16% 0.15551268266833157/1 [00:00<00:02,  3.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   2% 0.018846722208682266/1 [00:00<00:28, 29.27s/it]\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  17% 0.16539916664626175/1 [00:00<00:02,  3.52s/it]\u001b[A\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :   8% 0.08250163318922044/1 [00:00<00:06,  6.93s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  19% 0.19140022482256194/1 [00:00<00:02,  3.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   2% 0.022401357156902085/1 [00:00<00:28, 28.92s/it]\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  20% 0.19617779417869655/1 [00:00<00:02,  3.43s/it]\u001b[A\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  10% 0.09722558614757296/1 [00:00<00:06,  6.89s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  23% 0.22739956617353446/1 [00:00<00:02,  2.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   3% 0.025860565663558956/1 [00:00<00:28, 29.53s/it]\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  11% 0.11180704923858659/1 [00:00<00:06,  7.06s/it]\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  23% 0.22542228451325297/1 [00:00<00:02,  3.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  26% 0.2614983211798904/1 [00:00<00:02,  3.05s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   3% 0.029260132644238986/1 [00:00<00:29, 30.64s/it]\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  13% 0.1260085393500298/1 [00:00<00:06,  7.23s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  29% 0.29447908421882485/1 [00:00<00:02,  3.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  25% 0.25332440479966584/1 [00:00<00:02,  3.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   3% 0.032600058098942175/1 [00:01<00:29, 30.44s/it]\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  14% 0.14011503621658047/1 [00:01<00:06,  7.19s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  33% 0.3292486344056337/1 [00:01<00:02,  3.09s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  28% 0.2806512236368743/1 [00:01<00:02,  3.80s/it] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   4% 0.036321689319897155/1 [00:01<00:28, 29.29s/it]\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  15% 0.15407904321579222/1 [00:01<00:06,  7.21s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  36% 0.36178220065759953/1 [00:01<00:01,  3.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  31% 0.30721097387514357/1 [00:01<00:02,  3.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   4% 0.03980475443694476/1 [00:01<00:27, 29.13s/it] \n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  17% 0.16828053332723547/1 [00:01<00:05,  7.17s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  40% 0.3954337588769869/1 [00:01<00:01,  3.07s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  33% 0.33367484053854546/1 [00:01<00:02,  3.84s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  25% 0.25132480751093417/1 [00:01<00:03,  4.80s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :   4% 0.04325203463840626/1 [00:01<00:29, 31.12s/it]\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  43% 0.42819092352243704/1 [00:01<00:01,  3.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  36% 0.35985105647734517/1 [00:01<00:02,  3.94s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  27% 0.2723011539490937/1 [00:01<00:03,  4.93s/it] \u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :   5% 0.046496533651546504/1 [00:01<00:30, 31.65s/it]\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  46% 0.4611716865613715/1 [00:01<00:01,  3.08s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  39% 0.3883284782129624/1 [00:01<00:02,  3.81s/it] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  30% 0.2992260165413582/1 [00:01<00:03,  4.51s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :   5% 0.05019430826211074/1 [00:01<00:28, 30.21s/it] \n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  49% 0.49437604799379026/1 [00:01<00:01,  3.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  42% 0.4170935506731818/1 [00:01<00:02,  3.71s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  32% 0.3216112220686479/1 [00:01<00:03,  4.65s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :   5% 0.053713158294744455/1 [00:01<00:28, 29.69s/it]\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  53% 0.5271332126392404/1 [00:01<00:01,  3.10s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  45% 0.44643392458260567/1 [00:01<00:02,  3.62s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  35% 0.3454052866850677/1 [00:01<00:02,  4.51s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :   6% 0.05729164985335501/1 [00:01<00:27, 29.16s/it] \n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  56% 0.5612319676455964/1 [00:01<00:01,  3.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  47% 0.4743360448690185/1 [00:01<00:01,  3.61s/it] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  37% 0.3677122222629613/1 [00:01<00:02,  4.70s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :   6% 0.060738930054816516/1 [00:01<00:28, 30.69s/it]\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  50% 0.502142281580564/1 [00:01<00:01,  3.74s/it] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  59% 0.5941009314877886/1 [00:01<00:01,  3.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  39% 0.3911932070817966/1 [00:01<00:02,  4.57s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :   6% 0.06418621025627802/1 [00:01<00:28, 30.22s/it] \n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  63% 0.6250693089853644/1 [00:01<00:01,  3.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  53% 0.5289896825434354/1 [00:01<00:01,  3.83s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  41% 0.4132653328115018/1 [00:02<00:02,  4.81s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :   7% 0.0675261357109812/1 [00:02<00:28, 30.58s/it] \n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  66% 0.6559258872861979/1 [00:02<00:01,  3.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  56% 0.5552617820571025/1 [00:02<00:01,  3.98s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   7% 0.07081834794490292/1 [00:02<00:29, 32.03s/it]\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  29% 0.28906444420817246/1 [00:02<00:06,  8.44s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  69% 0.6858880720130943/1 [00:02<00:01,  3.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  58% 0.5805750458220956/1 [00:02<00:01,  4.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  46% 0.45513975573842486/1 [00:02<00:02,  5.12s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :   7% 0.07396742051648021/1 [00:02<00:30, 32.95s/it]\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  71% 0.714620465575827/1 [00:02<00:01,  3.56s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  60% 0.6049294738384148/1 [00:02<00:01,  4.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  48% 0.4753334026826232/1 [00:02<00:02,  5.08s/it] \u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :   8% 0.07702106664649455/1 [00:02<00:31, 33.94s/it]\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  75% 0.7465950358440822/1 [00:02<00:00,  3.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  63% 0.6306262719028775/1 [00:02<00:01,  4.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  50% 0.4961532092219905/1 [00:02<00:02,  5.00s/it]\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  32% 0.32435443468577224/1 [00:02<00:05,  8.84s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   8% 0.07999121464014132/1 [00:02<00:31, 34.44s/it]\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  66% 0.6552683506437987/1 [00:02<00:01,  4.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  52% 0.517755715255319/1 [00:02<00:02,  4.89s/it] \u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  34% 0.3370835295013802/1 [00:02<00:05,  8.54s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   8% 0.08306871738054639/1 [00:02<00:31, 33.89s/it]\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  68% 0.6810610322831289/1 [00:02<00:01,  4.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  54% 0.5387320616934786/1 [00:02<00:02,  4.85s/it]\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  35% 0.35071506014346787/1 [00:02<00:05,  8.15s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   9% 0.08643249944564031/1 [00:02<00:29, 32.61s/it]\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  71% 0.7058948781737849/1 [00:02<00:01,  4.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  56% 0.5594735982834498/1 [00:02<00:02,  5.02s/it]\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  36% 0.36325416846929065/1 [00:02<00:05,  8.10s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   9% 0.08983206642632034/1 [00:02<00:28, 31.63s/it]\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  73% 0.7304410733398389/1 [00:02<00:01,  4.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  58% 0.579510705328856/1 [00:02<00:02,  5.16s/it] \u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  38% 0.37569828355022084/1 [00:02<00:05,  8.10s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :   9% 0.09300499560828837/1 [00:02<00:29, 32.34s/it]\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | :  75% 0.7545078506315558/1 [00:02<00:01,  4.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  60% 0.5989999227284893/1 [00:03<00:02,  5.27s/it]\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  39% 0.3880949020087048/1 [00:03<00:04,  8.16s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  93% 0.9258091482217496/1 [00:03<00:00,  3.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  10% 0.09610635495908419/1 [00:03<00:29, 32.77s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  62% 0.6206024287618178/1 [00:03<00:01,  5.07s/it]\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  40% 0.40058651371208126/1 [00:03<00:04,  8.12s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  96% 0.9555477345551617/1 [00:03<00:00,  3.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  10% 0.0991719293942939/1 [00:03<00:29, 33.17s/it] \n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  64% 0.6413439653517891/1 [00:03<00:01,  5.00s/it]\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  41% 0.41388556799704485/1 [00:03<00:04,  7.94s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | :  99% 0.9895346903647755/1 [00:03<00:00,  3.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  10% 0.10265499451134151/1 [00:03<00:28, 31.76s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  67% 0.6663120792091506/1 [00:03<00:01,  4.66s/it]\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  43% 0.42808705810848807/1 [00:03<00:04,  7.65s/it]\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  11% 0.10610227471280301/1 [00:03<00:27, 30.91s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  69% 0.6889320945846286/1 [00:03<00:01,  4.59s/it]\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  44% 0.44148110563834425/1 [00:03<00:04,  7.59s/it]\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  11% 0.10968076627141357/1 [00:03<00:26, 30.06s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  72% 0.7160917670250815/1 [00:03<00:01,  4.28s/it]\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  46% 0.4574874674027468/1 [00:03<00:03,  7.14s/it] \u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  11% 0.11365289190147128/1 [00:03<00:25, 28.43s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  74% 0.7412164207812353/1 [00:03<00:01,  4.19s/it]\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  47% 0.47487123121809205/1 [00:03<00:03,  6.66s/it]\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  12% 0.11774430058348269/1 [00:03<00:23, 27.12s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  77% 0.7664193444867852/1 [00:03<00:00,  4.12s/it]\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  49% 0.4899276605335687/1 [00:03<00:03,  6.87s/it] \u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  12% 0.12144207519404693/1 [00:03<00:24, 27.53s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  79% 0.7938138267754264/1 [00:03<00:00,  3.97s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :  13% 0.12541420082410465/1 [00:03<00:23, 26.82s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  82% 0.8219127386086327/1 [00:03<00:00,  3.84s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :  13% 0.12976803222041416/1 [00:03<00:22, 25.55s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  85% 0.8516553193791574/1 [00:04<00:00,  3.68s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :  13% 0.13422921836348198/1 [00:04<00:21, 24.54s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  88% 0.8821023296942473/1 [00:04<00:00,  3.56s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :  14% 0.13867847620135446/1 [00:04<00:20, 23.91s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  91% 0.9121579902623564/1 [00:04<00:00,  3.49s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :  14% 0.1428653113249288/1 [00:04<00:21, 24.63s/it] \n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  94% 0.9408830616907317/1 [00:04<00:00,  3.56s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :  15% 0.14750542204592715/1 [00:04<00:20, 23.65s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  97% 0.9690602434733341/1 [00:04<00:00,  3.62s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :  15% 0.15178768361106446/1 [00:04<00:19, 23.56s/it]\n",
            "pytorch-2.1.2        | 1.28 GB   | :  16% 0.15666636043597018/1 [00:04<00:19, 22.58s/it]\n",
            "pytorch-2.1.2        | 1.28 GB   | :  16% 0.16319114337783677/1 [00:04<00:16, 19.80s/it]\n",
            "pytorch-2.1.2        | 1.28 GB   | :  17% 0.1692387941118886/1 [00:04<00:15, 18.70s/it] \n",
            "pytorch-2.1.2        | 1.28 GB   | :  18% 0.1753460863719173/1 [00:04<00:14, 17.94s/it]\n",
            "pytorch-2.1.2        | 1.28 GB   | :  18% 0.18092853320334976/1 [00:04<00:14, 18.08s/it]\n",
            "pytorch-2.1.2        | 1.28 GB   | :  19% 0.18702389715818307/1 [00:05<00:14, 17.56s/it]\n",
            "pytorch-2.1.2        | 1.28 GB   | :  19% 0.19372760467798017/1 [00:05<00:13, 16.69s/it]\n",
            "pytorch-2.1.2        | 1.28 GB   | :  20% 0.20025238761984676/1 [00:05<00:13, 16.26s/it]\n",
            "pytorch-2.1.2        | 1.28 GB   | :  21% 0.20641932140585229/1 [00:05<00:13, 17.04s/it]\n",
            "pytorch-2.1.2        | 1.28 GB   | :  21% 0.21290831943213276/1 [00:05<00:13, 16.53s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :   0% 0.0001518791001574902/1 [00:05<10:02:19, 36144.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  87% 0.8668608562673933/1 [00:05<00:00,  4.38s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  22% 0.2189917550817707/1 [00:05<00:13, 17.75s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  11% 0.10920107301323546/1 [00:05<00:27, 31.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :  22% 0.22470541327035223/1 [00:05<00:15, 20.07s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  16% 0.16357379086961696/1 [00:05<00:14, 17.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :  23% 0.22985844111475143/1 [00:05<00:16, 20.98s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  22% 0.22189536533009319/1 [00:05<00:08, 11.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :  23% 0.23474904624485252/1 [00:05<00:16, 21.67s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  28% 0.2756605667858447/1 [00:05<00:05,  7.86s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  95% 0.9474626245587617/1 [00:06<00:00,  5.66s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  24% 0.23944879849182774/1 [00:06<00:17, 22.55s/it]\n",
            "libcublas-12.1.0.26  | 329.0 MB  | :  97% 0.9654638444659089/1 [00:06<00:00,  5.79s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  24% 0.2439338412452863/1 [00:06<00:17, 23.72s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.4.5 | 98.3 MB   | :   0% 0.00015896474520165442/1 [00:06<10:59:12, 39559.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | :  25% 0.24819224620003286/1 [00:06<00:18, 23.95s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  44% 0.4422719396586115/1 [00:06<00:02,  4.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  25% 0.25239100962880257/1 [00:06<00:20, 27.17s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  49% 0.488898823406961/1 [00:06<00:01,  3.61s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  26% 0.2562796371224927/1 [00:06<00:19, 26.80s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  54% 0.5434234203635/1 [00:06<00:01,  3.05s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.4.5 | 98.3 MB   | :  13% 0.13400728020499467/1 [00:06<00:20, 23.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  26% 0.26009669478501063/1 [00:06<00:20, 27.79s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.4.5 | 98.3 MB   | :  18% 0.17788154988065127/1 [00:06<00:12, 15.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  65% 0.6473087248712233/1 [00:06<00:00,  2.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  26% 0.2637586844799888/1 [00:06<00:20, 28.25s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  27% 0.26733717603859936/1 [00:06<00:21, 29.41s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  70% 0.6963656742220926/1 [00:06<00:00,  2.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  27% 0.27093952420760065/1 [00:07<00:21, 28.95s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  74% 0.7446632280721744/1 [00:07<00:00,  2.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  27% 0.2744345176298436/1 [00:07<00:20, 28.90s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  79% 0.7946314520239888/1 [00:07<00:00,  2.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  28% 0.2782635035975569/1 [00:07<00:20, 28.06s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  85% 0.8456628296769055/1 [00:07<00:00,  2.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  28% 0.28267697651984325/1 [00:07<00:18, 26.26s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  90% 0.8986686356318696/1 [00:07<00:00,  2.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  29% 0.286553675708338/1 [00:07<00:18, 26.12s/it]  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | :  95% 0.9484849804835264/1 [00:07<00:00,  2.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  29% 0.2906450843903494/1 [00:07<00:18, 25.63s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  30% 0.29601282172826526/1 [00:07<00:16, 23.06s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  30% 0.30158334025450234/1 [00:07<00:14, 21.27s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 90.9 MB   | :   0% 0.00017190793687843579/1 [00:07<12:29:32, 44980.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.4.5 | 98.3 MB   | :  74% 0.7422063953465244/1 [00:07<00:00,  1.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  31% 0.3063069491118683/1 [00:07<00:17, 24.56s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 90.9 MB   | :   8% 0.07529567635275487/1 [00:07<00:56, 61.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  31% 0.3105414974562241/1 [00:08<00:18, 26.59s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 90.9 MB   | :  13% 0.1266961494794072/1 [00:08<00:25, 29.69s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  31% 0.314453981560305/1 [00:08<00:18, 27.48s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 90.9 MB   | :  17% 0.1689855019515024/1 [00:08<00:15, 19.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.4.5 | 98.3 MB   | :  90% 0.8989456341153557/1 [00:08<00:00,  2.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  32% 0.31818754108645536/1 [00:08<00:19, 28.86s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.4.5 | 98.3 MB   | :  95% 0.946635057675852/1 [00:08<00:00,  2.25s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  32% 0.3217302477294798/1 [00:08<00:20, 29.96s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.4.5 | 98.3 MB   | :  99% 0.9925758690391301/1 [00:08<00:00,  2.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  33% 0.32533259589848107/1 [00:08<00:19, 29.38s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  33% 0.3294597894960786/1 [00:08<00:18, 27.78s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  33% 0.33341998682094093/1 [00:08<00:18, 27.02s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  34% 0.3375471804185385/1 [00:08<00:17, 26.17s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  34% 0.3416505174057452/1 [00:08<00:16, 25.64s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  35% 0.34558685812021683/1 [00:08<00:17, 26.01s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  35% 0.35009575748406613/1 [00:09<00:16, 24.76s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  35% 0.3541633095556868/1 [00:09<00:16, 25.27s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  36% 0.3581473634909399/1 [00:09<00:16, 25.80s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  36% 0.3620479192898254/1 [00:09<00:17, 27.37s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  37% 0.36573376559519427/1 [00:09<00:17, 27.58s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  37% 0.3699086724135733/1 [00:09<00:16, 26.45s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  38% 0.37714915366716195/1 [00:09<00:13, 20.93s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  38% 0.3819920455764816/1 [00:09<00:13, 22.59s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  39% 0.38651287324552625/1 [00:09<00:14, 23.69s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  39% 0.3908070631158589/1 [00:10<00:14, 24.38s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  40% 0.39557838519400634/1 [00:10<00:14, 23.35s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  40% 0.39992028828512044/1 [00:10<00:14, 24.12s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.7.77  | 39.7 MB   | :  79% 0.7929152530626865/1 [00:10<00:00,  3.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-12.1.105  | 19.7 MB   | :   0% 0.0007921420514148734/1 [00:10<3:37:16, 13047.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  40% 0.4041071234086948/1 [00:10<00:14, 25.06s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  41% 0.40813889056472935/1 [00:10<00:15, 26.49s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  41% 0.41202751805841953/1 [00:10<00:15, 26.29s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  43% 0.4297768361891279/1 [00:10<00:10, 18.46s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  44% 0.4357171321764214/1 [00:11<00:10, 17.97s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-12.1.105  | 15.4 MB   | :   0% 0.001016089426288399/1 [00:11<3:01:14, 10885.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnvjitlink-12.1.10 | 16.9 MB   | :  35% 0.3522489393452132/1 [00:11<00:14, 22.10s/it]        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.13.3         | 4.4 MB    | :   0% 0.0035797327348467667/1 [00:11<51:43, 3114.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  44% 0.44133536392343997/1 [00:11<00:11, 20.78s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnvjitlink-12.1.10 | 16.9 MB   | :  70% 0.7035757610481614/1 [00:11<00:02,  9.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.13.3         | 4.4 MB    | :  94% 0.9378899765298528/1 [00:11<00:00,  8.44s/it]     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  45% 0.4463691087158855/1 [00:11<00:13, 24.74s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-12.1.105  | 15.4 MB   | :  93% 0.9327700933327504/1 [00:11<00:00,  5.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  45% 0.4508660797745394/1 [00:11<00:13, 24.10s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  46% 0.45593560948257106/1 [00:11<00:12, 22.81s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  46% 0.4606950032555231/1 [00:11<00:12, 22.30s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | :   1% 0.011201624454327605/1 [00:11<17:11, 1043.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.4.2       | 1.2 MB    | :   1% 0.012951696748162857/1 [00:11<14:52, 904.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | :  47% 0.4675060655220785/1 [00:11<00:10, 19.52s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.28 GB   | : 100% 0.9969558199237057/1 [00:14<00:00,  5.75s/it]\n",
            "\n",
            "\n",
            "\n",
            "libnpp-12.0.2.50     | 139.8 MB  | : 100% 1.0/1 [00:26<00:00,  3.28s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-12.0.2.5 | 163.0 MB  | : 100% 1.0/1 [00:28<00:00,  3.38s/it]               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | : 100% 1.0/1 [00:39<00:00, 188.19s/it]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-11.0.2.4    | 102.9 MB  | : 100% 1.0/1 [00:39<00:00, 188.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-12.1.105  | 19.7 MB   | : 100% 1.0/1 [00:42<00:00,  6.56s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.7.77  | 39.7 MB   | : 100% 1.0/1 [00:42<00:00,  2.75s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | : 100% 1.0/1 [00:44<00:00, 414.53s/it]              \u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | : 100% 1.0/1 [00:44<00:00, 414.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.13.3         | 4.4 MB    | : 100% 1.0/1 [00:45<00:00,  8.44s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | : 100% 1.0/1 [00:45<00:00, 42.08s/it]                   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | : 100% 1.0/1 [00:45<00:00, 42.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.4.5 | 98.3 MB   | : 100% 1.0/1 [00:47<00:00,  2.28s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnvjitlink-12.1.10 | 16.9 MB   | : 100% 1.0/1 [00:47<00:00,  9.31s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnvjpeg-12.1.1.14  | 2.9 MB    | : 100% 1.0/1 [00:48<00:00, 44.79s/it]                   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnvjpeg-12.1.1.14  | 2.9 MB    | : 100% 1.0/1 [00:48<00:00, 44.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.4.0        | 2.8 MB    | : 100% 1.0/1 [00:48<00:00, 44.88s/it]                   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.4.0        | 2.8 MB    | : 100% 1.0/1 [00:48<00:00, 44.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | : 100% 1.0/1 [00:48<00:00, 44.98s/it]                   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | : 100% 1.0/1 [00:48<00:00, 44.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.4.2       | 1.2 MB    | : 100% 1.0/1 [00:48<00:00, 45.17s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.4.2       | 1.2 MB    | : 100% 1.0/1 [00:48<00:00, 45.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.9.3       | 2.5 MB    | : 100% 1.0/1 [00:48<00:00, 45.34s/it]                   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.9.3       | 2.5 MB    | : 100% 1.0/1 [00:48<00:00, 45.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-12.1.105  | 15.4 MB   | : 100% 1.0/1 [00:50<00:00,  5.88s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 90.9 MB   | : 100% 1.0/1 [00:56<00:00,  1.67s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.28 GB   | : 100% 1.0/1 [03:39<00:00,  5.75s/it]               \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                         \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                         \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n"
          ]
        }
      ],
      "source": [
        "!conda install pytorch==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVg5zqMtAKjy",
        "outputId": "d8f5b2c8-d1a8-4335-e842-d5a648cadc37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric==2.4.0\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from torch_geometric==2.4.0) (4.66.1)\n",
            "Collecting numpy (from torch_geometric==2.4.0)\n",
            "  Downloading numpy-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from torch_geometric==2.4.0)\n",
            "  Downloading scipy-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch_geometric==2.4.0) (3.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from torch_geometric==2.4.0) (2.31.0)\n",
            "Collecting pyparsing (from torch_geometric==2.4.0)\n",
            "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting scikit-learn (from torch_geometric==2.4.0)\n",
            "  Downloading scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting psutil>=5.8.0 (from torch_geometric==2.4.0)\n",
            "  Downloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch_geometric==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (2024.12.14)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn->torch_geometric==2.4.0)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->torch_geometric==2.4.0)\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, pyparsing, psutil, numpy, joblib, scipy, scikit-learn, torch_geometric\n",
            "Successfully installed joblib-1.4.2 numpy-2.2.1 psutil-6.1.1 pyparsing-3.2.1 scikit-learn-1.6.0 scipy-1.15.0 threadpoolctl-3.5.0 torch_geometric-2.4.0\n",
            "Collecting torch-scatter==2.1.2+pt21cu121\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Collecting torch-sparse==0.6.18+pt21cu121\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from torch-sparse==0.6.18+pt21cu121) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.10/site-packages (from scipy->torch-sparse==0.6.18+pt21cu121) (2.2.1)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "finished!\n"
          ]
        }
      ],
      "source": [
        "#!conda install pytorch==2.1.2 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "!pip install torch_geometric==2.4.0  # maybe latest also works\n",
        "!pip install https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl\n",
        "!pip install https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl\n",
        "\n",
        "# maybe need to downgrade numpy\n",
        "print(\"finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BblntAPTPjc",
        "outputId": "2e69a2a3-2066-470e-c402-43e353f8198b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/site-packages (1.26.4)\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.10/site-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/site-packages (from ogb) (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/site-packages (from ogb) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/site-packages (from ogb) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/site-packages (from ogb) (1.6.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/site-packages (from ogb) (2.2.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/site-packages (from ogb) (2.1.0)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/site-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (68.2.2)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (2024.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: ml-collections in /usr/local/lib/python3.10/site-packages (1.0.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from ml-collections) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from ml-collections) (1.17.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/site-packages (from ml-collections) (6.0.2)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3109, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2902, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 245, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 444, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 575, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/operations/check.py\", line 106, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/metadata/pkg_resources.py\", line 221, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2822, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3111, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3121, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3174, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/util.py\", line 256, in _inner\n",
            "    return fn(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 1180, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 845, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 4038, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 845, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 4272, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 845, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 4038, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 845, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 5214, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 845, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 4038, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 845, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 4038, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 845, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 5485, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 4548, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 845, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 4016, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 845, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 4272, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 845, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 4548, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 845, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 4038, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/core.py\", line 853, in _parseNoCache\n",
            "    ret_tokens = ParseResults(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyparsing/results.py\", line 149, in __new__\n",
            "    if isinstance(toklist, ParseResults):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 234, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 217, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/local/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/logging/__init__.py\", line 1622, in _log\n",
            "    record = self.makeRecord(self.name, level, fn, lno, msg, args,\n",
            "  File \"/usr/local/lib/python3.10/logging/__init__.py\", line 1591, in makeRecord\n",
            "    rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n",
            "  File \"/usr/local/lib/python3.10/logging/__init__.py\", line 331, in __init__\n",
            "    self.thread = threading.get_ident()\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: sacred in /usr/local/lib/python3.10/site-packages (0.8.7)\n",
            "Requirement already satisfied: docopt-ng<1.0,>=0.9 in /usr/local/lib/python3.10/site-packages (from sacred) (0.9.0)\n",
            "Requirement already satisfied: jsonpickle>=2.2.0 in /usr/local/lib/python3.10/site-packages (from sacred) (4.0.1)\n",
            "Requirement already satisfied: munch<5.0,>=2.5 in /usr/local/lib/python3.10/site-packages (from sacred) (4.0.0)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from sacred) (1.17.0)\n",
            "Requirement already satisfied: py-cpuinfo>=4.0 in /usr/local/lib/python3.10/site-packages (from sacred) (9.0.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.10/site-packages (from sacred) (0.4.6)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.10/site-packages (from sacred) (23.2)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.10/site-packages (from sacred) (3.1.44)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/site-packages (from GitPython->sacred) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython->sacred) (5.0.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/site-packages (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/site-packages (from wandb) (4.1.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/site-packages (from wandb) (5.29.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (6.1.1)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/site-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from wandb) (68.2.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.26.4\n",
        "!pip install ogb\n",
        "!pip install ml-collections\n",
        "!pip install sacred\n",
        "!pip install wandb\n",
        "!pip install gdown\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: PR-MPNN requires `numpy == 1.26.4`, if using google colab here we need to restart session."
      ],
      "metadata": {
        "id": "cKum7A6VDbBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Oj5tcyoysxo",
        "outputId": "5cd700ae-1a79-4a68-c2f0-effbeae8f3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PR-MPNN\n"
          ]
        }
      ],
      "source": [
        "%cd /content/PR-MPNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify that that all required dependencies are installed."
      ],
      "metadata": {
        "id": "C10MELnKDwaE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfCtQZ91SdaI",
        "outputId": "c7964cb1-1aa8-4f76-cda0-af398df82720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# packages in environment at /usr/local:\n",
            "#\n",
            "# Name                    Version                   Build  Channel\n",
            "_libgcc_mutex             0.1                 conda_forge    conda-forge\n",
            "_openmp_mutex             4.5                  2_kmp_llvm    conda-forge\n",
            "absl-py                   2.1.0                    pypi_0    pypi\n",
            "annotated-types           0.7.0                    pypi_0    pypi\n",
            "archspec                  0.2.2              pyhd8ed1ab_0    conda-forge\n",
            "beautifulsoup4            4.12.3                   pypi_0    pypi\n",
            "blas                      2.116                       mkl    conda-forge\n",
            "blas-devel                3.9.0            16_linux64_mkl    conda-forge\n",
            "boltons                   23.1.1             pyhd8ed1ab_0    conda-forge\n",
            "brotli-python             1.1.0           py310hc6cd4ac_1    conda-forge\n",
            "bzip2                     1.0.8                hd590300_5    conda-forge\n",
            "c-ares                    1.24.0               hd590300_0    conda-forge\n",
            "ca-certificates           2024.12.14           hbcca054_0    conda-forge\n",
            "certifi                   2024.12.14         pyhd8ed1ab_0    conda-forge\n",
            "cffi                      1.16.0          py310h2fee648_0    conda-forge\n",
            "charset-normalizer        3.3.2              pyhd8ed1ab_0    conda-forge\n",
            "click                     8.1.8                    pypi_0    pypi\n",
            "colorama                  0.4.6              pyhd8ed1ab_0    conda-forge\n",
            "conda                     23.11.0         py310hff52083_1    conda-forge\n",
            "conda-libmamba-solver     23.12.0            pyhd8ed1ab_0    conda-forge\n",
            "conda-package-handling    2.2.0              pyh38be061_0    conda-forge\n",
            "conda-package-streaming   0.9.0              pyhd8ed1ab_0    conda-forge\n",
            "cuda-cudart               12.1.105                      0    nvidia\n",
            "cuda-cupti                12.1.105                      0    nvidia\n",
            "cuda-libraries            12.1.0                        0    nvidia\n",
            "cuda-nvrtc                12.1.105                      0    nvidia\n",
            "cuda-nvtx                 12.1.105                      0    nvidia\n",
            "cuda-opencl               12.6.77                       0    nvidia\n",
            "cuda-runtime              12.1.0                        0    nvidia\n",
            "cuda-version              12.6                          3    nvidia\n",
            "distro                    1.8.0              pyhd8ed1ab_0    conda-forge\n",
            "docker-pycreds            0.4.0                    pypi_0    pypi\n",
            "docopt-ng                 0.9.0                    pypi_0    pypi\n",
            "filelock                  3.16.1             pyhd8ed1ab_1    conda-forge\n",
            "fmt                       10.1.1               h00ab1b0_1    conda-forge\n",
            "fsspec                    2024.12.0                pypi_0    pypi\n",
            "gdown                     5.2.0                    pypi_0    pypi\n",
            "gitdb                     4.0.12                   pypi_0    pypi\n",
            "gitpython                 3.1.44                   pypi_0    pypi\n",
            "gmp                       6.3.0                hac33072_2    conda-forge\n",
            "gmpy2                     2.1.5           py310he8512ff_3    conda-forge\n",
            "icu                       73.2                 h59595ed_0    conda-forge\n",
            "idna                      3.6                pyhd8ed1ab_0    conda-forge\n",
            "jinja2                    3.1.5              pyhd8ed1ab_0    conda-forge\n",
            "joblib                    1.4.2                    pypi_0    pypi\n",
            "jsonpatch                 1.33               pyhd8ed1ab_0    conda-forge\n",
            "jsonpickle                4.0.1                    pypi_0    pypi\n",
            "jsonpointer               2.4             py310hff52083_3    conda-forge\n",
            "keyutils                  1.6.1                h166bdaf_0    conda-forge\n",
            "krb5                      1.21.2               h659d440_0    conda-forge\n",
            "ld_impl_linux-64          2.40                 h41732ed_0    conda-forge\n",
            "libarchive                3.7.2                h2aa1ff5_1    conda-forge\n",
            "libblas                   3.9.0            16_linux64_mkl    conda-forge\n",
            "libcblas                  3.9.0            16_linux64_mkl    conda-forge\n",
            "libcublas                 12.1.0.26                     0    nvidia\n",
            "libcufft                  11.0.2.4                      0    nvidia\n",
            "libcufile                 1.11.1.6                      0    nvidia\n",
            "libcurand                 10.3.7.77                     0    nvidia\n",
            "libcurl                   8.5.0                hca28451_0    conda-forge\n",
            "libcusolver               11.4.4.55                     0    nvidia\n",
            "libcusparse               12.0.2.55                     0    nvidia\n",
            "libedit                   3.1.20191231         he28a2e2_2    conda-forge\n",
            "libev                     4.33                 hd590300_2    conda-forge\n",
            "libffi                    3.4.2                h7f98852_5    conda-forge\n",
            "libgcc                    14.2.0               h77fa898_1    conda-forge\n",
            "libgcc-ng                 14.2.0               h69a702a_1    conda-forge\n",
            "libgfortran               14.2.0               h69a702a_1    conda-forge\n",
            "libgfortran-ng            14.2.0               h69a702a_1    conda-forge\n",
            "libgfortran5              14.2.0               hd5240d6_1    conda-forge\n",
            "libgomp                   14.2.0               h77fa898_1    conda-forge\n",
            "libhwloc                  2.9.3           default_h554bfaf_1009    conda-forge\n",
            "libiconv                  1.17                 hd590300_2    conda-forge\n",
            "liblapack                 3.9.0            16_linux64_mkl    conda-forge\n",
            "liblapacke                3.9.0            16_linux64_mkl    conda-forge\n",
            "libmamba                  1.5.5                had39da4_0    conda-forge\n",
            "libmambapy                1.5.5           py310h39ff949_0    conda-forge\n",
            "libnghttp2                1.58.0               h47da74e_1    conda-forge\n",
            "libnpp                    12.0.2.50                     0    nvidia\n",
            "libnsl                    2.0.1                hd590300_0    conda-forge\n",
            "libnvjitlink              12.1.105                      0    nvidia\n",
            "libnvjpeg                 12.1.1.14                     0    nvidia\n",
            "libsolv                   0.7.27               hfc55251_0    conda-forge\n",
            "libsqlite                 3.44.2               h2797004_0    conda-forge\n",
            "libssh2                   1.11.0               h0841786_0    conda-forge\n",
            "libstdcxx-ng              13.2.0               h7e041cc_3    conda-forge\n",
            "libuuid                   2.38.1               h0b41bf4_0    conda-forge\n",
            "libxml2                   2.12.3               h232c23b_0    conda-forge\n",
            "libzlib                   1.2.13               hd590300_5    conda-forge\n",
            "littleutils               0.2.4                    pypi_0    pypi\n",
            "llvm-openmp               15.0.7               h0cdce71_0    conda-forge\n",
            "lz4-c                     1.9.4                hcb278e6_0    conda-forge\n",
            "lzo                       2.10              h516909a_1000    conda-forge\n",
            "mamba                     1.5.5           py310h51d5547_0    conda-forge\n",
            "markupsafe                3.0.2           py310h89163eb_1    conda-forge\n",
            "menuinst                  2.0.1           py310hff52083_0    conda-forge\n",
            "mkl                       2022.1.0           h84fe81f_915    conda-forge\n",
            "mkl-devel                 2022.1.0           ha770c72_916    conda-forge\n",
            "mkl-include               2022.1.0           h84fe81f_915    conda-forge\n",
            "ml-collections            1.0.0                    pypi_0    pypi\n",
            "mpc                       1.3.1                h24ddda3_1    conda-forge\n",
            "mpfr                      4.2.1                h90cbb55_3    conda-forge\n",
            "mpmath                    1.3.0              pyhd8ed1ab_1    conda-forge\n",
            "munch                     4.0.0                    pypi_0    pypi\n",
            "ncurses                   6.4                  h59595ed_2    conda-forge\n",
            "networkx                  3.4.2              pyh267e887_2    conda-forge\n",
            "numpy                     1.26.4                   pypi_0    pypi\n",
            "ogb                       1.3.6                    pypi_0    pypi\n",
            "openssl                   3.4.0                hb9d3cd8_0    conda-forge\n",
            "outdated                  0.2.2                    pypi_0    pypi\n",
            "packaging                 23.2               pyhd8ed1ab_0    conda-forge\n",
            "pandas                    2.2.3                    pypi_0    pypi\n",
            "pip                       23.3.2             pyhd8ed1ab_0    conda-forge\n",
            "platformdirs              4.1.0              pyhd8ed1ab_0    conda-forge\n",
            "pluggy                    1.3.0              pyhd8ed1ab_0    conda-forge\n",
            "protobuf                  5.29.2                   pypi_0    pypi\n",
            "psutil                    6.1.1                    pypi_0    pypi\n",
            "py-cpuinfo                9.0.0                    pypi_0    pypi\n",
            "pybind11-abi              4                    hd8ed1ab_3    conda-forge\n",
            "pycosat                   0.6.6           py310h2372a71_0    conda-forge\n",
            "pycparser                 2.21               pyhd8ed1ab_0    conda-forge\n",
            "pydantic                  2.10.4                   pypi_0    pypi\n",
            "pydantic-core             2.27.2                   pypi_0    pypi\n",
            "pyparsing                 3.2.1                    pypi_0    pypi\n",
            "pysocks                   1.7.1              pyha2e5f31_6    conda-forge\n",
            "python                    3.10.13         hd12c33a_0_cpython    conda-forge\n",
            "python-dateutil           2.9.0.post0              pypi_0    pypi\n",
            "python_abi                3.10                    4_cp310    conda-forge\n",
            "pytorch                   2.1.2           py3.10_cuda12.1_cudnn8.9.2_0    pytorch\n",
            "pytorch-cuda              12.1                 ha16c6d3_6    pytorch\n",
            "pytorch-mutex             1.0                        cuda    pytorch\n",
            "pytz                      2024.2                   pypi_0    pypi\n",
            "pyyaml                    6.0.2           py310ha75aee5_1    conda-forge\n",
            "readline                  8.2                  h8228510_1    conda-forge\n",
            "reproc                    14.2.4.post0         hd590300_1    conda-forge\n",
            "reproc-cpp                14.2.4.post0         h59595ed_1    conda-forge\n",
            "requests                  2.31.0             pyhd8ed1ab_0    conda-forge\n",
            "ruamel.yaml               0.18.5          py310h2372a71_0    conda-forge\n",
            "ruamel.yaml.clib          0.2.7           py310h2372a71_2    conda-forge\n",
            "sacred                    0.8.7                    pypi_0    pypi\n",
            "scikit-learn              1.6.0                    pypi_0    pypi\n",
            "scipy                     1.15.0                   pypi_0    pypi\n",
            "sentry-sdk                2.19.2                   pypi_0    pypi\n",
            "setproctitle              1.3.4                    pypi_0    pypi\n",
            "setuptools                68.2.2             pyhd8ed1ab_0    conda-forge\n",
            "six                       1.17.0                   pypi_0    pypi\n",
            "smmap                     5.0.2                    pypi_0    pypi\n",
            "soupsieve                 2.6                      pypi_0    pypi\n",
            "sympy                     1.13.3          pypyh2585a3b_103    conda-forge\n",
            "tbb                       2021.11.0            h00ab1b0_1    conda-forge\n",
            "threadpoolctl             3.5.0                    pypi_0    pypi\n",
            "tk                        8.6.13          noxft_h4845f30_101    conda-forge\n",
            "torch-geometric           2.4.0                    pypi_0    pypi\n",
            "torch-scatter             2.1.2+pt21cu121          pypi_0    pypi\n",
            "torch-sparse              0.6.18+pt21cu121          pypi_0    pypi\n",
            "torchtriton               2.1.0                     py310    pytorch\n",
            "tqdm                      4.66.1             pyhd8ed1ab_0    conda-forge\n",
            "truststore                0.8.0              pyhd8ed1ab_0    conda-forge\n",
            "typing_extensions         4.12.2             pyha770c72_1    conda-forge\n",
            "tzdata                    2024.2                   pypi_0    pypi\n",
            "urllib3                   2.1.0              pyhd8ed1ab_0    conda-forge\n",
            "wandb                     0.19.1                   pypi_0    pypi\n",
            "wheel                     0.42.0             pyhd8ed1ab_0    conda-forge\n",
            "wrapt                     1.17.0                   pypi_0    pypi\n",
            "xz                        5.2.6                h166bdaf_0    conda-forge\n",
            "yaml                      0.2.5                h7f98852_2    conda-forge\n",
            "yaml-cpp                  0.8.0                h59595ed_0    conda-forge\n",
            "zstandard                 0.22.0          py310h1275a96_0    conda-forge\n",
            "zstd                      1.5.5                hfc55251_0    conda-forge\n"
          ]
        }
      ],
      "source": [
        "!conda list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify that the correct version of Numpy is installed."
      ],
      "metadata": {
        "id": "3tOVw-HYD3nJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBK0Ih15S5rq",
        "outputId": "4b450742-0c5f-4cd1-a089-05656cf2b0cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.26.4\n",
            "Requirement already satisfied: sacred in /usr/local/lib/python3.10/site-packages (0.8.7)\n",
            "Requirement already satisfied: docopt-ng<1.0,>=0.9 in /usr/local/lib/python3.10/site-packages (from sacred) (0.9.0)\n",
            "Requirement already satisfied: jsonpickle>=2.2.0 in /usr/local/lib/python3.10/site-packages (from sacred) (4.0.1)\n",
            "Requirement already satisfied: munch<5.0,>=2.5 in /usr/local/lib/python3.10/site-packages (from sacred) (4.0.0)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from sacred) (1.17.0)\n",
            "Requirement already satisfied: py-cpuinfo>=4.0 in /usr/local/lib/python3.10/site-packages (from sacred) (9.0.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.10/site-packages (from sacred) (0.4.6)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.10/site-packages (from sacred) (23.2)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.10/site-packages (from sacred) (3.1.44)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/site-packages (from GitPython->sacred) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython->sacred) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy\n",
        "print(numpy.__version__)\n",
        "!pip install sacred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to run the following command to execute `run.py`"
      ],
      "metadata": {
        "id": "mlgVq0IbEEio"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-WUTC7wxdQm",
        "outputId": "257f393a-86bf-4f5a-ab0c-ddc339b8de4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libarcher.so is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before running\n",
        "\n",
        "Follow the steps below to run PR-MPNN on `4cycles` and `skipcircles`:\n",
        "\n",
        "1. Download `symmetreis_4d_hot.py`, `prior5_4cycle.yaml`, `prior5_skipcircles.yaml`, `prior10_4cycle.yaml`, `prior10_skipcircles.yaml` from the repository.\n",
        "\n",
        "2. Upload `symmetries_4d_hot.py` to the `/content/PR-MPNN/data/custom_datasets` repository. Delete the existing file `symmetries.py`, and rename `symmetries_4d_hot.py` as `symmetries.py`.\n",
        "\n",
        "3. Go to line 30 of `/content/PR-MPNN/data/const.py` and change the value of `edge` from 3 to 4 in the `sym_skipcircles` dictionary.\n",
        "\n",
        "4. Upload `prior5_4cycle.yaml`, `prior5_skipcircles.yaml`, `prior10_4cycle.yaml`, `prior10_skipcircles.yaml` to the directory `/content/PR-MPNN/configs/sym_4cycles`\n",
        "\n",
        "All results in the report can be replicated by running the corresponding `yaml` file."
      ],
      "metadata": {
        "id": "hrfJtyLMHI2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run!\n",
        "Before running, if using wandb, we need to first create an team in the wandb account and replace the `entity` variable in the 30th line of `run.py` with the name of the team."
      ],
      "metadata": {
        "id": "rK8gaNiU2jzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NneLupQlyyyK",
        "outputId": "98f033d5-fb55-4fda-8c01-d42b1d59930a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING - root - Added new config entry: \"fixed.batch_size\"\n",
            "WARNING - root - Added new config entry: \"fixed.bn\"\n",
            "WARNING - root - Added new config entry: \"fixed.data_path\"\n",
            "WARNING - root - Added new config entry: \"fixed.dataset\"\n",
            "WARNING - root - Added new config entry: \"fixed.debug\"\n",
            "WARNING - root - Added new config entry: \"fixed.dropout\"\n",
            "WARNING - root - Added new config entry: \"fixed.early_stop.patience\"\n",
            "WARNING - root - Added new config entry: \"fixed.graph_pooling\"\n",
            "WARNING - root - Added new config entry: \"fixed.hid_size\"\n",
            "WARNING - root - Added new config entry: \"fixed.imle_configs.batchnorm\"\n",
            "WARNING - root - Added new config entry: \"fixed.imle_configs.emb_hid_size\"\n",
            "WARNING - root - Added new config entry: \"fixed.imle_configs.gnn_layer\"\n",
            "WARNING - root - Added new config entry: \"fixed.imle_configs.mlp_layer\"\n",
            "WARNING - root - Added new config entry: \"fixed.imle_configs.num_train_ensemble\"\n",
            "WARNING - root - Added new config entry: \"fixed.imle_configs.num_val_ensemble\"\n",
            "WARNING - root - Added new config entry: \"fixed.inter_graph_pooling\"\n",
            "WARNING - root - Added new config entry: \"fixed.log_path\"\n",
            "WARNING - root - Added new config entry: \"fixed.lr\"\n",
            "WARNING - root - Added new config entry: \"fixed.lr_decay.decay_rate\"\n",
            "WARNING - root - Added new config entry: \"fixed.lr_decay.mode\"\n",
            "WARNING - root - Added new config entry: \"fixed.lr_decay.patience\"\n",
            "WARNING - root - Added new config entry: \"fixed.max_epochs\"\n",
            "WARNING - root - Added new config entry: \"fixed.min_epochs\"\n",
            "WARNING - root - Added new config entry: \"fixed.mlp_layers_intergraph\"\n",
            "WARNING - root - Added new config entry: \"fixed.mlp_layers_intragraph\"\n",
            "WARNING - root - Added new config entry: \"fixed.model\"\n",
            "WARNING - root - Added new config entry: \"fixed.num_convlayers\"\n",
            "WARNING - root - Added new config entry: \"fixed.num_runs\"\n",
            "WARNING - root - Added new config entry: \"fixed.reg\"\n",
            "WARNING - root - Added new config entry: \"fixed.residual\"\n",
            "WARNING - root - Added new config entry: \"fixed.sample_configs.candid_pool\"\n",
            "WARNING - root - Added new config entry: \"fixed.sample_configs.directed\"\n",
            "WARNING - root - Added new config entry: \"fixed.sample_configs.ensemble\"\n",
            "WARNING - root - Added new config entry: \"fixed.sample_configs.heuristic\"\n",
            "WARNING - root - Added new config entry: \"fixed.sample_configs.include_original_graph\"\n",
            "WARNING - root - Added new config entry: \"fixed.sample_configs.merge_priors\"\n",
            "WARNING - root - Added new config entry: \"fixed.sample_configs.sample_k\"\n",
            "WARNING - root - Added new config entry: \"fixed.sample_configs.sample_k2\"\n",
            "WARNING - root - Added new config entry: \"fixed.sample_configs.separate\"\n",
            "WARNING - root - Added new config entry: \"fixed.use_wandb\"\n",
            "WARNING - root - Added new config entry: \"fixed.wandb_name\"\n",
            "WARNING - root - Added new config entry: \"fixed.wandb_project\"\n",
            "WARNING - run - No observers have been added to this run\n",
            "INFO - run - Running command 'main'\n",
            "INFO - run - Started\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzihanzhou2021\u001b[0m (\u001b[33mGRL_miniproject\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/PR-MPNN/wandb/run-20250104_020136-3v3buwaj\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mprior5_skipcircles-mean\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/GRL_miniproject/grl-miniproject-skipcircles\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/GRL_miniproject/grl-miniproject-skipcircles/runs/3v3buwaj\u001b[0m\n",
            "INFO - root - epoch: 0, training loss: 2.35717, val loss: 2.309, patience: 0, training metric: 0.1012, val metric: 0.1\n",
            "INFO - root - epoch: 1, training loss: 2.18734, val loss: 2.30926, patience: 1, training metric: 0.2528, val metric: 0.1\n",
            "INFO - root - epoch: 2, training loss: 2.04957, val loss: 2.30963, patience: 2, training metric: 0.3744, val metric: 0.1\n",
            "INFO - root - epoch: 3, training loss: 1.96133, val loss: 2.31018, patience: 3, training metric: 0.4996, val metric: 0.1\n",
            "INFO - root - epoch: 4, training loss: 1.89534, val loss: 2.31077, patience: 4, training metric: 0.5764, val metric: 0.1\n",
            "INFO - root - epoch: 5, training loss: 1.84048, val loss: 2.31161, patience: 5, training metric: 0.6064, val metric: 0.1\n",
            "INFO - root - epoch: 6, training loss: 1.79061, val loss: 2.31261, patience: 6, training metric: 0.6344, val metric: 0.1\n",
            "INFO - root - epoch: 7, training loss: 1.75141, val loss: 2.31376, patience: 7, training metric: 0.6616, val metric: 0.1\n",
            "INFO - root - epoch: 8, training loss: 1.70919, val loss: 2.31526, patience: 8, training metric: 0.6724, val metric: 0.1\n",
            "INFO - root - epoch: 9, training loss: 1.67511, val loss: 2.31674, patience: 9, training metric: 0.6888, val metric: 0.1\n",
            "INFO - root - epoch: 10, training loss: 1.63603, val loss: 2.31804, patience: 10, training metric: 0.7036, val metric: 0.1\n",
            "INFO - root - epoch: 11, training loss: 1.59981, val loss: 2.31923, patience: 11, training metric: 0.7128, val metric: 0.1\n",
            "INFO - root - epoch: 12, training loss: 1.56476, val loss: 2.31987, patience: 12, training metric: 0.7232, val metric: 0.1\n",
            "INFO - root - epoch: 13, training loss: 1.52917, val loss: 2.32014, patience: 13, training metric: 0.7532, val metric: 0.1\n",
            "INFO - root - epoch: 14, training loss: 1.49165, val loss: 2.32035, patience: 14, training metric: 0.762, val metric: 0.1\n",
            "INFO - root - epoch: 15, training loss: 1.46988, val loss: 2.32069, patience: 15, training metric: 0.7692, val metric: 0.1\n",
            "INFO - root - epoch: 16, training loss: 1.44446, val loss: 2.32145, patience: 16, training metric: 0.7792, val metric: 0.1\n",
            "INFO - root - epoch: 17, training loss: 1.41865, val loss: 2.32278, patience: 17, training metric: 0.7948, val metric: 0.1\n",
            "INFO - root - epoch: 18, training loss: 1.39889, val loss: 2.32471, patience: 18, training metric: 0.796, val metric: 0.1\n",
            "INFO - root - epoch: 19, training loss: 1.366, val loss: 2.32779, patience: 19, training metric: 0.8268, val metric: 0.1\n",
            "INFO - root - epoch: 20, training loss: 1.34358, val loss: 2.33216, patience: 20, training metric: 0.8448, val metric: 0.1\n",
            "INFO - root - epoch: 21, training loss: 1.31296, val loss: 2.33837, patience: 21, training metric: 0.84, val metric: 0.1\n",
            "INFO - root - epoch: 22, training loss: 1.28625, val loss: 2.34534, patience: 22, training metric: 0.8572, val metric: 0.1\n",
            "INFO - root - epoch: 23, training loss: 1.25972, val loss: 2.3535, patience: 23, training metric: 0.8804, val metric: 0.1\n",
            "INFO - root - epoch: 24, training loss: 1.23144, val loss: 2.36238, patience: 24, training metric: 0.8816, val metric: 0.1\n",
            "INFO - root - epoch: 25, training loss: 1.20698, val loss: 2.37101, patience: 25, training metric: 0.8944, val metric: 0.094\n",
            "INFO - root - epoch: 26, training loss: 1.18983, val loss: 2.38035, patience: 26, training metric: 0.9064, val metric: 0.1\n",
            "INFO - root - epoch: 27, training loss: 1.15856, val loss: 2.38872, patience: 27, training metric: 0.9244, val metric: 0.1\n",
            "INFO - root - epoch: 28, training loss: 1.13489, val loss: 2.39618, patience: 28, training metric: 0.9344, val metric: 0.1\n",
            "INFO - root - epoch: 29, training loss: 1.11091, val loss: 2.39826, patience: 29, training metric: 0.9544, val metric: 0.1\n",
            "INFO - root - epoch: 30, training loss: 1.08992, val loss: 2.40159, patience: 30, training metric: 0.9668, val metric: 0.1\n",
            "INFO - root - epoch: 31, training loss: 1.062, val loss: 2.40293, patience: 31, training metric: 0.9788, val metric: 0.1\n",
            "INFO - root - epoch: 32, training loss: 1.03607, val loss: 2.40437, patience: 32, training metric: 0.9884, val metric: 0.1\n",
            "INFO - root - epoch: 33, training loss: 1.00985, val loss: 2.40531, patience: 33, training metric: 0.9952, val metric: 0.1\n",
            "INFO - root - epoch: 34, training loss: 0.98859, val loss: 2.40669, patience: 34, training metric: 0.9976, val metric: 0.1\n",
            "INFO - root - epoch: 35, training loss: 0.96659, val loss: 2.41299, patience: 0, training metric: 0.9984, val metric: 0.1204\n",
            "INFO - root - epoch: 36, training loss: 0.94274, val loss: 2.42336, patience: 1, training metric: 0.9992, val metric: 0.1028\n",
            "INFO - root - epoch: 37, training loss: 0.92145, val loss: 2.43297, patience: 2, training metric: 0.9996, val metric: 0.1008\n",
            "INFO - root - epoch: 38, training loss: 0.90403, val loss: 2.4357, patience: 3, training metric: 0.9996, val metric: 0.1104\n",
            "INFO - root - epoch: 39, training loss: 0.88191, val loss: 2.43857, patience: 4, training metric: 0.9996, val metric: 0.084\n",
            "INFO - root - epoch: 40, training loss: 0.85508, val loss: 2.45993, patience: 5, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 41, training loss: 0.84023, val loss: 2.50609, patience: 6, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 42, training loss: 0.82277, val loss: 2.55557, patience: 7, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 43, training loss: 0.80674, val loss: 2.58659, patience: 8, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 44, training loss: 0.78435, val loss: 2.61626, patience: 9, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 45, training loss: 0.76625, val loss: 2.65452, patience: 10, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 46, training loss: 0.74628, val loss: 2.7262, patience: 11, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 47, training loss: 0.72633, val loss: 2.81733, patience: 12, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 48, training loss: 0.71447, val loss: 2.93256, patience: 13, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 49, training loss: 0.69538, val loss: 3.04992, patience: 14, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 50, training loss: 0.67455, val loss: 3.1111, patience: 15, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 51, training loss: 0.6611, val loss: 3.0921, patience: 16, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 52, training loss: 0.64422, val loss: 3.05446, patience: 17, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 53, training loss: 0.62537, val loss: 3.05677, patience: 18, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 54, training loss: 0.6072, val loss: 3.05164, patience: 19, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 55, training loss: 0.59365, val loss: 3.00683, patience: 20, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 56, training loss: 0.57944, val loss: 3.06012, patience: 21, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 57, training loss: 0.56236, val loss: 3.13603, patience: 22, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 58, training loss: 0.5465, val loss: 3.22459, patience: 23, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 59, training loss: 0.53101, val loss: 3.3226, patience: 24, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 60, training loss: 0.51712, val loss: 3.39454, patience: 25, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 61, training loss: 0.5064, val loss: 3.45843, patience: 26, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 62, training loss: 0.49145, val loss: 3.6214, patience: 27, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 63, training loss: 0.47733, val loss: 3.85693, patience: 28, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 64, training loss: 0.46585, val loss: 4.28636, patience: 29, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 65, training loss: 0.45166, val loss: 4.65056, patience: 30, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 66, training loss: 0.44152, val loss: 5.19785, patience: 31, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 67, training loss: 0.42899, val loss: 5.74744, patience: 32, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 68, training loss: 0.41619, val loss: 6.51384, patience: 33, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 69, training loss: 0.40448, val loss: 7.20537, patience: 34, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 70, training loss: 0.39498, val loss: 7.35748, patience: 35, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 71, training loss: 0.38369, val loss: 6.6673, patience: 36, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 72, training loss: 0.37434, val loss: 5.8136, patience: 37, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 73, training loss: 0.36479, val loss: 4.85831, patience: 38, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 74, training loss: 0.35498, val loss: 4.79246, patience: 39, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 75, training loss: 0.34542, val loss: 5.28446, patience: 40, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 76, training loss: 0.33725, val loss: 6.00455, patience: 41, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 77, training loss: 0.32864, val loss: 6.44579, patience: 42, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 78, training loss: 0.321, val loss: 6.43602, patience: 43, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 79, training loss: 0.31293, val loss: 6.05795, patience: 44, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 80, training loss: 0.30547, val loss: 5.61919, patience: 45, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 81, training loss: 0.29706, val loss: 5.34265, patience: 46, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 82, training loss: 0.29028, val loss: 5.47149, patience: 47, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 83, training loss: 0.28279, val loss: 5.84508, patience: 48, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 84, training loss: 0.27584, val loss: 6.07868, patience: 49, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 85, training loss: 0.26937, val loss: 6.00177, patience: 50, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 86, training loss: 0.26424, val loss: 5.59165, patience: 51, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 87, training loss: 0.25735, val loss: 5.09917, patience: 52, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 88, training loss: 0.25411, val loss: 4.62207, patience: 53, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 89, training loss: 0.25182, val loss: 4.23219, patience: 54, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 90, training loss: 0.24941, val loss: 3.92661, patience: 55, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 91, training loss: 0.24614, val loss: 3.70024, patience: 56, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 92, training loss: 0.24366, val loss: 3.55644, patience: 57, training metric: 1.0, val metric: 0.0612\n",
            "INFO - root - epoch: 93, training loss: 0.24125, val loss: 3.54921, patience: 58, training metric: 1.0, val metric: 0.02\n",
            "INFO - root - epoch: 94, training loss: 0.23822, val loss: 3.60098, patience: 59, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 95, training loss: 0.23686, val loss: 3.62127, patience: 60, training metric: 1.0, val metric: 0.0008\n",
            "INFO - root - epoch: 96, training loss: 0.2333, val loss: 3.60055, patience: 61, training metric: 1.0, val metric: 0.0\n",
            "INFO - root - epoch: 97, training loss: 0.23106, val loss: 3.55972, patience: 62, training metric: 1.0, val metric: 0.0004\n",
            "INFO - root - epoch: 98, training loss: 0.22795, val loss: 3.49747, patience: 63, training metric: 1.0, val metric: 0.0012\n",
            "INFO - root - epoch: 99, training loss: 0.22614, val loss: 3.41679, patience: 64, training metric: 1.0, val metric: 0.0024\n",
            "INFO - root - epoch: 100, training loss: 0.22338, val loss: 3.32961, patience: 65, training metric: 1.0, val metric: 0.004\n",
            "INFO - root - epoch: 101, training loss: 0.22119, val loss: 3.24953, patience: 66, training metric: 1.0, val metric: 0.0024\n",
            "INFO - root - epoch: 102, training loss: 0.21917, val loss: 3.18947, patience: 67, training metric: 1.0, val metric: 0.0008\n",
            "INFO - root - epoch: 103, training loss: 0.21694, val loss: 3.12614, patience: 68, training metric: 1.0, val metric: 0.0016\n",
            "INFO - root - epoch: 104, training loss: 0.21498, val loss: 3.09134, patience: 69, training metric: 1.0, val metric: 0.0008\n",
            "INFO - root - epoch: 105, training loss: 0.21247, val loss: 3.07357, patience: 70, training metric: 1.0, val metric: 0.0008\n",
            "INFO - root - epoch: 106, training loss: 0.21003, val loss: 3.08704, patience: 71, training metric: 1.0, val metric: 0.0024\n",
            "INFO - root - epoch: 107, training loss: 0.20784, val loss: 3.10422, patience: 72, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 108, training loss: 0.20625, val loss: 3.11101, patience: 73, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 109, training loss: 0.20476, val loss: 3.13087, patience: 74, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 110, training loss: 0.20207, val loss: 3.15336, patience: 75, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 111, training loss: 0.2, val loss: 3.18973, patience: 76, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 112, training loss: 0.19831, val loss: 3.22951, patience: 77, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 113, training loss: 0.1967, val loss: 3.2415, patience: 78, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 114, training loss: 0.19451, val loss: 3.24556, patience: 79, training metric: 1.0, val metric: 0.0256\n",
            "INFO - root - epoch: 115, training loss: 0.19284, val loss: 3.24377, patience: 80, training metric: 1.0, val metric: 0.0532\n",
            "INFO - root - epoch: 116, training loss: 0.19108, val loss: 3.23553, patience: 81, training metric: 1.0, val metric: 0.0644\n",
            "INFO - root - epoch: 117, training loss: 0.18897, val loss: 3.21992, patience: 82, training metric: 1.0, val metric: 0.074\n",
            "INFO - root - epoch: 118, training loss: 0.18733, val loss: 3.2162, patience: 83, training metric: 1.0, val metric: 0.0884\n",
            "INFO - root - epoch: 119, training loss: 0.18521, val loss: 3.20773, patience: 84, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 120, training loss: 0.18341, val loss: 3.18269, patience: 85, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 121, training loss: 0.18113, val loss: 3.1801, patience: 86, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 122, training loss: 0.17984, val loss: 3.17222, patience: 87, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 123, training loss: 0.178, val loss: 3.16384, patience: 88, training metric: 1.0, val metric: 0.0696\n",
            "INFO - root - epoch: 124, training loss: 0.177, val loss: 3.15082, patience: 89, training metric: 1.0, val metric: 0.0444\n",
            "INFO - root - epoch: 125, training loss: 0.17526, val loss: 3.10375, patience: 90, training metric: 1.0, val metric: 0.0332\n",
            "INFO - root - epoch: 126, training loss: 0.17353, val loss: 3.07971, patience: 91, training metric: 1.0, val metric: 0.0324\n",
            "INFO - root - epoch: 127, training loss: 0.17218, val loss: 3.07579, patience: 92, training metric: 1.0, val metric: 0.034\n",
            "INFO - root - epoch: 128, training loss: 0.17028, val loss: 3.13336, patience: 93, training metric: 1.0, val metric: 0.0268\n",
            "INFO - root - epoch: 129, training loss: 0.16883, val loss: 3.1799, patience: 94, training metric: 1.0, val metric: 0.0208\n",
            "INFO - root - epoch: 130, training loss: 0.16719, val loss: 3.21977, patience: 95, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 131, training loss: 0.16579, val loss: 3.25169, patience: 96, training metric: 1.0, val metric: 0.036\n",
            "INFO - root - epoch: 132, training loss: 0.16429, val loss: 3.24582, patience: 97, training metric: 1.0, val metric: 0.0256\n",
            "INFO - root - epoch: 133, training loss: 0.16255, val loss: 3.25627, patience: 98, training metric: 1.0, val metric: 0.0516\n",
            "INFO - root - epoch: 134, training loss: 0.16129, val loss: 3.27689, patience: 99, training metric: 1.0, val metric: 0.0568\n",
            "INFO - root - epoch: 135, training loss: 0.16018, val loss: 3.3106, patience: 100, training metric: 1.0, val metric: 0.058\n",
            "INFO - root - epoch: 136, training loss: 0.15783, val loss: 3.32043, patience: 101, training metric: 1.0, val metric: 0.0552\n",
            "INFO - root - epoch: 137, training loss: 0.1571, val loss: 3.32954, patience: 102, training metric: 1.0, val metric: 0.0348\n",
            "INFO - root - epoch: 138, training loss: 0.15522, val loss: 3.30392, patience: 103, training metric: 1.0, val metric: 0.052\n",
            "INFO - root - epoch: 139, training loss: 0.15494, val loss: 3.28806, patience: 104, training metric: 1.0, val metric: 0.0588\n",
            "INFO - root - epoch: 140, training loss: 0.15398, val loss: 3.26482, patience: 105, training metric: 1.0, val metric: 0.0736\n",
            "INFO - root - epoch: 141, training loss: 0.15357, val loss: 3.25343, patience: 106, training metric: 1.0, val metric: 0.0776\n",
            "INFO - root - epoch: 142, training loss: 0.15214, val loss: 3.25265, patience: 107, training metric: 1.0, val metric: 0.0828\n",
            "INFO - root - epoch: 143, training loss: 0.15269, val loss: 3.25171, patience: 108, training metric: 1.0, val metric: 0.0612\n",
            "INFO - root - epoch: 144, training loss: 0.1515, val loss: 3.26245, patience: 109, training metric: 1.0, val metric: 0.0368\n",
            "INFO - root - epoch: 145, training loss: 0.15083, val loss: 3.2836, patience: 110, training metric: 1.0, val metric: 0.0236\n",
            "INFO - root - epoch: 146, training loss: 0.15035, val loss: 3.27917, patience: 111, training metric: 1.0, val metric: 0.0188\n",
            "INFO - root - epoch: 147, training loss: 0.14953, val loss: 3.27451, patience: 112, training metric: 1.0, val metric: 0.014\n",
            "INFO - root - epoch: 148, training loss: 0.14897, val loss: 3.27806, patience: 113, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 149, training loss: 0.14872, val loss: 3.27367, patience: 114, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 150, training loss: 0.14768, val loss: 3.27394, patience: 115, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 151, training loss: 0.147, val loss: 3.28207, patience: 116, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 152, training loss: 0.14588, val loss: 3.29163, patience: 117, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 153, training loss: 0.14559, val loss: 3.31397, patience: 118, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 154, training loss: 0.1445, val loss: 3.3232, patience: 119, training metric: 1.0, val metric: 0.0128\n",
            "INFO - root - epoch: 155, training loss: 0.14486, val loss: 3.33566, patience: 120, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 156, training loss: 0.14419, val loss: 3.34093, patience: 121, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 157, training loss: 0.14322, val loss: 3.34062, patience: 122, training metric: 1.0, val metric: 0.0272\n",
            "INFO - root - epoch: 158, training loss: 0.14286, val loss: 3.33089, patience: 123, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 159, training loss: 0.14195, val loss: 3.33562, patience: 124, training metric: 1.0, val metric: 0.0552\n",
            "INFO - root - epoch: 160, training loss: 0.1412, val loss: 3.32324, patience: 125, training metric: 1.0, val metric: 0.0696\n",
            "INFO - root - epoch: 161, training loss: 0.14088, val loss: 3.31374, patience: 126, training metric: 1.0, val metric: 0.0852\n",
            "INFO - root - epoch: 162, training loss: 0.14027, val loss: 3.3121, patience: 127, training metric: 1.0, val metric: 0.086\n",
            "INFO - root - epoch: 163, training loss: 0.14, val loss: 3.30699, patience: 128, training metric: 1.0, val metric: 0.0832\n",
            "INFO - root - epoch: 164, training loss: 0.13965, val loss: 3.30043, patience: 129, training metric: 1.0, val metric: 0.0784\n",
            "INFO - root - epoch: 165, training loss: 0.13887, val loss: 3.31055, patience: 130, training metric: 1.0, val metric: 0.076\n",
            "INFO - root - epoch: 166, training loss: 0.13813, val loss: 3.31859, patience: 131, training metric: 1.0, val metric: 0.066\n",
            "INFO - root - epoch: 167, training loss: 0.13767, val loss: 3.32818, patience: 132, training metric: 1.0, val metric: 0.0748\n",
            "INFO - root - epoch: 168, training loss: 0.13705, val loss: 3.35427, patience: 133, training metric: 1.0, val metric: 0.062\n",
            "INFO - root - epoch: 169, training loss: 0.13622, val loss: 3.38518, patience: 134, training metric: 1.0, val metric: 0.0584\n",
            "INFO - root - epoch: 170, training loss: 0.13577, val loss: 3.40548, patience: 135, training metric: 1.0, val metric: 0.0676\n",
            "INFO - root - epoch: 171, training loss: 0.13504, val loss: 3.40937, patience: 136, training metric: 1.0, val metric: 0.0812\n",
            "INFO - root - epoch: 172, training loss: 0.13444, val loss: 3.42094, patience: 137, training metric: 1.0, val metric: 0.0876\n",
            "INFO - root - epoch: 173, training loss: 0.13335, val loss: 3.41427, patience: 138, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 174, training loss: 0.13331, val loss: 3.4146, patience: 139, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 175, training loss: 0.13216, val loss: 3.40487, patience: 140, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 176, training loss: 0.13156, val loss: 3.41225, patience: 141, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 177, training loss: 0.1317, val loss: 3.42195, patience: 142, training metric: 1.0, val metric: 0.0884\n",
            "INFO - root - epoch: 178, training loss: 0.13094, val loss: 3.43232, patience: 143, training metric: 1.0, val metric: 0.0852\n",
            "INFO - root - epoch: 179, training loss: 0.13018, val loss: 3.44724, patience: 144, training metric: 1.0, val metric: 0.0708\n",
            "INFO - root - epoch: 180, training loss: 0.12959, val loss: 3.44183, patience: 145, training metric: 1.0, val metric: 0.0636\n",
            "INFO - root - epoch: 181, training loss: 0.12893, val loss: 3.43697, patience: 146, training metric: 1.0, val metric: 0.058\n",
            "INFO - root - epoch: 182, training loss: 0.12904, val loss: 3.435, patience: 147, training metric: 1.0, val metric: 0.0636\n",
            "INFO - root - epoch: 183, training loss: 0.12899, val loss: 3.42014, patience: 148, training metric: 1.0, val metric: 0.0736\n",
            "INFO - root - epoch: 184, training loss: 0.1276, val loss: 3.40539, patience: 149, training metric: 1.0, val metric: 0.0756\n",
            "INFO - root - epoch: 185, training loss: 0.12717, val loss: 3.38675, patience: 150, training metric: 1.0, val metric: 0.0688\n",
            "INFO - root - epoch: 186, training loss: 0.12662, val loss: 3.34733, patience: 151, training metric: 1.0, val metric: 0.062\n",
            "INFO - root - epoch: 187, training loss: 0.12606, val loss: 3.3107, patience: 152, training metric: 1.0, val metric: 0.0696\n",
            "INFO - root - epoch: 188, training loss: 0.12612, val loss: 3.2905, patience: 153, training metric: 1.0, val metric: 0.0732\n",
            "INFO - root - epoch: 189, training loss: 0.1251, val loss: 3.28168, patience: 154, training metric: 1.0, val metric: 0.0728\n",
            "INFO - root - epoch: 190, training loss: 0.12513, val loss: 3.28261, patience: 155, training metric: 1.0, val metric: 0.0688\n",
            "INFO - root - epoch: 191, training loss: 0.1251, val loss: 3.2974, patience: 156, training metric: 1.0, val metric: 0.0572\n",
            "INFO - root - epoch: 192, training loss: 0.12548, val loss: 3.3073, patience: 157, training metric: 1.0, val metric: 0.0452\n",
            "INFO - root - epoch: 193, training loss: 0.12462, val loss: 3.32085, patience: 158, training metric: 1.0, val metric: 0.0508\n",
            "INFO - root - epoch: 194, training loss: 0.12415, val loss: 3.33359, patience: 159, training metric: 1.0, val metric: 0.0508\n",
            "INFO - root - epoch: 195, training loss: 0.12397, val loss: 3.33591, patience: 160, training metric: 1.0, val metric: 0.0508\n",
            "INFO - root - epoch: 196, training loss: 0.12348, val loss: 3.34517, patience: 161, training metric: 1.0, val metric: 0.058\n",
            "INFO - root - epoch: 197, training loss: 0.12296, val loss: 3.34445, patience: 162, training metric: 1.0, val metric: 0.0576\n",
            "INFO - root - epoch: 198, training loss: 0.12301, val loss: 3.34929, patience: 163, training metric: 1.0, val metric: 0.0652\n",
            "INFO - root - epoch: 199, training loss: 0.12257, val loss: 3.34372, patience: 164, training metric: 1.0, val metric: 0.0636\n",
            "INFO - root - epoch: 200, training loss: 0.12292, val loss: 3.34063, patience: 165, training metric: 1.0, val metric: 0.0624\n",
            "INFO - root - epoch: 201, training loss: 0.1221, val loss: 3.33279, patience: 166, training metric: 1.0, val metric: 0.066\n",
            "INFO - root - epoch: 202, training loss: 0.12207, val loss: 3.3398, patience: 167, training metric: 1.0, val metric: 0.0684\n",
            "INFO - root - epoch: 203, training loss: 0.12189, val loss: 3.33697, patience: 168, training metric: 1.0, val metric: 0.0648\n",
            "INFO - root - epoch: 204, training loss: 0.12154, val loss: 3.3438, patience: 169, training metric: 1.0, val metric: 0.056\n",
            "INFO - root - epoch: 205, training loss: 0.12111, val loss: 3.34808, patience: 170, training metric: 1.0, val metric: 0.046\n",
            "INFO - root - epoch: 206, training loss: 0.12088, val loss: 3.35616, patience: 171, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 207, training loss: 0.12068, val loss: 3.37598, patience: 172, training metric: 1.0, val metric: 0.032\n",
            "INFO - root - epoch: 208, training loss: 0.12064, val loss: 3.37602, patience: 173, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 209, training loss: 0.12022, val loss: 3.37655, patience: 174, training metric: 1.0, val metric: 0.0324\n",
            "INFO - root - epoch: 210, training loss: 0.11969, val loss: 3.38678, patience: 175, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 211, training loss: 0.11946, val loss: 3.37554, patience: 176, training metric: 1.0, val metric: 0.0476\n",
            "INFO - root - epoch: 212, training loss: 0.11974, val loss: 3.38631, patience: 177, training metric: 1.0, val metric: 0.0584\n",
            "INFO - root - epoch: 213, training loss: 0.11993, val loss: 3.38138, patience: 178, training metric: 1.0, val metric: 0.0564\n",
            "INFO - root - epoch: 214, training loss: 0.11957, val loss: 3.382, patience: 179, training metric: 1.0, val metric: 0.066\n",
            "INFO - root - epoch: 215, training loss: 0.1188, val loss: 3.3774, patience: 180, training metric: 1.0, val metric: 0.0784\n",
            "INFO - root - epoch: 216, training loss: 0.11848, val loss: 3.37982, patience: 181, training metric: 1.0, val metric: 0.078\n",
            "INFO - root - epoch: 217, training loss: 0.11805, val loss: 3.37958, patience: 182, training metric: 1.0, val metric: 0.0844\n",
            "INFO - root - epoch: 218, training loss: 0.11803, val loss: 3.36769, patience: 183, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 219, training loss: 0.11738, val loss: 3.38028, patience: 184, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 220, training loss: 0.11741, val loss: 3.37918, patience: 185, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 221, training loss: 0.11692, val loss: 3.379, patience: 186, training metric: 1.0, val metric: 0.0892\n",
            "INFO - root - epoch: 222, training loss: 0.11681, val loss: 3.38621, patience: 187, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 223, training loss: 0.11674, val loss: 3.3812, patience: 188, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 224, training loss: 0.11686, val loss: 3.39079, patience: 189, training metric: 1.0, val metric: 0.0852\n",
            "INFO - root - epoch: 225, training loss: 0.1162, val loss: 3.39691, patience: 190, training metric: 1.0, val metric: 0.0784\n",
            "INFO - root - epoch: 226, training loss: 0.11606, val loss: 3.39932, patience: 191, training metric: 1.0, val metric: 0.0716\n",
            "INFO - root - epoch: 227, training loss: 0.11567, val loss: 3.41434, patience: 192, training metric: 1.0, val metric: 0.0568\n",
            "INFO - root - epoch: 228, training loss: 0.11526, val loss: 3.42152, patience: 193, training metric: 1.0, val metric: 0.0476\n",
            "INFO - root - epoch: 229, training loss: 0.11542, val loss: 3.43008, patience: 194, training metric: 1.0, val metric: 0.0444\n",
            "INFO - root - epoch: 230, training loss: 0.11554, val loss: 3.45144, patience: 195, training metric: 1.0, val metric: 0.032\n",
            "INFO - root - epoch: 231, training loss: 0.11474, val loss: 3.45844, patience: 196, training metric: 1.0, val metric: 0.0248\n",
            "INFO - root - epoch: 232, training loss: 0.11512, val loss: 3.47602, patience: 197, training metric: 1.0, val metric: 0.0188\n",
            "INFO - root - epoch: 233, training loss: 0.11476, val loss: 3.48209, patience: 198, training metric: 1.0, val metric: 0.0148\n",
            "INFO - root - epoch: 234, training loss: 0.11407, val loss: 3.49608, patience: 199, training metric: 1.0, val metric: 0.0156\n",
            "INFO - root - epoch: 235, training loss: 0.11404, val loss: 3.49418, patience: 200, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 236, training loss: 0.114, val loss: 3.50306, patience: 201, training metric: 1.0, val metric: 0.0132\n",
            "INFO - root - epoch: 237, training loss: 0.11333, val loss: 3.4864, patience: 202, training metric: 1.0, val metric: 0.0132\n",
            "INFO - root - epoch: 238, training loss: 0.11349, val loss: 3.47803, patience: 203, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 239, training loss: 0.11298, val loss: 3.47605, patience: 204, training metric: 1.0, val metric: 0.018\n",
            "INFO - root - epoch: 240, training loss: 0.11285, val loss: 3.4634, patience: 205, training metric: 1.0, val metric: 0.0144\n",
            "INFO - root - epoch: 241, training loss: 0.11272, val loss: 3.4566, patience: 206, training metric: 1.0, val metric: 0.0232\n",
            "INFO - root - epoch: 242, training loss: 0.11257, val loss: 3.44638, patience: 207, training metric: 1.0, val metric: 0.0324\n",
            "INFO - root - epoch: 243, training loss: 0.11258, val loss: 3.44288, patience: 208, training metric: 1.0, val metric: 0.048\n",
            "INFO - root - epoch: 244, training loss: 0.11234, val loss: 3.41829, patience: 209, training metric: 1.0, val metric: 0.0656\n",
            "INFO - root - epoch: 245, training loss: 0.11222, val loss: 3.41808, patience: 210, training metric: 1.0, val metric: 0.0772\n",
            "INFO - root - epoch: 246, training loss: 0.112, val loss: 3.39383, patience: 211, training metric: 1.0, val metric: 0.0836\n",
            "INFO - root - epoch: 247, training loss: 0.11181, val loss: 3.39229, patience: 212, training metric: 1.0, val metric: 0.088\n",
            "INFO - root - epoch: 248, training loss: 0.11149, val loss: 3.38777, patience: 213, training metric: 1.0, val metric: 0.088\n",
            "INFO - root - epoch: 249, training loss: 0.11185, val loss: 3.38424, patience: 214, training metric: 1.0, val metric: 0.0904\n",
            "INFO - root - epoch: 250, training loss: 0.11148, val loss: 3.38829, patience: 215, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 251, training loss: 0.11122, val loss: 3.39622, patience: 216, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 252, training loss: 0.11115, val loss: 3.38601, patience: 217, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 253, training loss: 0.1112, val loss: 3.38761, patience: 218, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 254, training loss: 0.11089, val loss: 3.3951, patience: 219, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 255, training loss: 0.1108, val loss: 3.38651, patience: 220, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 256, training loss: 0.1107, val loss: 3.38579, patience: 221, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 257, training loss: 0.11069, val loss: 3.40115, patience: 222, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 258, training loss: 0.11052, val loss: 3.39603, patience: 223, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 259, training loss: 0.11054, val loss: 3.39076, patience: 224, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 260, training loss: 0.11023, val loss: 3.39993, patience: 225, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 261, training loss: 0.11036, val loss: 3.40653, patience: 226, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 262, training loss: 0.11025, val loss: 3.40818, patience: 227, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 263, training loss: 0.10962, val loss: 3.41526, patience: 228, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 264, training loss: 0.10992, val loss: 3.41365, patience: 229, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 265, training loss: 0.10954, val loss: 3.41939, patience: 230, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 266, training loss: 0.10989, val loss: 3.42859, patience: 231, training metric: 1.0, val metric: 0.0868\n",
            "INFO - root - epoch: 267, training loss: 0.10941, val loss: 3.43129, patience: 232, training metric: 1.0, val metric: 0.086\n",
            "INFO - root - epoch: 268, training loss: 0.1092, val loss: 3.44291, patience: 233, training metric: 1.0, val metric: 0.0796\n",
            "INFO - root - epoch: 269, training loss: 0.10971, val loss: 3.44371, patience: 234, training metric: 1.0, val metric: 0.0716\n",
            "INFO - root - epoch: 270, training loss: 0.10943, val loss: 3.43797, patience: 235, training metric: 1.0, val metric: 0.0716\n",
            "INFO - root - epoch: 271, training loss: 0.10948, val loss: 3.43984, patience: 236, training metric: 1.0, val metric: 0.0692\n",
            "INFO - root - epoch: 272, training loss: 0.1088, val loss: 3.43185, patience: 237, training metric: 1.0, val metric: 0.0692\n",
            "INFO - root - epoch: 273, training loss: 0.10892, val loss: 3.42998, patience: 238, training metric: 1.0, val metric: 0.0628\n",
            "INFO - root - epoch: 274, training loss: 0.10841, val loss: 3.42325, patience: 239, training metric: 1.0, val metric: 0.0664\n",
            "INFO - root - epoch: 275, training loss: 0.1088, val loss: 3.4324, patience: 240, training metric: 1.0, val metric: 0.0608\n",
            "INFO - root - epoch: 276, training loss: 0.10858, val loss: 3.43698, patience: 241, training metric: 1.0, val metric: 0.0588\n",
            "INFO - root - epoch: 277, training loss: 0.10855, val loss: 3.41876, patience: 242, training metric: 1.0, val metric: 0.068\n",
            "INFO - root - epoch: 278, training loss: 0.10829, val loss: 3.41326, patience: 243, training metric: 1.0, val metric: 0.0712\n",
            "INFO - root - epoch: 279, training loss: 0.10853, val loss: 3.4006, patience: 244, training metric: 1.0, val metric: 0.0712\n",
            "INFO - root - epoch: 280, training loss: 0.10818, val loss: 3.39679, patience: 245, training metric: 1.0, val metric: 0.0736\n",
            "INFO - root - epoch: 281, training loss: 0.10811, val loss: 3.39506, patience: 246, training metric: 1.0, val metric: 0.0844\n",
            "INFO - root - epoch: 282, training loss: 0.10794, val loss: 3.38491, patience: 247, training metric: 1.0, val metric: 0.0856\n",
            "INFO - root - epoch: 283, training loss: 0.10759, val loss: 3.37072, patience: 248, training metric: 1.0, val metric: 0.0864\n",
            "INFO - root - epoch: 284, training loss: 0.10769, val loss: 3.37432, patience: 249, training metric: 1.0, val metric: 0.0892\n",
            "INFO - root - epoch: 285, training loss: 0.10779, val loss: 3.37732, patience: 250, training metric: 1.0, val metric: 0.088\n",
            "INFO - root - epoch: 286, training loss: 0.10787, val loss: 3.38164, patience: 251, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 287, training loss: 0.10771, val loss: 3.3761, patience: 252, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 288, training loss: 0.10741, val loss: 3.37428, patience: 253, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 289, training loss: 0.10745, val loss: 3.38085, patience: 254, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 290, training loss: 0.10688, val loss: 3.36545, patience: 255, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 291, training loss: 0.10679, val loss: 3.37677, patience: 256, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 292, training loss: 0.10687, val loss: 3.37213, patience: 257, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 293, training loss: 0.10659, val loss: 3.36611, patience: 258, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 294, training loss: 0.107, val loss: 3.37824, patience: 259, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 295, training loss: 0.10749, val loss: 3.37356, patience: 260, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 296, training loss: 0.10697, val loss: 3.38326, patience: 261, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 297, training loss: 0.10673, val loss: 3.38107, patience: 262, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 298, training loss: 0.10646, val loss: 3.38176, patience: 263, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 299, training loss: 0.10651, val loss: 3.3835, patience: 264, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 300, training loss: 0.10627, val loss: 3.383, patience: 265, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 301, training loss: 0.10637, val loss: 3.38878, patience: 266, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 302, training loss: 0.10624, val loss: 3.3959, patience: 267, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 303, training loss: 0.106, val loss: 3.40602, patience: 268, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 304, training loss: 0.1059, val loss: 3.40531, patience: 269, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 305, training loss: 0.1062, val loss: 3.4036, patience: 270, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 306, training loss: 0.10568, val loss: 3.41893, patience: 271, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 307, training loss: 0.10595, val loss: 3.41682, patience: 272, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 308, training loss: 0.10575, val loss: 3.42204, patience: 273, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 309, training loss: 0.10558, val loss: 3.42115, patience: 274, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 310, training loss: 0.10592, val loss: 3.42772, patience: 275, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 311, training loss: 0.10548, val loss: 3.42616, patience: 276, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 312, training loss: 0.10559, val loss: 3.4343, patience: 277, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 313, training loss: 0.10513, val loss: 3.43228, patience: 278, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 314, training loss: 0.10505, val loss: 3.43883, patience: 279, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 315, training loss: 0.10541, val loss: 3.44849, patience: 280, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 316, training loss: 0.10534, val loss: 3.44186, patience: 281, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 317, training loss: 0.10522, val loss: 3.43671, patience: 282, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 318, training loss: 0.10487, val loss: 3.4395, patience: 283, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 319, training loss: 0.10479, val loss: 3.43212, patience: 284, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 320, training loss: 0.10478, val loss: 3.43852, patience: 285, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 321, training loss: 0.10475, val loss: 3.43547, patience: 286, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 322, training loss: 0.10467, val loss: 3.43581, patience: 287, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 323, training loss: 0.10464, val loss: 3.43177, patience: 288, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 324, training loss: 0.10468, val loss: 3.43878, patience: 289, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 325, training loss: 0.10469, val loss: 3.43247, patience: 290, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 326, training loss: 0.10496, val loss: 3.44103, patience: 291, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 327, training loss: 0.10455, val loss: 3.4431, patience: 292, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 328, training loss: 0.10473, val loss: 3.44989, patience: 293, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 329, training loss: 0.10442, val loss: 3.4476, patience: 294, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 330, training loss: 0.10444, val loss: 3.45162, patience: 295, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 331, training loss: 0.10455, val loss: 3.44848, patience: 296, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 332, training loss: 0.10454, val loss: 3.45235, patience: 297, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 333, training loss: 0.10452, val loss: 3.45885, patience: 298, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 334, training loss: 0.10462, val loss: 3.45511, patience: 299, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 335, training loss: 0.10449, val loss: 3.44356, patience: 300, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 336, training loss: 0.10434, val loss: 3.44231, patience: 301, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 337, training loss: 0.10428, val loss: 3.45103, patience: 302, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 338, training loss: 0.10467, val loss: 3.44284, patience: 303, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 339, training loss: 0.10424, val loss: 3.44471, patience: 304, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 340, training loss: 0.1042, val loss: 3.44302, patience: 305, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 341, training loss: 0.10412, val loss: 3.44321, patience: 306, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 342, training loss: 0.10393, val loss: 3.44522, patience: 307, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 343, training loss: 0.10378, val loss: 3.44341, patience: 308, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 344, training loss: 0.10395, val loss: 3.44602, patience: 309, training metric: 1.0, val metric: 0.092\n",
            "INFO - root - epoch: 345, training loss: 0.10357, val loss: 3.4466, patience: 310, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 346, training loss: 0.10383, val loss: 3.44711, patience: 311, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 347, training loss: 0.10367, val loss: 3.45094, patience: 312, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 348, training loss: 0.10335, val loss: 3.44218, patience: 313, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 349, training loss: 0.1035, val loss: 3.44432, patience: 314, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 350, training loss: 0.10351, val loss: 3.44742, patience: 315, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 351, training loss: 0.10334, val loss: 3.43871, patience: 316, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 352, training loss: 0.10335, val loss: 3.44868, patience: 317, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 353, training loss: 0.10354, val loss: 3.4461, patience: 318, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 354, training loss: 0.10338, val loss: 3.4445, patience: 319, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 355, training loss: 0.10351, val loss: 3.43657, patience: 320, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 356, training loss: 0.10373, val loss: 3.44682, patience: 321, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 357, training loss: 0.10326, val loss: 3.445, patience: 322, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 358, training loss: 0.10362, val loss: 3.44652, patience: 323, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 359, training loss: 0.10311, val loss: 3.44341, patience: 324, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 360, training loss: 0.10325, val loss: 3.44425, patience: 325, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 361, training loss: 0.1034, val loss: 3.44583, patience: 326, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 362, training loss: 0.10325, val loss: 3.44395, patience: 327, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 363, training loss: 0.10311, val loss: 3.44586, patience: 328, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 364, training loss: 0.10326, val loss: 3.44548, patience: 329, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 365, training loss: 0.10314, val loss: 3.44577, patience: 330, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 366, training loss: 0.103, val loss: 3.44827, patience: 331, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 367, training loss: 0.10285, val loss: 3.45391, patience: 332, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 368, training loss: 0.10281, val loss: 3.45104, patience: 333, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 369, training loss: 0.10298, val loss: 3.45131, patience: 334, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 370, training loss: 0.10263, val loss: 3.45118, patience: 335, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 371, training loss: 0.10275, val loss: 3.44741, patience: 336, training metric: 1.0, val metric: 0.092\n",
            "INFO - root - epoch: 372, training loss: 0.10278, val loss: 3.45339, patience: 337, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 373, training loss: 0.1027, val loss: 3.45487, patience: 338, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 374, training loss: 0.10259, val loss: 3.44927, patience: 339, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 375, training loss: 0.10281, val loss: 3.45323, patience: 340, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 376, training loss: 0.10277, val loss: 3.44322, patience: 341, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 377, training loss: 0.10246, val loss: 3.45139, patience: 342, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 378, training loss: 0.10259, val loss: 3.45565, patience: 343, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 379, training loss: 0.1025, val loss: 3.45326, patience: 344, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 380, training loss: 0.10264, val loss: 3.46161, patience: 345, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 381, training loss: 0.10281, val loss: 3.45011, patience: 346, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 382, training loss: 0.10266, val loss: 3.44726, patience: 347, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 383, training loss: 0.10242, val loss: 3.45153, patience: 348, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 384, training loss: 0.10258, val loss: 3.45283, patience: 349, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 385, training loss: 0.10259, val loss: 3.45356, patience: 350, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 386, training loss: 0.10244, val loss: 3.4576, patience: 351, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 387, training loss: 0.10279, val loss: 3.46284, patience: 352, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 388, training loss: 0.10246, val loss: 3.4643, patience: 353, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 389, training loss: 0.10268, val loss: 3.45868, patience: 354, training metric: 1.0, val metric: 0.092\n",
            "INFO - root - epoch: 390, training loss: 0.1024, val loss: 3.46451, patience: 355, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 391, training loss: 0.10274, val loss: 3.45887, patience: 356, training metric: 1.0, val metric: 0.0892\n",
            "INFO - root - epoch: 392, training loss: 0.10211, val loss: 3.46436, patience: 357, training metric: 1.0, val metric: 0.09\n",
            "INFO - root - epoch: 393, training loss: 0.10238, val loss: 3.45949, patience: 358, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 394, training loss: 0.10247, val loss: 3.46026, patience: 359, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 395, training loss: 0.10227, val loss: 3.46265, patience: 360, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 396, training loss: 0.10234, val loss: 3.46097, patience: 361, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 397, training loss: 0.10233, val loss: 3.46052, patience: 362, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 398, training loss: 0.10223, val loss: 3.46328, patience: 363, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 399, training loss: 0.10203, val loss: 3.46602, patience: 364, training metric: 1.0, val metric: 0.092\n",
            "INFO - root - epoch: 400, training loss: 0.10224, val loss: 3.46243, patience: 365, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 401, training loss: 0.10197, val loss: 3.4629, patience: 366, training metric: 1.0, val metric: 0.0904\n",
            "INFO - root - epoch: 402, training loss: 0.10205, val loss: 3.45825, patience: 367, training metric: 1.0, val metric: 0.0892\n",
            "INFO - root - epoch: 403, training loss: 0.10194, val loss: 3.46049, patience: 368, training metric: 1.0, val metric: 0.0876\n",
            "INFO - root - epoch: 404, training loss: 0.10193, val loss: 3.46124, patience: 369, training metric: 1.0, val metric: 0.09\n",
            "INFO - root - epoch: 405, training loss: 0.10198, val loss: 3.45456, patience: 370, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 406, training loss: 0.10167, val loss: 3.45968, patience: 371, training metric: 1.0, val metric: 0.0904\n",
            "INFO - root - epoch: 407, training loss: 0.10179, val loss: 3.4644, patience: 372, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 408, training loss: 0.10172, val loss: 3.45824, patience: 373, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 409, training loss: 0.10156, val loss: 3.4548, patience: 374, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 410, training loss: 0.10182, val loss: 3.44864, patience: 375, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 411, training loss: 0.10173, val loss: 3.4593, patience: 376, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 412, training loss: 0.10167, val loss: 3.45534, patience: 377, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 413, training loss: 0.1017, val loss: 3.44741, patience: 378, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 414, training loss: 0.10171, val loss: 3.45116, patience: 379, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 415, training loss: 0.10127, val loss: 3.44881, patience: 380, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 416, training loss: 0.10182, val loss: 3.45534, patience: 381, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 417, training loss: 0.10182, val loss: 3.45573, patience: 382, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 418, training loss: 0.1017, val loss: 3.45089, patience: 383, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 419, training loss: 0.10147, val loss: 3.44668, patience: 384, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 420, training loss: 0.1017, val loss: 3.44815, patience: 385, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 421, training loss: 0.10171, val loss: 3.44936, patience: 386, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 422, training loss: 0.10139, val loss: 3.44352, patience: 387, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 423, training loss: 0.10173, val loss: 3.45102, patience: 388, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 424, training loss: 0.10146, val loss: 3.45358, patience: 389, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 425, training loss: 0.10165, val loss: 3.43855, patience: 390, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 426, training loss: 0.1016, val loss: 3.45013, patience: 391, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 427, training loss: 0.1017, val loss: 3.44465, patience: 392, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 428, training loss: 0.10143, val loss: 3.44294, patience: 393, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 429, training loss: 0.10139, val loss: 3.44708, patience: 394, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 430, training loss: 0.1015, val loss: 3.44387, patience: 395, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 431, training loss: 0.10161, val loss: 3.45744, patience: 396, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 432, training loss: 0.10154, val loss: 3.44849, patience: 397, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 433, training loss: 0.10158, val loss: 3.44956, patience: 398, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 434, training loss: 0.10144, val loss: 3.45182, patience: 399, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 435, training loss: 0.10155, val loss: 3.44541, patience: 400, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 436, training loss: 0.10151, val loss: 3.45556, patience: 401, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 437, training loss: 0.10155, val loss: 3.44808, patience: 402, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 438, training loss: 0.10148, val loss: 3.44915, patience: 403, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 439, training loss: 0.10143, val loss: 3.4583, patience: 404, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 440, training loss: 0.10172, val loss: 3.44911, patience: 405, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 441, training loss: 0.10156, val loss: 3.45733, patience: 406, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 442, training loss: 0.1016, val loss: 3.45973, patience: 407, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 443, training loss: 0.10115, val loss: 3.45696, patience: 408, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 444, training loss: 0.10146, val loss: 3.45364, patience: 409, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 445, training loss: 0.10146, val loss: 3.45872, patience: 410, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 446, training loss: 0.10133, val loss: 3.447, patience: 411, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 447, training loss: 0.10152, val loss: 3.45854, patience: 412, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 448, training loss: 0.10144, val loss: 3.45993, patience: 413, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 449, training loss: 0.10094, val loss: 3.46276, patience: 414, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 450, training loss: 0.10146, val loss: 3.45764, patience: 415, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 451, training loss: 0.10133, val loss: 3.44756, patience: 416, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 452, training loss: 0.10124, val loss: 3.45058, patience: 417, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 453, training loss: 0.10098, val loss: 3.4489, patience: 418, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 454, training loss: 0.1011, val loss: 3.45439, patience: 419, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 455, training loss: 0.10107, val loss: 3.45096, patience: 420, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 456, training loss: 0.10094, val loss: 3.46048, patience: 421, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 457, training loss: 0.10124, val loss: 3.44451, patience: 422, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 458, training loss: 0.10098, val loss: 3.4534, patience: 423, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 459, training loss: 0.10106, val loss: 3.4476, patience: 424, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 460, training loss: 0.10084, val loss: 3.4567, patience: 425, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 461, training loss: 0.10105, val loss: 3.45302, patience: 426, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 462, training loss: 0.10109, val loss: 3.44895, patience: 427, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 463, training loss: 0.101, val loss: 3.44867, patience: 428, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 464, training loss: 0.10105, val loss: 3.45278, patience: 429, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 465, training loss: 0.10077, val loss: 3.44914, patience: 430, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 466, training loss: 0.10084, val loss: 3.44895, patience: 431, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 467, training loss: 0.10104, val loss: 3.45222, patience: 432, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 468, training loss: 0.10091, val loss: 3.44905, patience: 433, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 469, training loss: 0.10113, val loss: 3.45741, patience: 434, training metric: 1.0, val metric: 0.0904\n",
            "INFO - root - epoch: 470, training loss: 0.10079, val loss: 3.45596, patience: 435, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 471, training loss: 0.10065, val loss: 3.44875, patience: 436, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 472, training loss: 0.10118, val loss: 3.45476, patience: 437, training metric: 1.0, val metric: 0.092\n",
            "INFO - root - epoch: 473, training loss: 0.10084, val loss: 3.45063, patience: 438, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 474, training loss: 0.10081, val loss: 3.45338, patience: 439, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 475, training loss: 0.10071, val loss: 3.45633, patience: 440, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 476, training loss: 0.10063, val loss: 3.45949, patience: 441, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 477, training loss: 0.10081, val loss: 3.4508, patience: 442, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 478, training loss: 0.10081, val loss: 3.45271, patience: 443, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 479, training loss: 0.10063, val loss: 3.45242, patience: 444, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 480, training loss: 0.10069, val loss: 3.45579, patience: 445, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 481, training loss: 0.1003, val loss: 3.4551, patience: 446, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 482, training loss: 0.10063, val loss: 3.46121, patience: 447, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 483, training loss: 0.10054, val loss: 3.45413, patience: 448, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 484, training loss: 0.10035, val loss: 3.45635, patience: 449, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 485, training loss: 0.1005, val loss: 3.45274, patience: 450, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 486, training loss: 0.10045, val loss: 3.45869, patience: 451, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 487, training loss: 0.10037, val loss: 3.46299, patience: 452, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 488, training loss: 0.10058, val loss: 3.45437, patience: 453, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 489, training loss: 0.10043, val loss: 3.46125, patience: 454, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 490, training loss: 0.10061, val loss: 3.45606, patience: 455, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 491, training loss: 0.1003, val loss: 3.45746, patience: 456, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 492, training loss: 0.10024, val loss: 3.44699, patience: 457, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 493, training loss: 0.10029, val loss: 3.45473, patience: 458, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 494, training loss: 0.10017, val loss: 3.44883, patience: 459, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 495, training loss: 0.10021, val loss: 3.45316, patience: 460, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 496, training loss: 0.10012, val loss: 3.45088, patience: 461, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 497, training loss: 0.10014, val loss: 3.44315, patience: 462, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 498, training loss: 0.10021, val loss: 3.45123, patience: 463, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 499, training loss: 0.10006, val loss: 3.44304, patience: 464, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 500, training loss: 0.10002, val loss: 3.45129, patience: 465, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 501, training loss: 0.10034, val loss: 3.44488, patience: 466, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 502, training loss: 0.10038, val loss: 3.44848, patience: 467, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 503, training loss: 0.10042, val loss: 3.44402, patience: 468, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 504, training loss: 0.10046, val loss: 3.44678, patience: 469, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 505, training loss: 0.09996, val loss: 3.45056, patience: 470, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 506, training loss: 0.1002, val loss: 3.44214, patience: 471, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 507, training loss: 0.10037, val loss: 3.45359, patience: 472, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 508, training loss: 0.10037, val loss: 3.45164, patience: 473, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 509, training loss: 0.10023, val loss: 3.44998, patience: 474, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 510, training loss: 0.10016, val loss: 3.44785, patience: 475, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 511, training loss: 0.10034, val loss: 3.45813, patience: 476, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 512, training loss: 0.10007, val loss: 3.45607, patience: 477, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 513, training loss: 0.09989, val loss: 3.46364, patience: 478, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 514, training loss: 0.09985, val loss: 3.45452, patience: 479, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 515, training loss: 0.09993, val loss: 3.45271, patience: 480, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 516, training loss: 0.10003, val loss: 3.45388, patience: 481, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 517, training loss: 0.10012, val loss: 3.45602, patience: 482, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 518, training loss: 0.10026, val loss: 3.46038, patience: 483, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 519, training loss: 0.1, val loss: 3.46187, patience: 484, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 520, training loss: 0.09998, val loss: 3.46567, patience: 485, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 521, training loss: 0.09965, val loss: 3.46355, patience: 486, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 522, training loss: 0.09983, val loss: 3.46574, patience: 487, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 523, training loss: 0.09978, val loss: 3.46171, patience: 488, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 524, training loss: 0.09988, val loss: 3.46217, patience: 489, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 525, training loss: 0.09953, val loss: 3.45988, patience: 490, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 526, training loss: 0.09976, val loss: 3.46508, patience: 491, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 527, training loss: 0.09965, val loss: 3.47084, patience: 492, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 528, training loss: 0.09956, val loss: 3.46675, patience: 493, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 529, training loss: 0.09961, val loss: 3.46337, patience: 494, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 530, training loss: 0.09951, val loss: 3.4599, patience: 495, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 531, training loss: 0.09964, val loss: 3.46372, patience: 496, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 532, training loss: 0.09965, val loss: 3.45291, patience: 497, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 533, training loss: 0.09972, val loss: 3.47473, patience: 498, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 534, training loss: 0.09973, val loss: 3.4579, patience: 499, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 535, training loss: 0.09936, val loss: 3.468, patience: 500, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 536, training loss: 0.09958, val loss: 3.46209, patience: 501, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 537, training loss: 0.09944, val loss: 3.45786, patience: 502, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 538, training loss: 0.09924, val loss: 3.4636, patience: 503, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 539, training loss: 0.09937, val loss: 3.46028, patience: 504, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 540, training loss: 0.0997, val loss: 3.45745, patience: 505, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 541, training loss: 0.09962, val loss: 3.46007, patience: 506, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 542, training loss: 0.09951, val loss: 3.45908, patience: 507, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 543, training loss: 0.09969, val loss: 3.47404, patience: 508, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 544, training loss: 0.09926, val loss: 3.46245, patience: 509, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 545, training loss: 0.0996, val loss: 3.46932, patience: 510, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 546, training loss: 0.09904, val loss: 3.45803, patience: 511, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 547, training loss: 0.09945, val loss: 3.46404, patience: 512, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 548, training loss: 0.099, val loss: 3.46826, patience: 513, training metric: 1.0, val metric: 0.088\n",
            "INFO - root - epoch: 549, training loss: 0.09908, val loss: 3.46888, patience: 514, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 550, training loss: 0.09887, val loss: 3.47827, patience: 515, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 551, training loss: 0.0992, val loss: 3.46961, patience: 516, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 552, training loss: 0.09917, val loss: 3.47618, patience: 517, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 553, training loss: 0.09907, val loss: 3.47289, patience: 518, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 554, training loss: 0.09905, val loss: 3.46869, patience: 519, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 555, training loss: 0.09913, val loss: 3.46754, patience: 520, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 556, training loss: 0.09902, val loss: 3.47125, patience: 521, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 557, training loss: 0.09889, val loss: 3.47203, patience: 522, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 558, training loss: 0.09879, val loss: 3.46933, patience: 523, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 559, training loss: 0.09901, val loss: 3.47672, patience: 524, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 560, training loss: 0.09887, val loss: 3.46319, patience: 525, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 561, training loss: 0.0988, val loss: 3.47757, patience: 526, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 562, training loss: 0.09864, val loss: 3.46887, patience: 527, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 563, training loss: 0.09875, val loss: 3.46772, patience: 528, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 564, training loss: 0.09914, val loss: 3.45556, patience: 529, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 565, training loss: 0.09881, val loss: 3.46375, patience: 530, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 566, training loss: 0.09905, val loss: 3.46803, patience: 531, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 567, training loss: 0.09909, val loss: 3.45512, patience: 532, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 568, training loss: 0.09858, val loss: 3.46738, patience: 533, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 569, training loss: 0.09899, val loss: 3.46627, patience: 534, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 570, training loss: 0.09861, val loss: 3.45957, patience: 535, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 571, training loss: 0.09872, val loss: 3.46192, patience: 536, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 572, training loss: 0.09853, val loss: 3.46235, patience: 537, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 573, training loss: 0.09853, val loss: 3.46021, patience: 538, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 574, training loss: 0.09841, val loss: 3.4498, patience: 539, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 575, training loss: 0.09849, val loss: 3.45281, patience: 540, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 576, training loss: 0.09815, val loss: 3.45965, patience: 541, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 577, training loss: 0.09851, val loss: 3.451, patience: 542, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 578, training loss: 0.09839, val loss: 3.44989, patience: 543, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 579, training loss: 0.09865, val loss: 3.44894, patience: 544, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 580, training loss: 0.09844, val loss: 3.45042, patience: 545, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 581, training loss: 0.09821, val loss: 3.45354, patience: 546, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 582, training loss: 0.09832, val loss: 3.45316, patience: 547, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 583, training loss: 0.09821, val loss: 3.45134, patience: 548, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 584, training loss: 0.09841, val loss: 3.45405, patience: 549, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 585, training loss: 0.0982, val loss: 3.45251, patience: 550, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 586, training loss: 0.09809, val loss: 3.44884, patience: 551, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 587, training loss: 0.09846, val loss: 3.46274, patience: 552, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 588, training loss: 0.09826, val loss: 3.46023, patience: 553, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 589, training loss: 0.09825, val loss: 3.45797, patience: 554, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 590, training loss: 0.09799, val loss: 3.45957, patience: 555, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 591, training loss: 0.09812, val loss: 3.4651, patience: 556, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 592, training loss: 0.09814, val loss: 3.46589, patience: 557, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 593, training loss: 0.09806, val loss: 3.45864, patience: 558, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 594, training loss: 0.0982, val loss: 3.46101, patience: 559, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 595, training loss: 0.09828, val loss: 3.45926, patience: 560, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 596, training loss: 0.09813, val loss: 3.45435, patience: 561, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 597, training loss: 0.09799, val loss: 3.46309, patience: 562, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 598, training loss: 0.09829, val loss: 3.46155, patience: 563, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 599, training loss: 0.09829, val loss: 3.46118, patience: 564, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 600, training loss: 0.09793, val loss: 3.45784, patience: 565, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 601, training loss: 0.09811, val loss: 3.45495, patience: 566, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 602, training loss: 0.09797, val loss: 3.46471, patience: 567, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 603, training loss: 0.09801, val loss: 3.45752, patience: 568, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 604, training loss: 0.09802, val loss: 3.46523, patience: 569, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 605, training loss: 0.09813, val loss: 3.46705, patience: 570, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 606, training loss: 0.09816, val loss: 3.45321, patience: 571, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 607, training loss: 0.09776, val loss: 3.46181, patience: 572, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 608, training loss: 0.09775, val loss: 3.46084, patience: 573, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 609, training loss: 0.09789, val loss: 3.45684, patience: 574, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 610, training loss: 0.09771, val loss: 3.45621, patience: 575, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 611, training loss: 0.0977, val loss: 3.4488, patience: 576, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 612, training loss: 0.09781, val loss: 3.45585, patience: 577, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 613, training loss: 0.09796, val loss: 3.45092, patience: 578, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 614, training loss: 0.09771, val loss: 3.45057, patience: 579, training metric: 1.0, val metric: 0.092\n",
            "INFO - root - epoch: 615, training loss: 0.09769, val loss: 3.44456, patience: 580, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 616, training loss: 0.09766, val loss: 3.4566, patience: 581, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 617, training loss: 0.09787, val loss: 3.45361, patience: 582, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 618, training loss: 0.09766, val loss: 3.44759, patience: 583, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 619, training loss: 0.09756, val loss: 3.44913, patience: 584, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 620, training loss: 0.09796, val loss: 3.452, patience: 585, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 621, training loss: 0.09752, val loss: 3.46324, patience: 586, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 622, training loss: 0.09768, val loss: 3.45931, patience: 587, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 623, training loss: 0.09781, val loss: 3.45383, patience: 588, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 624, training loss: 0.09762, val loss: 3.46286, patience: 589, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 625, training loss: 0.09759, val loss: 3.46211, patience: 590, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 626, training loss: 0.09773, val loss: 3.4632, patience: 591, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 627, training loss: 0.09757, val loss: 3.46543, patience: 592, training metric: 1.0, val metric: 0.0928\n",
            "INFO - root - epoch: 628, training loss: 0.09732, val loss: 3.4656, patience: 593, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 629, training loss: 0.09731, val loss: 3.4651, patience: 594, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 630, training loss: 0.09731, val loss: 3.46975, patience: 595, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 631, training loss: 0.09768, val loss: 3.47215, patience: 596, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 632, training loss: 0.0975, val loss: 3.46825, patience: 597, training metric: 1.0, val metric: 0.0892\n",
            "INFO - root - epoch: 633, training loss: 0.09771, val loss: 3.47459, patience: 598, training metric: 1.0, val metric: 0.088\n",
            "INFO - root - epoch: 634, training loss: 0.09763, val loss: 3.47314, patience: 599, training metric: 1.0, val metric: 0.09\n",
            "INFO - root - epoch: 635, training loss: 0.09745, val loss: 3.48066, patience: 600, training metric: 1.0, val metric: 0.0892\n",
            "INFO - root - epoch: 636, training loss: 0.09768, val loss: 3.47473, patience: 601, training metric: 1.0, val metric: 0.0888\n",
            "INFO - root - epoch: 637, training loss: 0.09729, val loss: 3.47112, patience: 602, training metric: 1.0, val metric: 0.0896\n",
            "INFO - root - epoch: 638, training loss: 0.09721, val loss: 3.47113, patience: 603, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 639, training loss: 0.09718, val loss: 3.4642, patience: 604, training metric: 1.0, val metric: 0.0888\n",
            "INFO - root - epoch: 640, training loss: 0.09713, val loss: 3.48253, patience: 605, training metric: 1.0, val metric: 0.0916\n",
            "INFO - root - epoch: 641, training loss: 0.09735, val loss: 3.47261, patience: 606, training metric: 1.0, val metric: 0.0892\n",
            "INFO - root - epoch: 642, training loss: 0.09712, val loss: 3.46808, patience: 607, training metric: 1.0, val metric: 0.0888\n",
            "INFO - root - epoch: 643, training loss: 0.09693, val loss: 3.47276, patience: 608, training metric: 1.0, val metric: 0.088\n",
            "INFO - root - epoch: 644, training loss: 0.09702, val loss: 3.47499, patience: 609, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 645, training loss: 0.09705, val loss: 3.47009, patience: 610, training metric: 1.0, val metric: 0.09\n",
            "INFO - root - epoch: 646, training loss: 0.09721, val loss: 3.47416, patience: 611, training metric: 1.0, val metric: 0.088\n",
            "INFO - root - epoch: 647, training loss: 0.09692, val loss: 3.46733, patience: 612, training metric: 1.0, val metric: 0.0876\n",
            "INFO - root - epoch: 648, training loss: 0.09712, val loss: 3.47847, patience: 613, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 649, training loss: 0.09716, val loss: 3.4682, patience: 614, training metric: 1.0, val metric: 0.0904\n",
            "INFO - root - epoch: 650, training loss: 0.09699, val loss: 3.46402, patience: 615, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 651, training loss: 0.09715, val loss: 3.47111, patience: 616, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 652, training loss: 0.09699, val loss: 3.46459, patience: 617, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 653, training loss: 0.0973, val loss: 3.45999, patience: 618, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 654, training loss: 0.09724, val loss: 3.45626, patience: 619, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 655, training loss: 0.0973, val loss: 3.46188, patience: 620, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 656, training loss: 0.09714, val loss: 3.4701, patience: 621, training metric: 1.0, val metric: 0.092\n",
            "INFO - root - epoch: 657, training loss: 0.09703, val loss: 3.46641, patience: 622, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 658, training loss: 0.09711, val loss: 3.45874, patience: 623, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 659, training loss: 0.09721, val loss: 3.46496, patience: 624, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 660, training loss: 0.09702, val loss: 3.46402, patience: 625, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 661, training loss: 0.09698, val loss: 3.46825, patience: 626, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 662, training loss: 0.09701, val loss: 3.46092, patience: 627, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 663, training loss: 0.09685, val loss: 3.46182, patience: 628, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 664, training loss: 0.09702, val loss: 3.47083, patience: 629, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 665, training loss: 0.09701, val loss: 3.4656, patience: 630, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 666, training loss: 0.09714, val loss: 3.45368, patience: 631, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 667, training loss: 0.09691, val loss: 3.47377, patience: 632, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 668, training loss: 0.09679, val loss: 3.46996, patience: 633, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 669, training loss: 0.0968, val loss: 3.46398, patience: 634, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 670, training loss: 0.09653, val loss: 3.46378, patience: 635, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 671, training loss: 0.09681, val loss: 3.47137, patience: 636, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 672, training loss: 0.09663, val loss: 3.46861, patience: 637, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 673, training loss: 0.09636, val loss: 3.47278, patience: 638, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 674, training loss: 0.09643, val loss: 3.46889, patience: 639, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 675, training loss: 0.09639, val loss: 3.47214, patience: 640, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 676, training loss: 0.09658, val loss: 3.47399, patience: 641, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 677, training loss: 0.09637, val loss: 3.46727, patience: 642, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 678, training loss: 0.09641, val loss: 3.47156, patience: 643, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 679, training loss: 0.09663, val loss: 3.46949, patience: 644, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 680, training loss: 0.09646, val loss: 3.46498, patience: 645, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 681, training loss: 0.09651, val loss: 3.47417, patience: 646, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 682, training loss: 0.09642, val loss: 3.47468, patience: 647, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 683, training loss: 0.09621, val loss: 3.47066, patience: 648, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 684, training loss: 0.09643, val loss: 3.46575, patience: 649, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 685, training loss: 0.09651, val loss: 3.46591, patience: 650, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 686, training loss: 0.09629, val loss: 3.47029, patience: 651, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 687, training loss: 0.09628, val loss: 3.47236, patience: 652, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 688, training loss: 0.09628, val loss: 3.47533, patience: 653, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 689, training loss: 0.09612, val loss: 3.4733, patience: 654, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 690, training loss: 0.09617, val loss: 3.47354, patience: 655, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 691, training loss: 0.09639, val loss: 3.46568, patience: 656, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 692, training loss: 0.09623, val loss: 3.47334, patience: 657, training metric: 1.0, val metric: 0.096\n",
            "INFO - root - epoch: 693, training loss: 0.09616, val loss: 3.4711, patience: 658, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 694, training loss: 0.09624, val loss: 3.46123, patience: 659, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 695, training loss: 0.09637, val loss: 3.46808, patience: 660, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 696, training loss: 0.09612, val loss: 3.46275, patience: 661, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 697, training loss: 0.09625, val loss: 3.46758, patience: 662, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 698, training loss: 0.09605, val loss: 3.46855, patience: 663, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 699, training loss: 0.09617, val loss: 3.4561, patience: 664, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - loaded best model at epoch 35\n",
            "INFO - root - Best val loss: 2.309001922607422\n",
            "INFO - root - Best val metric: 0.1204\n",
            "INFO - root - test loss: 2.4086802005767822\n",
            "INFO - root - test metric: 0.1308\n",
            "INFO - root - epoch: 0, training loss: 2.29424, val loss: 2.3113, patience: 0, training metric: 0.0628, val metric: 0.1\n",
            "INFO - root - epoch: 1, training loss: 2.16005, val loss: 2.31122, patience: 1, training metric: 0.1348, val metric: 0.1\n",
            "INFO - root - epoch: 2, training loss: 2.08632, val loss: 2.31139, patience: 2, training metric: 0.2096, val metric: 0.1\n",
            "INFO - root - epoch: 3, training loss: 2.05309, val loss: 2.31154, patience: 3, training metric: 0.254, val metric: 0.1\n",
            "INFO - root - epoch: 4, training loss: 2.02482, val loss: 2.31162, patience: 4, training metric: 0.2836, val metric: 0.1\n",
            "INFO - root - epoch: 5, training loss: 1.96413, val loss: 2.31162, patience: 5, training metric: 0.3516, val metric: 0.1\n",
            "INFO - root - epoch: 6, training loss: 1.90978, val loss: 2.3115, patience: 6, training metric: 0.4076, val metric: 0.1\n",
            "INFO - root - epoch: 7, training loss: 1.89101, val loss: 2.31131, patience: 7, training metric: 0.4476, val metric: 0.1\n",
            "INFO - root - epoch: 8, training loss: 1.82716, val loss: 2.31112, patience: 8, training metric: 0.5336, val metric: 0.1\n",
            "INFO - root - epoch: 9, training loss: 1.78793, val loss: 2.31095, patience: 9, training metric: 0.5984, val metric: 0.1\n",
            "INFO - root - epoch: 10, training loss: 1.74638, val loss: 2.31076, patience: 10, training metric: 0.6348, val metric: 0.1\n",
            "INFO - root - epoch: 11, training loss: 1.71558, val loss: 2.3107, patience: 11, training metric: 0.6704, val metric: 0.1\n",
            "INFO - root - epoch: 12, training loss: 1.67495, val loss: 2.31086, patience: 12, training metric: 0.724, val metric: 0.1\n",
            "INFO - root - epoch: 13, training loss: 1.6428, val loss: 2.31126, patience: 13, training metric: 0.7528, val metric: 0.1\n",
            "INFO - root - epoch: 14, training loss: 1.61366, val loss: 2.31201, patience: 14, training metric: 0.7796, val metric: 0.1\n",
            "INFO - root - epoch: 15, training loss: 1.5873, val loss: 2.31338, patience: 15, training metric: 0.8088, val metric: 0.1\n",
            "INFO - root - epoch: 16, training loss: 1.54952, val loss: 2.31499, patience: 16, training metric: 0.8284, val metric: 0.1\n",
            "INFO - root - epoch: 17, training loss: 1.51741, val loss: 2.31721, patience: 17, training metric: 0.8556, val metric: 0.1\n",
            "INFO - root - epoch: 18, training loss: 1.47505, val loss: 2.31979, patience: 18, training metric: 0.8712, val metric: 0.1\n",
            "INFO - root - epoch: 19, training loss: 1.45019, val loss: 2.32288, patience: 19, training metric: 0.8936, val metric: 0.1\n",
            "INFO - root - epoch: 20, training loss: 1.42396, val loss: 2.32653, patience: 20, training metric: 0.9044, val metric: 0.1\n",
            "INFO - root - epoch: 21, training loss: 1.37969, val loss: 2.3306, patience: 21, training metric: 0.9372, val metric: 0.1\n",
            "INFO - root - epoch: 22, training loss: 1.35591, val loss: 2.33478, patience: 22, training metric: 0.9328, val metric: 0.1\n",
            "INFO - root - epoch: 23, training loss: 1.32435, val loss: 2.33975, patience: 23, training metric: 0.9504, val metric: 0.1\n",
            "INFO - root - epoch: 24, training loss: 1.29732, val loss: 2.34641, patience: 24, training metric: 0.9608, val metric: 0.1\n",
            "INFO - root - epoch: 25, training loss: 1.27159, val loss: 2.35781, patience: 25, training metric: 0.9716, val metric: 0.1\n",
            "INFO - root - epoch: 26, training loss: 1.24538, val loss: 2.37448, patience: 26, training metric: 0.976, val metric: 0.1\n",
            "INFO - root - epoch: 27, training loss: 1.2107, val loss: 2.39662, patience: 27, training metric: 0.9888, val metric: 0.1\n",
            "INFO - root - epoch: 28, training loss: 1.17885, val loss: 2.42212, patience: 28, training metric: 0.992, val metric: 0.1\n",
            "INFO - root - epoch: 29, training loss: 1.14988, val loss: 2.44767, patience: 29, training metric: 0.9936, val metric: 0.1\n",
            "INFO - root - epoch: 30, training loss: 1.13614, val loss: 2.4692, patience: 30, training metric: 0.9916, val metric: 0.1\n",
            "INFO - root - epoch: 31, training loss: 1.10292, val loss: 2.48356, patience: 31, training metric: 0.996, val metric: 0.1\n",
            "INFO - root - epoch: 32, training loss: 1.06249, val loss: 2.49094, patience: 32, training metric: 0.998, val metric: 0.1\n",
            "INFO - root - epoch: 33, training loss: 1.04356, val loss: 2.49041, patience: 33, training metric: 0.998, val metric: 0.1\n",
            "INFO - root - epoch: 34, training loss: 1.02092, val loss: 2.48805, patience: 34, training metric: 0.9972, val metric: 0.1\n",
            "INFO - root - epoch: 35, training loss: 0.99232, val loss: 2.49133, patience: 35, training metric: 0.9988, val metric: 0.1\n",
            "INFO - root - epoch: 36, training loss: 0.97211, val loss: 2.50016, patience: 36, training metric: 0.9976, val metric: 0.1\n",
            "INFO - root - epoch: 37, training loss: 0.94869, val loss: 2.50941, patience: 37, training metric: 0.9984, val metric: 0.1\n",
            "INFO - root - epoch: 38, training loss: 0.92212, val loss: 2.50822, patience: 38, training metric: 0.9984, val metric: 0.1\n",
            "INFO - root - epoch: 39, training loss: 0.89961, val loss: 2.52303, patience: 39, training metric: 0.9992, val metric: 0.1\n",
            "INFO - root - epoch: 40, training loss: 0.87614, val loss: 2.5528, patience: 40, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 41, training loss: 0.85235, val loss: 2.60224, patience: 41, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 42, training loss: 0.83104, val loss: 2.66283, patience: 42, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 43, training loss: 0.81022, val loss: 2.71206, patience: 43, training metric: 0.9992, val metric: 0.1\n",
            "INFO - root - epoch: 44, training loss: 0.78821, val loss: 2.77608, patience: 44, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 45, training loss: 0.76774, val loss: 2.84329, patience: 45, training metric: 0.9992, val metric: 0.1\n",
            "INFO - root - epoch: 46, training loss: 0.74456, val loss: 2.94178, patience: 46, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 47, training loss: 0.72545, val loss: 3.12495, patience: 47, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 48, training loss: 0.70726, val loss: 3.36982, patience: 48, training metric: 0.9992, val metric: 0.1\n",
            "INFO - root - epoch: 49, training loss: 0.68109, val loss: 3.60478, patience: 49, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 50, training loss: 0.66596, val loss: 3.71738, patience: 50, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 51, training loss: 0.64667, val loss: 3.67694, patience: 51, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 52, training loss: 0.63047, val loss: 3.61229, patience: 52, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 53, training loss: 0.61807, val loss: 3.54257, patience: 53, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 54, training loss: 0.60879, val loss: 3.49529, patience: 54, training metric: 0.9996, val metric: 0.0928\n",
            "INFO - root - epoch: 55, training loss: 0.60361, val loss: 3.46789, patience: 55, training metric: 1.0, val metric: 0.066\n",
            "INFO - root - epoch: 56, training loss: 0.59752, val loss: 3.4582, patience: 56, training metric: 0.9988, val metric: 0.0536\n",
            "INFO - root - epoch: 57, training loss: 0.58549, val loss: 3.46677, patience: 57, training metric: 0.9992, val metric: 0.0872\n",
            "INFO - root - epoch: 58, training loss: 0.57609, val loss: 3.50452, patience: 0, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 59, training loss: 0.56786, val loss: 3.59174, patience: 0, training metric: 0.9996, val metric: 0.1052\n",
            "INFO - root - epoch: 60, training loss: 0.55998, val loss: 3.69647, patience: 0, training metric: 0.9996, val metric: 0.1072\n",
            "INFO - root - epoch: 61, training loss: 0.55107, val loss: 3.82013, patience: 1, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 62, training loss: 0.54447, val loss: 3.93475, patience: 2, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 63, training loss: 0.53491, val loss: 4.02111, patience: 3, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 64, training loss: 0.52655, val loss: 4.07911, patience: 4, training metric: 0.9996, val metric: 0.1032\n",
            "INFO - root - epoch: 65, training loss: 0.51683, val loss: 4.06437, patience: 5, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 66, training loss: 0.50973, val loss: 4.01875, patience: 6, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 67, training loss: 0.50423, val loss: 3.95994, patience: 7, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 68, training loss: 0.49685, val loss: 3.90674, patience: 8, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 69, training loss: 0.4904, val loss: 3.85125, patience: 9, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 70, training loss: 0.48243, val loss: 3.7987, patience: 10, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 71, training loss: 0.47656, val loss: 3.74365, patience: 11, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 72, training loss: 0.47207, val loss: 3.70812, patience: 12, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 73, training loss: 0.4633, val loss: 3.71009, patience: 13, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 74, training loss: 0.45676, val loss: 3.72516, patience: 14, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 75, training loss: 0.45199, val loss: 3.73286, patience: 0, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 76, training loss: 0.44791, val loss: 3.7189, patience: 1, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 77, training loss: 0.44042, val loss: 3.71398, patience: 0, training metric: 1.0, val metric: 0.1384\n",
            "INFO - root - epoch: 78, training loss: 0.43119, val loss: 3.70272, patience: 0, training metric: 1.0, val metric: 0.176\n",
            "INFO - root - epoch: 79, training loss: 0.42449, val loss: 3.69239, patience: 0, training metric: 1.0, val metric: 0.1884\n",
            "INFO - root - epoch: 80, training loss: 0.41975, val loss: 3.6844, patience: 0, training metric: 1.0, val metric: 0.1896\n",
            "INFO - root - epoch: 81, training loss: 0.41514, val loss: 3.70141, patience: 1, training metric: 1.0, val metric: 0.1808\n",
            "INFO - root - epoch: 82, training loss: 0.40908, val loss: 3.72632, patience: 2, training metric: 1.0, val metric: 0.1512\n",
            "INFO - root - epoch: 83, training loss: 0.40431, val loss: 3.73767, patience: 3, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 84, training loss: 0.39975, val loss: 3.68674, patience: 4, training metric: 1.0, val metric: 0.12\n",
            "INFO - root - epoch: 85, training loss: 0.39453, val loss: 3.63185, patience: 5, training metric: 1.0, val metric: 0.1632\n",
            "INFO - root - epoch: 86, training loss: 0.38931, val loss: 3.58529, patience: 6, training metric: 1.0, val metric: 0.1772\n",
            "INFO - root - epoch: 87, training loss: 0.38399, val loss: 3.55172, patience: 7, training metric: 1.0, val metric: 0.1612\n",
            "INFO - root - epoch: 88, training loss: 0.38018, val loss: 3.58052, patience: 8, training metric: 1.0, val metric: 0.1276\n",
            "INFO - root - epoch: 89, training loss: 0.37562, val loss: 3.63066, patience: 9, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 90, training loss: 0.36943, val loss: 3.64948, patience: 10, training metric: 1.0, val metric: 0.1232\n",
            "INFO - root - epoch: 91, training loss: 0.36411, val loss: 3.60831, patience: 11, training metric: 1.0, val metric: 0.118\n",
            "INFO - root - epoch: 92, training loss: 0.36179, val loss: 3.54931, patience: 12, training metric: 1.0, val metric: 0.1176\n",
            "INFO - root - epoch: 93, training loss: 0.35865, val loss: 3.53251, patience: 13, training metric: 1.0, val metric: 0.13\n",
            "INFO - root - epoch: 94, training loss: 0.35301, val loss: 3.55461, patience: 14, training metric: 1.0, val metric: 0.1536\n",
            "INFO - root - epoch: 95, training loss: 0.34879, val loss: 3.54891, patience: 15, training metric: 1.0, val metric: 0.1664\n",
            "INFO - root - epoch: 96, training loss: 0.34622, val loss: 3.51169, patience: 16, training metric: 1.0, val metric: 0.1488\n",
            "INFO - root - epoch: 97, training loss: 0.34188, val loss: 3.45148, patience: 17, training metric: 1.0, val metric: 0.1248\n",
            "INFO - root - epoch: 98, training loss: 0.33573, val loss: 3.40688, patience: 18, training metric: 1.0, val metric: 0.1184\n",
            "INFO - root - epoch: 99, training loss: 0.3337, val loss: 3.41224, patience: 19, training metric: 1.0, val metric: 0.1196\n",
            "INFO - root - epoch: 100, training loss: 0.32873, val loss: 3.4572, patience: 20, training metric: 1.0, val metric: 0.13\n",
            "INFO - root - epoch: 101, training loss: 0.32355, val loss: 3.50257, patience: 21, training metric: 1.0, val metric: 0.1548\n",
            "INFO - root - epoch: 102, training loss: 0.32121, val loss: 3.54351, patience: 22, training metric: 1.0, val metric: 0.1868\n",
            "INFO - root - epoch: 103, training loss: 0.31758, val loss: 3.54198, patience: 0, training metric: 1.0, val metric: 0.2108\n",
            "INFO - root - epoch: 104, training loss: 0.31301, val loss: 3.53121, patience: 1, training metric: 1.0, val metric: 0.1828\n",
            "INFO - root - epoch: 105, training loss: 0.31065, val loss: 3.51691, patience: 2, training metric: 1.0, val metric: 0.1388\n",
            "INFO - root - epoch: 106, training loss: 0.3069, val loss: 3.53517, patience: 3, training metric: 1.0, val metric: 0.1148\n",
            "INFO - root - epoch: 107, training loss: 0.30167, val loss: 3.56032, patience: 4, training metric: 1.0, val metric: 0.1132\n",
            "INFO - root - epoch: 108, training loss: 0.29787, val loss: 3.5878, patience: 5, training metric: 1.0, val metric: 0.1144\n",
            "INFO - root - epoch: 109, training loss: 0.29429, val loss: 3.58904, patience: 6, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 110, training loss: 0.2922, val loss: 3.58292, patience: 7, training metric: 1.0, val metric: 0.1124\n",
            "INFO - root - epoch: 111, training loss: 0.28928, val loss: 3.57427, patience: 8, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 112, training loss: 0.28314, val loss: 3.56827, patience: 9, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 113, training loss: 0.27955, val loss: 3.56258, patience: 10, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 114, training loss: 0.27808, val loss: 3.5385, patience: 11, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 115, training loss: 0.27609, val loss: 3.52477, patience: 12, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 116, training loss: 0.2718, val loss: 3.51991, patience: 13, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 117, training loss: 0.2681, val loss: 3.51402, patience: 14, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 118, training loss: 0.26478, val loss: 3.51478, patience: 15, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 119, training loss: 0.26268, val loss: 3.51916, patience: 16, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 120, training loss: 0.25884, val loss: 3.519, patience: 17, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 121, training loss: 0.2542, val loss: 3.52078, patience: 18, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 122, training loss: 0.25359, val loss: 3.50468, patience: 19, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 123, training loss: 0.25012, val loss: 3.49431, patience: 20, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 124, training loss: 0.24639, val loss: 3.47992, patience: 21, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 125, training loss: 0.24539, val loss: 3.45729, patience: 22, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 126, training loss: 0.2423, val loss: 3.45916, patience: 23, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 127, training loss: 0.24005, val loss: 3.48609, patience: 24, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 128, training loss: 0.2378, val loss: 3.51153, patience: 25, training metric: 1.0, val metric: 0.1148\n",
            "INFO - root - epoch: 129, training loss: 0.23459, val loss: 3.51976, patience: 26, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 130, training loss: 0.2329, val loss: 3.49349, patience: 27, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 131, training loss: 0.22991, val loss: 3.47434, patience: 28, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 132, training loss: 0.22716, val loss: 3.45438, patience: 29, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 133, training loss: 0.22528, val loss: 3.45393, patience: 30, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 134, training loss: 0.2228, val loss: 3.4639, patience: 31, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 135, training loss: 0.22057, val loss: 3.46184, patience: 32, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 136, training loss: 0.21839, val loss: 3.42537, patience: 33, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 137, training loss: 0.21494, val loss: 3.38967, patience: 34, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 138, training loss: 0.21482, val loss: 3.34129, patience: 35, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 139, training loss: 0.21315, val loss: 3.31022, patience: 36, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 140, training loss: 0.20847, val loss: 3.29135, patience: 37, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 141, training loss: 0.20665, val loss: 3.27514, patience: 38, training metric: 1.0, val metric: 0.1116\n",
            "INFO - root - epoch: 142, training loss: 0.20482, val loss: 3.26534, patience: 39, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 143, training loss: 0.2018, val loss: 3.25336, patience: 40, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 144, training loss: 0.20056, val loss: 3.27088, patience: 41, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 145, training loss: 0.19816, val loss: 3.29015, patience: 42, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 146, training loss: 0.19655, val loss: 3.29845, patience: 43, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 147, training loss: 0.19482, val loss: 3.29912, patience: 44, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 148, training loss: 0.19248, val loss: 3.29833, patience: 45, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 149, training loss: 0.19074, val loss: 3.29642, patience: 46, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 150, training loss: 0.18946, val loss: 3.29206, patience: 47, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 151, training loss: 0.18717, val loss: 3.29291, patience: 48, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 152, training loss: 0.18603, val loss: 3.2892, patience: 49, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 153, training loss: 0.18346, val loss: 3.2572, patience: 50, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 154, training loss: 0.18281, val loss: 3.20601, patience: 51, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 155, training loss: 0.17937, val loss: 3.16701, patience: 52, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 156, training loss: 0.17881, val loss: 3.13324, patience: 53, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 157, training loss: 0.17865, val loss: 3.10938, patience: 54, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 158, training loss: 0.17759, val loss: 3.10115, patience: 55, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 159, training loss: 0.17667, val loss: 3.0844, patience: 56, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 160, training loss: 0.17614, val loss: 3.08485, patience: 57, training metric: 1.0, val metric: 0.1148\n",
            "INFO - root - epoch: 161, training loss: 0.17488, val loss: 3.08938, patience: 58, training metric: 1.0, val metric: 0.1264\n",
            "INFO - root - epoch: 162, training loss: 0.17476, val loss: 3.09605, patience: 59, training metric: 1.0, val metric: 0.1348\n",
            "INFO - root - epoch: 163, training loss: 0.17333, val loss: 3.11014, patience: 60, training metric: 1.0, val metric: 0.1468\n",
            "INFO - root - epoch: 164, training loss: 0.17196, val loss: 3.12434, patience: 61, training metric: 1.0, val metric: 0.1416\n",
            "INFO - root - epoch: 165, training loss: 0.1711, val loss: 3.14757, patience: 62, training metric: 1.0, val metric: 0.134\n",
            "INFO - root - epoch: 166, training loss: 0.17035, val loss: 3.14927, patience: 63, training metric: 1.0, val metric: 0.1196\n",
            "INFO - root - epoch: 167, training loss: 0.1703, val loss: 3.14967, patience: 64, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 168, training loss: 0.16805, val loss: 3.14775, patience: 65, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 169, training loss: 0.16835, val loss: 3.14523, patience: 66, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 170, training loss: 0.16659, val loss: 3.13648, patience: 67, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 171, training loss: 0.16819, val loss: 3.14111, patience: 68, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 172, training loss: 0.16654, val loss: 3.1419, patience: 69, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 173, training loss: 0.16586, val loss: 3.14935, patience: 70, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 174, training loss: 0.16493, val loss: 3.15528, patience: 71, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 175, training loss: 0.16428, val loss: 3.15372, patience: 72, training metric: 1.0, val metric: 0.1124\n",
            "INFO - root - epoch: 176, training loss: 0.16517, val loss: 3.14935, patience: 73, training metric: 1.0, val metric: 0.1184\n",
            "INFO - root - epoch: 177, training loss: 0.16331, val loss: 3.13583, patience: 74, training metric: 1.0, val metric: 0.1176\n",
            "INFO - root - epoch: 178, training loss: 0.16319, val loss: 3.11559, patience: 75, training metric: 1.0, val metric: 0.1248\n",
            "INFO - root - epoch: 179, training loss: 0.16109, val loss: 3.09581, patience: 76, training metric: 1.0, val metric: 0.1264\n",
            "INFO - root - epoch: 180, training loss: 0.15986, val loss: 3.09373, patience: 77, training metric: 1.0, val metric: 0.1168\n",
            "INFO - root - epoch: 181, training loss: 0.16004, val loss: 3.09491, patience: 78, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 182, training loss: 0.15944, val loss: 3.1093, patience: 79, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 183, training loss: 0.15886, val loss: 3.13597, patience: 80, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 184, training loss: 0.15858, val loss: 3.14789, patience: 81, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 185, training loss: 0.1582, val loss: 3.15987, patience: 82, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 186, training loss: 0.15648, val loss: 3.15605, patience: 83, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 187, training loss: 0.15665, val loss: 3.14545, patience: 84, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 188, training loss: 0.15595, val loss: 3.13791, patience: 85, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 189, training loss: 0.15535, val loss: 3.10996, patience: 86, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 190, training loss: 0.15461, val loss: 3.08715, patience: 87, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 191, training loss: 0.15338, val loss: 3.06174, patience: 88, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 192, training loss: 0.15247, val loss: 3.04223, patience: 89, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 193, training loss: 0.1521, val loss: 3.04265, patience: 90, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 194, training loss: 0.15123, val loss: 3.04753, patience: 91, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 195, training loss: 0.15069, val loss: 3.06116, patience: 92, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 196, training loss: 0.14958, val loss: 3.06901, patience: 93, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 197, training loss: 0.14945, val loss: 3.07334, patience: 94, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 198, training loss: 0.14907, val loss: 3.08276, patience: 95, training metric: 1.0, val metric: 0.11\n",
            "INFO - root - epoch: 199, training loss: 0.14916, val loss: 3.10074, patience: 96, training metric: 1.0, val metric: 0.1096\n",
            "INFO - root - epoch: 200, training loss: 0.1476, val loss: 3.12717, patience: 97, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 201, training loss: 0.14671, val loss: 3.15608, patience: 98, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 202, training loss: 0.14642, val loss: 3.18327, patience: 99, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 203, training loss: 0.14612, val loss: 3.19986, patience: 100, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 204, training loss: 0.14617, val loss: 3.19916, patience: 101, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 205, training loss: 0.14449, val loss: 3.18473, patience: 102, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 206, training loss: 0.14363, val loss: 3.15826, patience: 103, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 207, training loss: 0.14393, val loss: 3.13374, patience: 104, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 208, training loss: 0.14291, val loss: 3.11671, patience: 105, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 209, training loss: 0.14286, val loss: 3.10814, patience: 106, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 210, training loss: 0.14267, val loss: 3.10569, patience: 107, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 211, training loss: 0.14182, val loss: 3.09874, patience: 108, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 212, training loss: 0.1417, val loss: 3.09764, patience: 109, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 213, training loss: 0.14169, val loss: 3.09687, patience: 110, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 214, training loss: 0.14168, val loss: 3.08981, patience: 111, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 215, training loss: 0.1409, val loss: 3.09161, patience: 112, training metric: 1.0, val metric: 0.11\n",
            "INFO - root - epoch: 216, training loss: 0.14079, val loss: 3.10068, patience: 113, training metric: 1.0, val metric: 0.118\n",
            "INFO - root - epoch: 217, training loss: 0.13975, val loss: 3.11753, patience: 114, training metric: 1.0, val metric: 0.1268\n",
            "INFO - root - epoch: 218, training loss: 0.13979, val loss: 3.13417, patience: 115, training metric: 1.0, val metric: 0.1268\n",
            "INFO - root - epoch: 219, training loss: 0.13965, val loss: 3.14555, patience: 116, training metric: 1.0, val metric: 0.1328\n",
            "INFO - root - epoch: 220, training loss: 0.14015, val loss: 3.14429, patience: 117, training metric: 1.0, val metric: 0.1324\n",
            "INFO - root - epoch: 221, training loss: 0.14006, val loss: 3.13603, patience: 118, training metric: 1.0, val metric: 0.1348\n",
            "INFO - root - epoch: 222, training loss: 0.13952, val loss: 3.1222, patience: 119, training metric: 1.0, val metric: 0.13\n",
            "INFO - root - epoch: 223, training loss: 0.139, val loss: 3.09789, patience: 120, training metric: 1.0, val metric: 0.1276\n",
            "INFO - root - epoch: 224, training loss: 0.13827, val loss: 3.08999, patience: 121, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 225, training loss: 0.13845, val loss: 3.07377, patience: 122, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 226, training loss: 0.13783, val loss: 3.06777, patience: 123, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 227, training loss: 0.13759, val loss: 3.07029, patience: 124, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 228, training loss: 0.13743, val loss: 3.06867, patience: 125, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 229, training loss: 0.1376, val loss: 3.0746, patience: 126, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 230, training loss: 0.13621, val loss: 3.07499, patience: 127, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 231, training loss: 0.13574, val loss: 3.08195, patience: 128, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 232, training loss: 0.13512, val loss: 3.09522, patience: 129, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 233, training loss: 0.13492, val loss: 3.11059, patience: 130, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 234, training loss: 0.13431, val loss: 3.11979, patience: 131, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 235, training loss: 0.13481, val loss: 3.12953, patience: 132, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 236, training loss: 0.13443, val loss: 3.13267, patience: 133, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 237, training loss: 0.13494, val loss: 3.13669, patience: 134, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 238, training loss: 0.13486, val loss: 3.12811, patience: 135, training metric: 1.0, val metric: 0.1124\n",
            "INFO - root - epoch: 239, training loss: 0.13348, val loss: 3.12431, patience: 136, training metric: 1.0, val metric: 0.1172\n",
            "INFO - root - epoch: 240, training loss: 0.13378, val loss: 3.11569, patience: 137, training metric: 1.0, val metric: 0.1184\n",
            "INFO - root - epoch: 241, training loss: 0.13276, val loss: 3.11956, patience: 138, training metric: 1.0, val metric: 0.1232\n",
            "INFO - root - epoch: 242, training loss: 0.13312, val loss: 3.12084, patience: 139, training metric: 1.0, val metric: 0.126\n",
            "INFO - root - epoch: 243, training loss: 0.13305, val loss: 3.11475, patience: 140, training metric: 1.0, val metric: 0.13\n",
            "INFO - root - epoch: 244, training loss: 0.13253, val loss: 3.12658, patience: 141, training metric: 1.0, val metric: 0.1324\n",
            "INFO - root - epoch: 245, training loss: 0.13151, val loss: 3.13857, patience: 142, training metric: 1.0, val metric: 0.1268\n",
            "INFO - root - epoch: 246, training loss: 0.13199, val loss: 3.14277, patience: 143, training metric: 1.0, val metric: 0.13\n",
            "INFO - root - epoch: 247, training loss: 0.13147, val loss: 3.14921, patience: 144, training metric: 1.0, val metric: 0.1296\n",
            "INFO - root - epoch: 248, training loss: 0.13149, val loss: 3.15222, patience: 145, training metric: 1.0, val metric: 0.124\n",
            "INFO - root - epoch: 249, training loss: 0.13072, val loss: 3.15448, patience: 146, training metric: 1.0, val metric: 0.12\n",
            "INFO - root - epoch: 250, training loss: 0.13083, val loss: 3.14374, patience: 147, training metric: 1.0, val metric: 0.1212\n",
            "INFO - root - epoch: 251, training loss: 0.13068, val loss: 3.13727, patience: 148, training metric: 1.0, val metric: 0.1172\n",
            "INFO - root - epoch: 252, training loss: 0.13004, val loss: 3.1321, patience: 149, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 253, training loss: 0.13035, val loss: 3.12681, patience: 150, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 254, training loss: 0.13004, val loss: 3.12921, patience: 151, training metric: 1.0, val metric: 0.1096\n",
            "INFO - root - epoch: 255, training loss: 0.12922, val loss: 3.133, patience: 152, training metric: 1.0, val metric: 0.1116\n",
            "INFO - root - epoch: 256, training loss: 0.12854, val loss: 3.12951, patience: 153, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 257, training loss: 0.12861, val loss: 3.13911, patience: 154, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 258, training loss: 0.12918, val loss: 3.1341, patience: 155, training metric: 1.0, val metric: 0.1144\n",
            "INFO - root - epoch: 259, training loss: 0.12802, val loss: 3.14538, patience: 156, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 260, training loss: 0.12867, val loss: 3.14617, patience: 157, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 261, training loss: 0.12892, val loss: 3.14351, patience: 158, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 262, training loss: 0.1282, val loss: 3.14426, patience: 159, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 263, training loss: 0.12822, val loss: 3.13891, patience: 160, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 264, training loss: 0.12775, val loss: 3.13974, patience: 161, training metric: 1.0, val metric: 0.1096\n",
            "INFO - root - epoch: 265, training loss: 0.12715, val loss: 3.13426, patience: 162, training metric: 1.0, val metric: 0.11\n",
            "INFO - root - epoch: 266, training loss: 0.12746, val loss: 3.12523, patience: 163, training metric: 1.0, val metric: 0.1144\n",
            "INFO - root - epoch: 267, training loss: 0.12733, val loss: 3.12057, patience: 164, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 268, training loss: 0.12708, val loss: 3.12185, patience: 165, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 269, training loss: 0.12731, val loss: 3.11497, patience: 166, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 270, training loss: 0.12757, val loss: 3.11411, patience: 167, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 271, training loss: 0.12677, val loss: 3.11475, patience: 168, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 272, training loss: 0.12626, val loss: 3.11455, patience: 169, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 273, training loss: 0.1264, val loss: 3.11246, patience: 170, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 274, training loss: 0.12695, val loss: 3.12044, patience: 171, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 275, training loss: 0.1259, val loss: 3.12311, patience: 172, training metric: 1.0, val metric: 0.1132\n",
            "INFO - root - epoch: 276, training loss: 0.12604, val loss: 3.12686, patience: 173, training metric: 1.0, val metric: 0.1172\n",
            "INFO - root - epoch: 277, training loss: 0.12543, val loss: 3.13333, patience: 174, training metric: 1.0, val metric: 0.1168\n",
            "INFO - root - epoch: 278, training loss: 0.12554, val loss: 3.134, patience: 175, training metric: 1.0, val metric: 0.1156\n",
            "INFO - root - epoch: 279, training loss: 0.12558, val loss: 3.13129, patience: 176, training metric: 1.0, val metric: 0.1208\n",
            "INFO - root - epoch: 280, training loss: 0.12621, val loss: 3.14067, patience: 177, training metric: 1.0, val metric: 0.1148\n",
            "INFO - root - epoch: 281, training loss: 0.1254, val loss: 3.14776, patience: 178, training metric: 1.0, val metric: 0.1208\n",
            "INFO - root - epoch: 282, training loss: 0.1251, val loss: 3.14911, patience: 179, training metric: 1.0, val metric: 0.116\n",
            "INFO - root - epoch: 283, training loss: 0.12483, val loss: 3.15077, patience: 180, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 284, training loss: 0.12486, val loss: 3.1484, patience: 181, training metric: 1.0, val metric: 0.1172\n",
            "INFO - root - epoch: 285, training loss: 0.12523, val loss: 3.14555, patience: 182, training metric: 1.0, val metric: 0.1148\n",
            "INFO - root - epoch: 286, training loss: 0.12502, val loss: 3.13962, patience: 183, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 287, training loss: 0.1248, val loss: 3.14733, patience: 184, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 288, training loss: 0.12484, val loss: 3.14246, patience: 185, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 289, training loss: 0.12408, val loss: 3.14228, patience: 186, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 290, training loss: 0.12468, val loss: 3.14415, patience: 187, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 291, training loss: 0.12447, val loss: 3.13983, patience: 188, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 292, training loss: 0.12418, val loss: 3.13788, patience: 189, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 293, training loss: 0.12496, val loss: 3.14076, patience: 190, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 294, training loss: 0.12371, val loss: 3.13881, patience: 191, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 295, training loss: 0.12389, val loss: 3.14, patience: 192, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 296, training loss: 0.12369, val loss: 3.13329, patience: 193, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 297, training loss: 0.12311, val loss: 3.1329, patience: 194, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 298, training loss: 0.12354, val loss: 3.13273, patience: 195, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 299, training loss: 0.12301, val loss: 3.13802, patience: 196, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 300, training loss: 0.12335, val loss: 3.14072, patience: 197, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 301, training loss: 0.12267, val loss: 3.1431, patience: 198, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 302, training loss: 0.12318, val loss: 3.14569, patience: 199, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 303, training loss: 0.1225, val loss: 3.15144, patience: 200, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 304, training loss: 0.12257, val loss: 3.14961, patience: 201, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 305, training loss: 0.1227, val loss: 3.16554, patience: 202, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 306, training loss: 0.12336, val loss: 3.15075, patience: 203, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 307, training loss: 0.1215, val loss: 3.15863, patience: 204, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 308, training loss: 0.12275, val loss: 3.15808, patience: 205, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 309, training loss: 0.12184, val loss: 3.16174, patience: 206, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 310, training loss: 0.12221, val loss: 3.16298, patience: 207, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 311, training loss: 0.12169, val loss: 3.15749, patience: 208, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 312, training loss: 0.12121, val loss: 3.15623, patience: 209, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 313, training loss: 0.12187, val loss: 3.15793, patience: 210, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 314, training loss: 0.12093, val loss: 3.16235, patience: 211, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 315, training loss: 0.12173, val loss: 3.15602, patience: 212, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 316, training loss: 0.12102, val loss: 3.15749, patience: 213, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 317, training loss: 0.12133, val loss: 3.15915, patience: 214, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 318, training loss: 0.12029, val loss: 3.16216, patience: 215, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 319, training loss: 0.12108, val loss: 3.16373, patience: 216, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 320, training loss: 0.12049, val loss: 3.15611, patience: 217, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 321, training loss: 0.1206, val loss: 3.15741, patience: 218, training metric: 1.0, val metric: 0.1116\n",
            "INFO - root - epoch: 322, training loss: 0.12089, val loss: 3.15815, patience: 219, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 323, training loss: 0.12092, val loss: 3.16058, patience: 220, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 324, training loss: 0.12073, val loss: 3.15351, patience: 221, training metric: 1.0, val metric: 0.114\n",
            "INFO - root - epoch: 325, training loss: 0.1207, val loss: 3.15381, patience: 222, training metric: 1.0, val metric: 0.1136\n",
            "INFO - root - epoch: 326, training loss: 0.12061, val loss: 3.1594, patience: 223, training metric: 1.0, val metric: 0.1124\n",
            "INFO - root - epoch: 327, training loss: 0.12059, val loss: 3.15713, patience: 224, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 328, training loss: 0.12005, val loss: 3.15614, patience: 225, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 329, training loss: 0.11955, val loss: 3.14511, patience: 226, training metric: 1.0, val metric: 0.11\n",
            "INFO - root - epoch: 330, training loss: 0.11996, val loss: 3.14986, patience: 227, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 331, training loss: 0.1201, val loss: 3.15091, patience: 228, training metric: 1.0, val metric: 0.1096\n",
            "INFO - root - epoch: 332, training loss: 0.11998, val loss: 3.15029, patience: 229, training metric: 1.0, val metric: 0.1132\n",
            "INFO - root - epoch: 333, training loss: 0.12015, val loss: 3.15054, patience: 230, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 334, training loss: 0.11982, val loss: 3.15216, patience: 231, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 335, training loss: 0.12006, val loss: 3.14851, patience: 232, training metric: 1.0, val metric: 0.1116\n",
            "INFO - root - epoch: 336, training loss: 0.11958, val loss: 3.15028, patience: 233, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 337, training loss: 0.11916, val loss: 3.14831, patience: 234, training metric: 1.0, val metric: 0.1132\n",
            "INFO - root - epoch: 338, training loss: 0.11949, val loss: 3.1574, patience: 235, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 339, training loss: 0.11976, val loss: 3.15064, patience: 236, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 340, training loss: 0.11988, val loss: 3.15584, patience: 237, training metric: 1.0, val metric: 0.1132\n",
            "INFO - root - epoch: 341, training loss: 0.11972, val loss: 3.15583, patience: 238, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 342, training loss: 0.11958, val loss: 3.15718, patience: 239, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 343, training loss: 0.11947, val loss: 3.15956, patience: 240, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 344, training loss: 0.11956, val loss: 3.15877, patience: 241, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 345, training loss: 0.11984, val loss: 3.15615, patience: 242, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 346, training loss: 0.12034, val loss: 3.15729, patience: 243, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 347, training loss: 0.11942, val loss: 3.15219, patience: 244, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 348, training loss: 0.1189, val loss: 3.15868, patience: 245, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 349, training loss: 0.11891, val loss: 3.15443, patience: 246, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 350, training loss: 0.1184, val loss: 3.15072, patience: 247, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 351, training loss: 0.11863, val loss: 3.15683, patience: 248, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 352, training loss: 0.11877, val loss: 3.15841, patience: 249, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 353, training loss: 0.11906, val loss: 3.15468, patience: 250, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 354, training loss: 0.11847, val loss: 3.15607, patience: 251, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 355, training loss: 0.11878, val loss: 3.15548, patience: 252, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 356, training loss: 0.11882, val loss: 3.15075, patience: 253, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 357, training loss: 0.11886, val loss: 3.15421, patience: 254, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 358, training loss: 0.11829, val loss: 3.15158, patience: 255, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 359, training loss: 0.11773, val loss: 3.14874, patience: 256, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 360, training loss: 0.11856, val loss: 3.15222, patience: 257, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 361, training loss: 0.11924, val loss: 3.14838, patience: 258, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 362, training loss: 0.11864, val loss: 3.1471, patience: 259, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 363, training loss: 0.11857, val loss: 3.15444, patience: 260, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 364, training loss: 0.1185, val loss: 3.15139, patience: 261, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 365, training loss: 0.11818, val loss: 3.14885, patience: 262, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 366, training loss: 0.11863, val loss: 3.15229, patience: 263, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 367, training loss: 0.11827, val loss: 3.14465, patience: 264, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 368, training loss: 0.11884, val loss: 3.1492, patience: 265, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 369, training loss: 0.11849, val loss: 3.14774, patience: 266, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 370, training loss: 0.11841, val loss: 3.15325, patience: 267, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 371, training loss: 0.11831, val loss: 3.14286, patience: 268, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 372, training loss: 0.11758, val loss: 3.14734, patience: 269, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 373, training loss: 0.11747, val loss: 3.15635, patience: 270, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 374, training loss: 0.11766, val loss: 3.14906, patience: 271, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 375, training loss: 0.11787, val loss: 3.15218, patience: 272, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 376, training loss: 0.11784, val loss: 3.15095, patience: 273, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 377, training loss: 0.11752, val loss: 3.15123, patience: 274, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 378, training loss: 0.11794, val loss: 3.15592, patience: 275, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 379, training loss: 0.11692, val loss: 3.15063, patience: 276, training metric: 1.0, val metric: 0.1136\n",
            "INFO - root - epoch: 380, training loss: 0.11742, val loss: 3.15412, patience: 277, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 381, training loss: 0.11757, val loss: 3.15052, patience: 278, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 382, training loss: 0.118, val loss: 3.15761, patience: 279, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 383, training loss: 0.11741, val loss: 3.14925, patience: 280, training metric: 1.0, val metric: 0.1136\n",
            "INFO - root - epoch: 384, training loss: 0.11739, val loss: 3.15448, patience: 281, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 385, training loss: 0.11752, val loss: 3.14569, patience: 282, training metric: 1.0, val metric: 0.1116\n",
            "INFO - root - epoch: 386, training loss: 0.11689, val loss: 3.1524, patience: 283, training metric: 1.0, val metric: 0.1124\n",
            "INFO - root - epoch: 387, training loss: 0.11754, val loss: 3.15322, patience: 284, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 388, training loss: 0.11706, val loss: 3.15294, patience: 285, training metric: 1.0, val metric: 0.114\n",
            "INFO - root - epoch: 389, training loss: 0.11721, val loss: 3.15226, patience: 286, training metric: 1.0, val metric: 0.116\n",
            "INFO - root - epoch: 390, training loss: 0.11664, val loss: 3.15005, patience: 287, training metric: 1.0, val metric: 0.1096\n",
            "INFO - root - epoch: 391, training loss: 0.11707, val loss: 3.16428, patience: 288, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 392, training loss: 0.11705, val loss: 3.14897, patience: 289, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 393, training loss: 0.11698, val loss: 3.14684, patience: 290, training metric: 1.0, val metric: 0.11\n",
            "INFO - root - epoch: 394, training loss: 0.11717, val loss: 3.14505, patience: 291, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 395, training loss: 0.11694, val loss: 3.15722, patience: 292, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 396, training loss: 0.1171, val loss: 3.15547, patience: 293, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 397, training loss: 0.11669, val loss: 3.15129, patience: 294, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 398, training loss: 0.1173, val loss: 3.1511, patience: 295, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 399, training loss: 0.11717, val loss: 3.15426, patience: 296, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 400, training loss: 0.11689, val loss: 3.15681, patience: 297, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 401, training loss: 0.11694, val loss: 3.15166, patience: 298, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 402, training loss: 0.11665, val loss: 3.15543, patience: 299, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 403, training loss: 0.11639, val loss: 3.1526, patience: 300, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 404, training loss: 0.1165, val loss: 3.15511, patience: 301, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 405, training loss: 0.11696, val loss: 3.14953, patience: 302, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 406, training loss: 0.11653, val loss: 3.15471, patience: 303, training metric: 1.0, val metric: 0.0932\n",
            "INFO - root - epoch: 407, training loss: 0.1168, val loss: 3.14935, patience: 304, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 408, training loss: 0.11677, val loss: 3.14921, patience: 305, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 409, training loss: 0.11644, val loss: 3.15252, patience: 306, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 410, training loss: 0.11622, val loss: 3.15102, patience: 307, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 411, training loss: 0.11682, val loss: 3.15015, patience: 308, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 412, training loss: 0.11658, val loss: 3.14905, patience: 309, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 413, training loss: 0.11605, val loss: 3.14905, patience: 310, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 414, training loss: 0.11647, val loss: 3.15262, patience: 311, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 415, training loss: 0.11624, val loss: 3.15512, patience: 312, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 416, training loss: 0.11584, val loss: 3.14682, patience: 313, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 417, training loss: 0.11664, val loss: 3.14951, patience: 314, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 418, training loss: 0.11642, val loss: 3.14978, patience: 315, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 419, training loss: 0.11627, val loss: 3.14861, patience: 316, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 420, training loss: 0.11603, val loss: 3.15044, patience: 317, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 421, training loss: 0.1164, val loss: 3.15107, patience: 318, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 422, training loss: 0.11631, val loss: 3.15136, patience: 319, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 423, training loss: 0.11636, val loss: 3.15406, patience: 320, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 424, training loss: 0.11672, val loss: 3.14599, patience: 321, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 425, training loss: 0.11663, val loss: 3.15145, patience: 322, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 426, training loss: 0.1162, val loss: 3.15072, patience: 323, training metric: 1.0, val metric: 0.1096\n",
            "INFO - root - epoch: 427, training loss: 0.11612, val loss: 3.14508, patience: 324, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 428, training loss: 0.11551, val loss: 3.14702, patience: 325, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 429, training loss: 0.11568, val loss: 3.13999, patience: 326, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 430, training loss: 0.11615, val loss: 3.14454, patience: 327, training metric: 1.0, val metric: 0.1132\n",
            "INFO - root - epoch: 431, training loss: 0.11624, val loss: 3.14831, patience: 328, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 432, training loss: 0.11589, val loss: 3.1504, patience: 329, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 433, training loss: 0.11578, val loss: 3.14565, patience: 330, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 434, training loss: 0.11565, val loss: 3.14725, patience: 331, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 435, training loss: 0.11592, val loss: 3.1514, patience: 332, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 436, training loss: 0.11596, val loss: 3.14902, patience: 333, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 437, training loss: 0.1165, val loss: 3.14645, patience: 334, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 438, training loss: 0.11542, val loss: 3.14789, patience: 335, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 439, training loss: 0.1161, val loss: 3.14945, patience: 336, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 440, training loss: 0.1161, val loss: 3.14648, patience: 337, training metric: 1.0, val metric: 0.11\n",
            "INFO - root - epoch: 441, training loss: 0.11607, val loss: 3.14724, patience: 338, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 442, training loss: 0.11637, val loss: 3.1506, patience: 339, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 443, training loss: 0.11536, val loss: 3.15003, patience: 340, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 444, training loss: 0.11571, val loss: 3.14123, patience: 341, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 445, training loss: 0.11587, val loss: 3.14849, patience: 342, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 446, training loss: 0.11575, val loss: 3.14762, patience: 343, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 447, training loss: 0.11559, val loss: 3.14854, patience: 344, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 448, training loss: 0.11607, val loss: 3.13835, patience: 345, training metric: 1.0, val metric: 0.1148\n",
            "INFO - root - epoch: 449, training loss: 0.11558, val loss: 3.14468, patience: 346, training metric: 1.0, val metric: 0.11\n",
            "INFO - root - epoch: 450, training loss: 0.11573, val loss: 3.15331, patience: 347, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 451, training loss: 0.11499, val loss: 3.14124, patience: 348, training metric: 1.0, val metric: 0.1152\n",
            "INFO - root - epoch: 452, training loss: 0.1159, val loss: 3.14514, patience: 349, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 453, training loss: 0.11579, val loss: 3.1478, patience: 350, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 454, training loss: 0.11591, val loss: 3.14335, patience: 351, training metric: 1.0, val metric: 0.114\n",
            "INFO - root - epoch: 455, training loss: 0.11614, val loss: 3.14921, patience: 352, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 456, training loss: 0.11595, val loss: 3.15153, patience: 353, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 457, training loss: 0.11526, val loss: 3.15198, patience: 354, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 458, training loss: 0.11551, val loss: 3.14959, patience: 355, training metric: 1.0, val metric: 0.1132\n",
            "INFO - root - epoch: 459, training loss: 0.1154, val loss: 3.15634, patience: 356, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 460, training loss: 0.1154, val loss: 3.15231, patience: 357, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 461, training loss: 0.11518, val loss: 3.15497, patience: 358, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 462, training loss: 0.11529, val loss: 3.14939, patience: 359, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 463, training loss: 0.11521, val loss: 3.15492, patience: 360, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 464, training loss: 0.11491, val loss: 3.15704, patience: 361, training metric: 1.0, val metric: 0.11\n",
            "INFO - root - epoch: 465, training loss: 0.1153, val loss: 3.15464, patience: 362, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 466, training loss: 0.11498, val loss: 3.15625, patience: 363, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 467, training loss: 0.11519, val loss: 3.1578, patience: 364, training metric: 1.0, val metric: 0.1124\n",
            "INFO - root - epoch: 468, training loss: 0.11558, val loss: 3.15458, patience: 365, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 469, training loss: 0.11502, val loss: 3.14934, patience: 366, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 470, training loss: 0.115, val loss: 3.16005, patience: 367, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 471, training loss: 0.11482, val loss: 3.16024, patience: 368, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 472, training loss: 0.11455, val loss: 3.1617, patience: 369, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 473, training loss: 0.11495, val loss: 3.15026, patience: 370, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 474, training loss: 0.11503, val loss: 3.15904, patience: 371, training metric: 1.0, val metric: 0.11\n",
            "INFO - root - epoch: 475, training loss: 0.11557, val loss: 3.16126, patience: 372, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 476, training loss: 0.11484, val loss: 3.15513, patience: 373, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 477, training loss: 0.11564, val loss: 3.15735, patience: 374, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 478, training loss: 0.11514, val loss: 3.15608, patience: 375, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 479, training loss: 0.11497, val loss: 3.15997, patience: 376, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 480, training loss: 0.11525, val loss: 3.1597, patience: 377, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 481, training loss: 0.11525, val loss: 3.14863, patience: 378, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 482, training loss: 0.11426, val loss: 3.1577, patience: 379, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 483, training loss: 0.11486, val loss: 3.15581, patience: 380, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 484, training loss: 0.11541, val loss: 3.15338, patience: 381, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 485, training loss: 0.11477, val loss: 3.16167, patience: 382, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 486, training loss: 0.11504, val loss: 3.15836, patience: 383, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 487, training loss: 0.11475, val loss: 3.15591, patience: 384, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 488, training loss: 0.11476, val loss: 3.16228, patience: 385, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 489, training loss: 0.11459, val loss: 3.15632, patience: 386, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 490, training loss: 0.11444, val loss: 3.15395, patience: 387, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 491, training loss: 0.11445, val loss: 3.15461, patience: 388, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 492, training loss: 0.11476, val loss: 3.15434, patience: 389, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 493, training loss: 0.11486, val loss: 3.15768, patience: 390, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 494, training loss: 0.11448, val loss: 3.15676, patience: 391, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 495, training loss: 0.11428, val loss: 3.16029, patience: 392, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 496, training loss: 0.11452, val loss: 3.15881, patience: 393, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 497, training loss: 0.11473, val loss: 3.16119, patience: 394, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 498, training loss: 0.11493, val loss: 3.15788, patience: 395, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 499, training loss: 0.11442, val loss: 3.15812, patience: 396, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 500, training loss: 0.11449, val loss: 3.16101, patience: 397, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 501, training loss: 0.11491, val loss: 3.15628, patience: 398, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 502, training loss: 0.11565, val loss: 3.15715, patience: 399, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 503, training loss: 0.1153, val loss: 3.16256, patience: 400, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 504, training loss: 0.1154, val loss: 3.15693, patience: 401, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 505, training loss: 0.11537, val loss: 3.15244, patience: 402, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 506, training loss: 0.11531, val loss: 3.15777, patience: 403, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 507, training loss: 0.11516, val loss: 3.16343, patience: 404, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 508, training loss: 0.11566, val loss: 3.15812, patience: 405, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 509, training loss: 0.11462, val loss: 3.16113, patience: 406, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 510, training loss: 0.1151, val loss: 3.15943, patience: 407, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 511, training loss: 0.11563, val loss: 3.15343, patience: 408, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 512, training loss: 0.11467, val loss: 3.15766, patience: 409, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 513, training loss: 0.1148, val loss: 3.1559, patience: 410, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 514, training loss: 0.11517, val loss: 3.15839, patience: 411, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 515, training loss: 0.11474, val loss: 3.15707, patience: 412, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 516, training loss: 0.11451, val loss: 3.16117, patience: 413, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 517, training loss: 0.1152, val loss: 3.15839, patience: 414, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 518, training loss: 0.1147, val loss: 3.15813, patience: 415, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 519, training loss: 0.11453, val loss: 3.15939, patience: 416, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 520, training loss: 0.11465, val loss: 3.16073, patience: 417, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 521, training loss: 0.11453, val loss: 3.15901, patience: 418, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 522, training loss: 0.11447, val loss: 3.16031, patience: 419, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 523, training loss: 0.11477, val loss: 3.16153, patience: 420, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 524, training loss: 0.11458, val loss: 3.16168, patience: 421, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 525, training loss: 0.11395, val loss: 3.17033, patience: 422, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 526, training loss: 0.11464, val loss: 3.16192, patience: 423, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 527, training loss: 0.11415, val loss: 3.16582, patience: 424, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 528, training loss: 0.11399, val loss: 3.1606, patience: 425, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 529, training loss: 0.11418, val loss: 3.16682, patience: 426, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 530, training loss: 0.11443, val loss: 3.16285, patience: 427, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 531, training loss: 0.11416, val loss: 3.162, patience: 428, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 532, training loss: 0.11434, val loss: 3.16332, patience: 429, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 533, training loss: 0.11379, val loss: 3.16783, patience: 430, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 534, training loss: 0.1142, val loss: 3.16828, patience: 431, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 535, training loss: 0.11354, val loss: 3.16622, patience: 432, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 536, training loss: 0.11374, val loss: 3.16876, patience: 433, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 537, training loss: 0.11399, val loss: 3.16344, patience: 434, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 538, training loss: 0.11412, val loss: 3.16162, patience: 435, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 539, training loss: 0.11405, val loss: 3.16409, patience: 436, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 540, training loss: 0.11363, val loss: 3.16384, patience: 437, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 541, training loss: 0.11443, val loss: 3.16073, patience: 438, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 542, training loss: 0.11375, val loss: 3.15672, patience: 439, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 543, training loss: 0.1138, val loss: 3.15921, patience: 440, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 544, training loss: 0.1134, val loss: 3.15971, patience: 441, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 545, training loss: 0.11351, val loss: 3.16492, patience: 442, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 546, training loss: 0.11392, val loss: 3.15174, patience: 443, training metric: 1.0, val metric: 0.1136\n",
            "INFO - root - epoch: 547, training loss: 0.11351, val loss: 3.1606, patience: 444, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 548, training loss: 0.11385, val loss: 3.16086, patience: 445, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 549, training loss: 0.11414, val loss: 3.15679, patience: 446, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 550, training loss: 0.11379, val loss: 3.15855, patience: 447, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 551, training loss: 0.11362, val loss: 3.15659, patience: 448, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 552, training loss: 0.11401, val loss: 3.1565, patience: 449, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 553, training loss: 0.11361, val loss: 3.15783, patience: 450, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 554, training loss: 0.11407, val loss: 3.16436, patience: 451, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 555, training loss: 0.11352, val loss: 3.15812, patience: 452, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 556, training loss: 0.11383, val loss: 3.15026, patience: 453, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 557, training loss: 0.11413, val loss: 3.1536, patience: 454, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 558, training loss: 0.11379, val loss: 3.15288, patience: 455, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 559, training loss: 0.11326, val loss: 3.15786, patience: 456, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 560, training loss: 0.11354, val loss: 3.16104, patience: 457, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 561, training loss: 0.1142, val loss: 3.1533, patience: 458, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 562, training loss: 0.11355, val loss: 3.15398, patience: 459, training metric: 1.0, val metric: 0.0956\n",
            "INFO - root - epoch: 563, training loss: 0.11352, val loss: 3.15615, patience: 460, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 564, training loss: 0.11268, val loss: 3.1498, patience: 461, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 565, training loss: 0.11379, val loss: 3.15215, patience: 462, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 566, training loss: 0.11339, val loss: 3.14749, patience: 463, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 567, training loss: 0.11369, val loss: 3.15292, patience: 464, training metric: 1.0, val metric: 0.0924\n",
            "INFO - root - epoch: 568, training loss: 0.11318, val loss: 3.15697, patience: 465, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 569, training loss: 0.11344, val loss: 3.15695, patience: 466, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 570, training loss: 0.1133, val loss: 3.1496, patience: 467, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 571, training loss: 0.11379, val loss: 3.15657, patience: 468, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 572, training loss: 0.11308, val loss: 3.15345, patience: 469, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 573, training loss: 0.11351, val loss: 3.15773, patience: 470, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 574, training loss: 0.1136, val loss: 3.1549, patience: 471, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 575, training loss: 0.11286, val loss: 3.15688, patience: 472, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 576, training loss: 0.11324, val loss: 3.14625, patience: 473, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 577, training loss: 0.11352, val loss: 3.15653, patience: 474, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 578, training loss: 0.11372, val loss: 3.15992, patience: 475, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 579, training loss: 0.11309, val loss: 3.15501, patience: 476, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 580, training loss: 0.11293, val loss: 3.15414, patience: 477, training metric: 1.0, val metric: 0.1136\n",
            "INFO - root - epoch: 581, training loss: 0.11274, val loss: 3.15606, patience: 478, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 582, training loss: 0.11255, val loss: 3.15173, patience: 479, training metric: 1.0, val metric: 0.1124\n",
            "INFO - root - epoch: 583, training loss: 0.11277, val loss: 3.15075, patience: 480, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 584, training loss: 0.11366, val loss: 3.15761, patience: 481, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 585, training loss: 0.11283, val loss: 3.15895, patience: 482, training metric: 1.0, val metric: 0.1116\n",
            "INFO - root - epoch: 586, training loss: 0.1134, val loss: 3.16063, patience: 483, training metric: 1.0, val metric: 0.1096\n",
            "INFO - root - epoch: 587, training loss: 0.11276, val loss: 3.16394, patience: 484, training metric: 1.0, val metric: 0.1132\n",
            "INFO - root - epoch: 588, training loss: 0.11275, val loss: 3.16052, patience: 485, training metric: 1.0, val metric: 0.114\n",
            "INFO - root - epoch: 589, training loss: 0.11352, val loss: 3.16206, patience: 486, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 590, training loss: 0.11268, val loss: 3.16295, patience: 487, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 591, training loss: 0.11268, val loss: 3.15159, patience: 488, training metric: 1.0, val metric: 0.1116\n",
            "INFO - root - epoch: 592, training loss: 0.11232, val loss: 3.16592, patience: 489, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 593, training loss: 0.11221, val loss: 3.16842, patience: 490, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 594, training loss: 0.11252, val loss: 3.15705, patience: 491, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 595, training loss: 0.1125, val loss: 3.15767, patience: 492, training metric: 1.0, val metric: 0.1124\n",
            "INFO - root - epoch: 596, training loss: 0.11237, val loss: 3.16435, patience: 493, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 597, training loss: 0.11219, val loss: 3.16081, patience: 494, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 598, training loss: 0.11299, val loss: 3.15878, patience: 495, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 599, training loss: 0.11235, val loss: 3.15628, patience: 496, training metric: 1.0, val metric: 0.1116\n",
            "INFO - root - epoch: 600, training loss: 0.11171, val loss: 3.16489, patience: 497, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 601, training loss: 0.11248, val loss: 3.16408, patience: 498, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 602, training loss: 0.1119, val loss: 3.16308, patience: 499, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 603, training loss: 0.11213, val loss: 3.16295, patience: 500, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 604, training loss: 0.1126, val loss: 3.16263, patience: 501, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 605, training loss: 0.11187, val loss: 3.1609, patience: 502, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 606, training loss: 0.11176, val loss: 3.16127, patience: 503, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 607, training loss: 0.11197, val loss: 3.16316, patience: 504, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 608, training loss: 0.11208, val loss: 3.1634, patience: 505, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 609, training loss: 0.11239, val loss: 3.16237, patience: 506, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 610, training loss: 0.11267, val loss: 3.16056, patience: 507, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 611, training loss: 0.11252, val loss: 3.16036, patience: 508, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 612, training loss: 0.11223, val loss: 3.16048, patience: 509, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 613, training loss: 0.11254, val loss: 3.16546, patience: 510, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 614, training loss: 0.1121, val loss: 3.16374, patience: 511, training metric: 1.0, val metric: 0.11\n",
            "INFO - root - epoch: 615, training loss: 0.11229, val loss: 3.16156, patience: 512, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 616, training loss: 0.11214, val loss: 3.1611, patience: 513, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 617, training loss: 0.11164, val loss: 3.16194, patience: 514, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 618, training loss: 0.11263, val loss: 3.16334, patience: 515, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 619, training loss: 0.11184, val loss: 3.17297, patience: 516, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 620, training loss: 0.11225, val loss: 3.16018, patience: 517, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 621, training loss: 0.11186, val loss: 3.16784, patience: 518, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 622, training loss: 0.1119, val loss: 3.16903, patience: 519, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 623, training loss: 0.11158, val loss: 3.16621, patience: 520, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 624, training loss: 0.11197, val loss: 3.15989, patience: 521, training metric: 1.0, val metric: 0.1144\n",
            "INFO - root - epoch: 625, training loss: 0.11175, val loss: 3.1584, patience: 522, training metric: 1.0, val metric: 0.1172\n",
            "INFO - root - epoch: 626, training loss: 0.11203, val loss: 3.1606, patience: 523, training metric: 1.0, val metric: 0.1152\n",
            "INFO - root - epoch: 627, training loss: 0.1122, val loss: 3.16556, patience: 524, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 628, training loss: 0.11154, val loss: 3.16868, patience: 525, training metric: 1.0, val metric: 0.11\n",
            "INFO - root - epoch: 629, training loss: 0.11215, val loss: 3.16228, patience: 526, training metric: 1.0, val metric: 0.1124\n",
            "INFO - root - epoch: 630, training loss: 0.11135, val loss: 3.1651, patience: 527, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 631, training loss: 0.11138, val loss: 3.16805, patience: 528, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 632, training loss: 0.11153, val loss: 3.1651, patience: 529, training metric: 1.0, val metric: 0.1112\n",
            "INFO - root - epoch: 633, training loss: 0.11161, val loss: 3.16726, patience: 530, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 634, training loss: 0.11231, val loss: 3.16479, patience: 531, training metric: 1.0, val metric: 0.1164\n",
            "INFO - root - epoch: 635, training loss: 0.11118, val loss: 3.16651, patience: 532, training metric: 1.0, val metric: 0.1096\n",
            "INFO - root - epoch: 636, training loss: 0.11197, val loss: 3.1661, patience: 533, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 637, training loss: 0.11156, val loss: 3.1707, patience: 534, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 638, training loss: 0.11165, val loss: 3.17007, patience: 535, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 639, training loss: 0.11122, val loss: 3.16854, patience: 536, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 640, training loss: 0.11164, val loss: 3.17384, patience: 537, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 641, training loss: 0.11166, val loss: 3.17366, patience: 538, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 642, training loss: 0.1117, val loss: 3.17057, patience: 539, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 643, training loss: 0.11116, val loss: 3.1719, patience: 540, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 644, training loss: 0.1113, val loss: 3.16848, patience: 541, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 645, training loss: 0.11133, val loss: 3.1734, patience: 542, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 646, training loss: 0.11091, val loss: 3.16935, patience: 543, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 647, training loss: 0.11083, val loss: 3.17153, patience: 544, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 648, training loss: 0.11096, val loss: 3.16517, patience: 545, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 649, training loss: 0.11072, val loss: 3.16854, patience: 546, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 650, training loss: 0.11121, val loss: 3.16938, patience: 547, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 651, training loss: 0.11055, val loss: 3.17048, patience: 548, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 652, training loss: 0.1108, val loss: 3.1663, patience: 549, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 653, training loss: 0.11038, val loss: 3.16417, patience: 550, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 654, training loss: 0.11096, val loss: 3.16688, patience: 551, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 655, training loss: 0.1109, val loss: 3.16707, patience: 552, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 656, training loss: 0.11054, val loss: 3.16635, patience: 553, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 657, training loss: 0.11136, val loss: 3.16282, patience: 554, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 658, training loss: 0.11061, val loss: 3.16204, patience: 555, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 659, training loss: 0.1108, val loss: 3.1601, patience: 556, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 660, training loss: 0.11044, val loss: 3.1654, patience: 557, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 661, training loss: 0.11029, val loss: 3.16315, patience: 558, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 662, training loss: 0.11099, val loss: 3.16707, patience: 559, training metric: 1.0, val metric: 0.106\n",
            "INFO - root - epoch: 663, training loss: 0.11053, val loss: 3.16688, patience: 560, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 664, training loss: 0.11107, val loss: 3.164, patience: 561, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 665, training loss: 0.11119, val loss: 3.15492, patience: 562, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 666, training loss: 0.11112, val loss: 3.1574, patience: 563, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 667, training loss: 0.11073, val loss: 3.16039, patience: 564, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 668, training loss: 0.11073, val loss: 3.16271, patience: 565, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 669, training loss: 0.11096, val loss: 3.16301, patience: 566, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 670, training loss: 0.11168, val loss: 3.16683, patience: 567, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 671, training loss: 0.11144, val loss: 3.16171, patience: 568, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 672, training loss: 0.11151, val loss: 3.16466, patience: 569, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 673, training loss: 0.11143, val loss: 3.16357, patience: 570, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 674, training loss: 0.11095, val loss: 3.15995, patience: 571, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 675, training loss: 0.11053, val loss: 3.16419, patience: 572, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 676, training loss: 0.11039, val loss: 3.15691, patience: 573, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 677, training loss: 0.11033, val loss: 3.16491, patience: 574, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 678, training loss: 0.11046, val loss: 3.15972, patience: 575, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 679, training loss: 0.11105, val loss: 3.16452, patience: 576, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 680, training loss: 0.11082, val loss: 3.15486, patience: 577, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 681, training loss: 0.11094, val loss: 3.15921, patience: 578, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 682, training loss: 0.11143, val loss: 3.15948, patience: 579, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 683, training loss: 0.11083, val loss: 3.16672, patience: 580, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 684, training loss: 0.11098, val loss: 3.16503, patience: 581, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 685, training loss: 0.11055, val loss: 3.15869, patience: 582, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 686, training loss: 0.11045, val loss: 3.15945, patience: 583, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 687, training loss: 0.11019, val loss: 3.16199, patience: 584, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 688, training loss: 0.11023, val loss: 3.16683, patience: 585, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 689, training loss: 0.10996, val loss: 3.16474, patience: 586, training metric: 1.0, val metric: 0.1084\n",
            "INFO - root - epoch: 690, training loss: 0.11045, val loss: 3.16493, patience: 587, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 691, training loss: 0.11086, val loss: 3.16735, patience: 588, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 692, training loss: 0.11151, val loss: 3.16211, patience: 589, training metric: 1.0, val metric: 0.1088\n",
            "INFO - root - epoch: 693, training loss: 0.11061, val loss: 3.17233, patience: 590, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 694, training loss: 0.1105, val loss: 3.17013, patience: 591, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 695, training loss: 0.11018, val loss: 3.16597, patience: 592, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 696, training loss: 0.10998, val loss: 3.16766, patience: 593, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 697, training loss: 0.10986, val loss: 3.17736, patience: 594, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 698, training loss: 0.11062, val loss: 3.16779, patience: 595, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 699, training loss: 0.11017, val loss: 3.17246, patience: 596, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - loaded best model at epoch 103\n",
            "INFO - root - Best val loss: 2.3107030391693115\n",
            "INFO - root - Best val metric: 0.2108\n",
            "INFO - root - test loss: 3.615215301513672\n",
            "INFO - root - test metric: 0.0\n",
            "INFO - root - epoch: 0, training loss: 2.45003, val loss: 2.30813, patience: 0, training metric: 0.0472, val metric: 0.1\n",
            "INFO - root - epoch: 1, training loss: 2.32546, val loss: 2.30801, patience: 1, training metric: 0.0708, val metric: 0.1\n",
            "INFO - root - epoch: 2, training loss: 2.22378, val loss: 2.30813, patience: 2, training metric: 0.146, val metric: 0.1\n",
            "INFO - root - epoch: 3, training loss: 2.13491, val loss: 2.30803, patience: 3, training metric: 0.2064, val metric: 0.1\n",
            "INFO - root - epoch: 4, training loss: 2.05682, val loss: 2.30817, patience: 4, training metric: 0.2556, val metric: 0.1\n",
            "INFO - root - epoch: 5, training loss: 1.99784, val loss: 2.30873, patience: 5, training metric: 0.2984, val metric: 0.1\n",
            "INFO - root - epoch: 6, training loss: 1.9449, val loss: 2.3091, patience: 6, training metric: 0.3428, val metric: 0.1\n",
            "INFO - root - epoch: 7, training loss: 1.89989, val loss: 2.30963, patience: 7, training metric: 0.4244, val metric: 0.1\n",
            "INFO - root - epoch: 8, training loss: 1.84807, val loss: 2.31033, patience: 8, training metric: 0.516, val metric: 0.1\n",
            "INFO - root - epoch: 9, training loss: 1.80553, val loss: 2.31112, patience: 9, training metric: 0.57, val metric: 0.1\n",
            "INFO - root - epoch: 10, training loss: 1.77712, val loss: 2.31206, patience: 10, training metric: 0.6028, val metric: 0.1\n",
            "INFO - root - epoch: 11, training loss: 1.73565, val loss: 2.31314, patience: 11, training metric: 0.6864, val metric: 0.1\n",
            "INFO - root - epoch: 12, training loss: 1.69316, val loss: 2.31434, patience: 12, training metric: 0.7296, val metric: 0.1\n",
            "INFO - root - epoch: 13, training loss: 1.66294, val loss: 2.31564, patience: 13, training metric: 0.7696, val metric: 0.1\n",
            "INFO - root - epoch: 14, training loss: 1.63175, val loss: 2.31707, patience: 14, training metric: 0.8076, val metric: 0.1\n",
            "INFO - root - epoch: 15, training loss: 1.60123, val loss: 2.3186, patience: 15, training metric: 0.8196, val metric: 0.1\n",
            "INFO - root - epoch: 16, training loss: 1.57669, val loss: 2.32037, patience: 16, training metric: 0.8356, val metric: 0.1\n",
            "INFO - root - epoch: 17, training loss: 1.52803, val loss: 2.32232, patience: 17, training metric: 0.8552, val metric: 0.1\n",
            "INFO - root - epoch: 18, training loss: 1.49417, val loss: 2.32463, patience: 18, training metric: 0.8632, val metric: 0.1\n",
            "INFO - root - epoch: 19, training loss: 1.46683, val loss: 2.32727, patience: 19, training metric: 0.876, val metric: 0.1\n",
            "INFO - root - epoch: 20, training loss: 1.42902, val loss: 2.3304, patience: 20, training metric: 0.8788, val metric: 0.1\n",
            "INFO - root - epoch: 21, training loss: 1.40227, val loss: 2.33412, patience: 21, training metric: 0.8876, val metric: 0.1\n",
            "INFO - root - epoch: 22, training loss: 1.37197, val loss: 2.33826, patience: 22, training metric: 0.888, val metric: 0.1\n",
            "INFO - root - epoch: 23, training loss: 1.34018, val loss: 2.34308, patience: 23, training metric: 0.8916, val metric: 0.1\n",
            "INFO - root - epoch: 24, training loss: 1.30994, val loss: 2.34898, patience: 24, training metric: 0.894, val metric: 0.1\n",
            "INFO - root - epoch: 25, training loss: 1.28487, val loss: 2.35611, patience: 25, training metric: 0.8952, val metric: 0.1\n",
            "INFO - root - epoch: 26, training loss: 1.25613, val loss: 2.36438, patience: 26, training metric: 0.8948, val metric: 0.1\n",
            "INFO - root - epoch: 27, training loss: 1.23388, val loss: 2.37266, patience: 27, training metric: 0.898, val metric: 0.1\n",
            "INFO - root - epoch: 28, training loss: 1.21015, val loss: 2.38082, patience: 28, training metric: 0.8976, val metric: 0.1\n",
            "INFO - root - epoch: 29, training loss: 1.18393, val loss: 2.38976, patience: 29, training metric: 0.8992, val metric: 0.1\n",
            "INFO - root - epoch: 30, training loss: 1.15306, val loss: 2.39496, patience: 30, training metric: 0.8988, val metric: 0.1\n",
            "INFO - root - epoch: 31, training loss: 1.13321, val loss: 2.39792, patience: 31, training metric: 0.8984, val metric: 0.1\n",
            "INFO - root - epoch: 32, training loss: 1.10077, val loss: 2.39916, patience: 32, training metric: 0.8992, val metric: 0.1\n",
            "INFO - root - epoch: 33, training loss: 1.07949, val loss: 2.39941, patience: 33, training metric: 0.8996, val metric: 0.1\n",
            "INFO - root - epoch: 34, training loss: 1.0506, val loss: 2.40169, patience: 34, training metric: 0.8992, val metric: 0.1\n",
            "INFO - root - epoch: 35, training loss: 1.02452, val loss: 2.40561, patience: 35, training metric: 0.9, val metric: 0.1\n",
            "INFO - root - epoch: 36, training loss: 1.00221, val loss: 2.4047, patience: 36, training metric: 0.9, val metric: 0.1\n",
            "INFO - root - epoch: 37, training loss: 0.97373, val loss: 2.40317, patience: 37, training metric: 0.9, val metric: 0.1\n",
            "INFO - root - epoch: 38, training loss: 0.96176, val loss: 2.4005, patience: 38, training metric: 0.9, val metric: 0.1\n",
            "INFO - root - epoch: 39, training loss: 0.93905, val loss: 2.40026, patience: 39, training metric: 0.9004, val metric: 0.1\n",
            "INFO - root - epoch: 40, training loss: 0.91483, val loss: 2.39784, patience: 40, training metric: 0.9024, val metric: 0.1\n",
            "INFO - root - epoch: 41, training loss: 0.89472, val loss: 2.3955, patience: 41, training metric: 0.9052, val metric: 0.1\n",
            "INFO - root - epoch: 42, training loss: 0.87353, val loss: 2.39865, patience: 0, training metric: 0.9064, val metric: 0.1004\n",
            "INFO - root - epoch: 43, training loss: 0.85596, val loss: 2.40283, patience: 0, training metric: 0.9108, val metric: 0.1284\n",
            "INFO - root - epoch: 44, training loss: 0.83408, val loss: 2.41241, patience: 1, training metric: 0.9156, val metric: 0.1084\n",
            "INFO - root - epoch: 45, training loss: 0.81374, val loss: 2.42624, patience: 2, training metric: 0.922, val metric: 0.0984\n",
            "INFO - root - epoch: 46, training loss: 0.79756, val loss: 2.43144, patience: 3, training metric: 0.926, val metric: 0.0988\n",
            "INFO - root - epoch: 47, training loss: 0.77757, val loss: 2.44236, patience: 4, training metric: 0.9344, val metric: 0.102\n",
            "INFO - root - epoch: 48, training loss: 0.75868, val loss: 2.44489, patience: 5, training metric: 0.9416, val metric: 0.1\n",
            "INFO - root - epoch: 49, training loss: 0.74142, val loss: 2.45916, patience: 6, training metric: 0.944, val metric: 0.1024\n",
            "INFO - root - epoch: 50, training loss: 0.72752, val loss: 2.49626, patience: 7, training metric: 0.9548, val metric: 0.106\n",
            "INFO - root - epoch: 51, training loss: 0.70663, val loss: 2.5284, patience: 8, training metric: 0.9684, val metric: 0.1204\n",
            "INFO - root - epoch: 52, training loss: 0.69459, val loss: 2.63994, patience: 9, training metric: 0.9764, val metric: 0.1\n",
            "INFO - root - epoch: 53, training loss: 0.67469, val loss: 2.95631, patience: 10, training metric: 0.9756, val metric: 0.1\n",
            "INFO - root - epoch: 54, training loss: 0.65903, val loss: 3.31285, patience: 11, training metric: 0.9776, val metric: 0.1\n",
            "INFO - root - epoch: 55, training loss: 0.64388, val loss: 3.79795, patience: 12, training metric: 0.9836, val metric: 0.1\n",
            "INFO - root - epoch: 56, training loss: 0.63088, val loss: 4.37016, patience: 13, training metric: 0.9936, val metric: 0.1\n",
            "INFO - root - epoch: 57, training loss: 0.61591, val loss: 4.92948, patience: 14, training metric: 0.9952, val metric: 0.1\n",
            "INFO - root - epoch: 58, training loss: 0.60458, val loss: 5.42494, patience: 15, training metric: 0.9964, val metric: 0.1\n",
            "INFO - root - epoch: 59, training loss: 0.58952, val loss: 5.71193, patience: 16, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 60, training loss: 0.5709, val loss: 5.76915, patience: 17, training metric: 0.9992, val metric: 0.1\n",
            "INFO - root - epoch: 61, training loss: 0.55921, val loss: 5.514, patience: 18, training metric: 0.9984, val metric: 0.1\n",
            "INFO - root - epoch: 62, training loss: 0.54194, val loss: 5.11011, patience: 19, training metric: 0.9992, val metric: 0.1\n",
            "INFO - root - epoch: 63, training loss: 0.53101, val loss: 5.15957, patience: 20, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 64, training loss: 0.51798, val loss: 5.21296, patience: 21, training metric: 0.9992, val metric: 0.1\n",
            "INFO - root - epoch: 65, training loss: 0.50393, val loss: 5.16647, patience: 22, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 66, training loss: 0.49096, val loss: 5.05593, patience: 23, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 67, training loss: 0.48044, val loss: 4.66928, patience: 24, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 68, training loss: 0.46821, val loss: 4.31447, patience: 25, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 69, training loss: 0.45679, val loss: 4.35245, patience: 0, training metric: 1.0, val metric: 0.2084\n",
            "INFO - root - epoch: 70, training loss: 0.4463, val loss: 4.19358, patience: 1, training metric: 1.0, val metric: 0.1612\n",
            "INFO - root - epoch: 71, training loss: 0.43461, val loss: 4.00447, patience: 2, training metric: 1.0, val metric: 0.1184\n",
            "INFO - root - epoch: 72, training loss: 0.42489, val loss: 3.98487, patience: 3, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 73, training loss: 0.41439, val loss: 3.95942, patience: 4, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 74, training loss: 0.4054, val loss: 3.76922, patience: 5, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 75, training loss: 0.39474, val loss: 3.61533, patience: 6, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 76, training loss: 0.3845, val loss: 3.60383, patience: 7, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 77, training loss: 0.37395, val loss: 3.72157, patience: 8, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 78, training loss: 0.36525, val loss: 3.92669, patience: 9, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 79, training loss: 0.35673, val loss: 4.16187, patience: 10, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 80, training loss: 0.34613, val loss: 4.33484, patience: 11, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 81, training loss: 0.33755, val loss: 4.37501, patience: 12, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 82, training loss: 0.33069, val loss: 4.39763, patience: 13, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 83, training loss: 0.32206, val loss: 4.49358, patience: 14, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 84, training loss: 0.31438, val loss: 4.57314, patience: 15, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 85, training loss: 0.30707, val loss: 4.54186, patience: 16, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 86, training loss: 0.29951, val loss: 4.52842, patience: 17, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 87, training loss: 0.29204, val loss: 4.49296, patience: 18, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 88, training loss: 0.28462, val loss: 4.5871, patience: 19, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 89, training loss: 0.27745, val loss: 4.66313, patience: 20, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 90, training loss: 0.27254, val loss: 4.71415, patience: 21, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 91, training loss: 0.2663, val loss: 4.74385, patience: 22, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 92, training loss: 0.25962, val loss: 4.74585, patience: 23, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 93, training loss: 0.25459, val loss: 4.67146, patience: 24, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 94, training loss: 0.24804, val loss: 4.53217, patience: 25, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 95, training loss: 0.24202, val loss: 4.24722, patience: 26, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 96, training loss: 0.2374, val loss: 4.03185, patience: 27, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 97, training loss: 0.23277, val loss: 4.05529, patience: 28, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 98, training loss: 0.22832, val loss: 4.21779, patience: 29, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 99, training loss: 0.22238, val loss: 4.1312, patience: 30, training metric: 1.0, val metric: 0.1192\n",
            "INFO - root - epoch: 100, training loss: 0.21729, val loss: 3.8573, patience: 31, training metric: 1.0, val metric: 0.1192\n",
            "INFO - root - epoch: 101, training loss: 0.21183, val loss: 3.64342, patience: 32, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 102, training loss: 0.20822, val loss: 3.39344, patience: 33, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 103, training loss: 0.20255, val loss: 3.25116, patience: 34, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 104, training loss: 0.19944, val loss: 3.14176, patience: 35, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 105, training loss: 0.19494, val loss: 3.06283, patience: 36, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 106, training loss: 0.1914, val loss: 3.02883, patience: 37, training metric: 1.0, val metric: 0.0908\n",
            "INFO - root - epoch: 107, training loss: 0.18681, val loss: 3.10322, patience: 38, training metric: 1.0, val metric: 0.0868\n",
            "INFO - root - epoch: 108, training loss: 0.18222, val loss: 3.1562, patience: 39, training metric: 1.0, val metric: 0.078\n",
            "INFO - root - epoch: 109, training loss: 0.17921, val loss: 3.1712, patience: 40, training metric: 1.0, val metric: 0.0776\n",
            "INFO - root - epoch: 110, training loss: 0.17549, val loss: 3.18228, patience: 41, training metric: 1.0, val metric: 0.0756\n",
            "INFO - root - epoch: 111, training loss: 0.17112, val loss: 3.19145, patience: 42, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 112, training loss: 0.16953, val loss: 3.09176, patience: 43, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 113, training loss: 0.16554, val loss: 2.93643, patience: 44, training metric: 1.0, val metric: 0.0308\n",
            "INFO - root - epoch: 114, training loss: 0.16229, val loss: 2.81509, patience: 45, training metric: 1.0, val metric: 0.0028\n",
            "INFO - root - epoch: 115, training loss: 0.15918, val loss: 2.80087, patience: 46, training metric: 1.0, val metric: 0.0008\n",
            "INFO - root - epoch: 116, training loss: 0.15621, val loss: 2.81908, patience: 47, training metric: 1.0, val metric: 0.0004\n",
            "INFO - root - epoch: 117, training loss: 0.15343, val loss: 2.83136, patience: 48, training metric: 1.0, val metric: 0.0008\n",
            "INFO - root - epoch: 118, training loss: 0.15003, val loss: 2.81326, patience: 49, training metric: 1.0, val metric: 0.0024\n",
            "INFO - root - epoch: 119, training loss: 0.1475, val loss: 2.82117, patience: 50, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 120, training loss: 0.14424, val loss: 2.87053, patience: 51, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 121, training loss: 0.14241, val loss: 2.86614, patience: 52, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 122, training loss: 0.14081, val loss: 2.83255, patience: 53, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 123, training loss: 0.13932, val loss: 2.79387, patience: 54, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 124, training loss: 0.13843, val loss: 2.78209, patience: 55, training metric: 1.0, val metric: 0.0344\n",
            "INFO - root - epoch: 125, training loss: 0.13751, val loss: 2.77343, patience: 56, training metric: 1.0, val metric: 0.0892\n",
            "INFO - root - epoch: 126, training loss: 0.13607, val loss: 2.79496, patience: 57, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 127, training loss: 0.13467, val loss: 2.81397, patience: 58, training metric: 1.0, val metric: 0.0824\n",
            "INFO - root - epoch: 128, training loss: 0.1333, val loss: 2.82534, patience: 59, training metric: 1.0, val metric: 0.0488\n",
            "INFO - root - epoch: 129, training loss: 0.13263, val loss: 2.81445, patience: 60, training metric: 1.0, val metric: 0.0532\n",
            "INFO - root - epoch: 130, training loss: 0.1312, val loss: 2.79905, patience: 61, training metric: 1.0, val metric: 0.0564\n",
            "INFO - root - epoch: 131, training loss: 0.1304, val loss: 2.77533, patience: 62, training metric: 1.0, val metric: 0.0672\n",
            "INFO - root - epoch: 132, training loss: 0.12893, val loss: 2.75565, patience: 63, training metric: 1.0, val metric: 0.0592\n",
            "INFO - root - epoch: 133, training loss: 0.12815, val loss: 2.74837, patience: 64, training metric: 1.0, val metric: 0.044\n",
            "INFO - root - epoch: 134, training loss: 0.12694, val loss: 2.77291, patience: 65, training metric: 1.0, val metric: 0.0316\n",
            "INFO - root - epoch: 135, training loss: 0.12649, val loss: 2.78533, patience: 66, training metric: 1.0, val metric: 0.0156\n",
            "INFO - root - epoch: 136, training loss: 0.1256, val loss: 2.78875, patience: 67, training metric: 1.0, val metric: 0.018\n",
            "INFO - root - epoch: 137, training loss: 0.12379, val loss: 2.79974, patience: 68, training metric: 1.0, val metric: 0.022\n",
            "INFO - root - epoch: 138, training loss: 0.12295, val loss: 2.80509, patience: 69, training metric: 1.0, val metric: 0.0244\n",
            "INFO - root - epoch: 139, training loss: 0.12202, val loss: 2.8144, patience: 70, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 140, training loss: 0.12067, val loss: 2.83681, patience: 71, training metric: 1.0, val metric: 0.0312\n",
            "INFO - root - epoch: 141, training loss: 0.1202, val loss: 2.86147, patience: 72, training metric: 1.0, val metric: 0.0344\n",
            "INFO - root - epoch: 142, training loss: 0.11908, val loss: 2.8714, patience: 73, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 143, training loss: 0.11787, val loss: 2.89041, patience: 74, training metric: 1.0, val metric: 0.0324\n",
            "INFO - root - epoch: 144, training loss: 0.11738, val loss: 2.88802, patience: 75, training metric: 1.0, val metric: 0.0308\n",
            "INFO - root - epoch: 145, training loss: 0.11666, val loss: 2.86976, patience: 76, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 146, training loss: 0.11596, val loss: 2.85958, patience: 77, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 147, training loss: 0.11484, val loss: 2.85451, patience: 78, training metric: 1.0, val metric: 0.0364\n",
            "INFO - root - epoch: 148, training loss: 0.11411, val loss: 2.86642, patience: 79, training metric: 1.0, val metric: 0.026\n",
            "INFO - root - epoch: 149, training loss: 0.11316, val loss: 2.85482, patience: 80, training metric: 1.0, val metric: 0.0236\n",
            "INFO - root - epoch: 150, training loss: 0.112, val loss: 2.8351, patience: 81, training metric: 1.0, val metric: 0.0256\n",
            "INFO - root - epoch: 151, training loss: 0.11157, val loss: 2.82093, patience: 82, training metric: 1.0, val metric: 0.036\n",
            "INFO - root - epoch: 152, training loss: 0.11033, val loss: 2.80976, patience: 83, training metric: 1.0, val metric: 0.0472\n",
            "INFO - root - epoch: 153, training loss: 0.11013, val loss: 2.79214, patience: 84, training metric: 1.0, val metric: 0.0588\n",
            "INFO - root - epoch: 154, training loss: 0.10866, val loss: 2.79429, patience: 85, training metric: 1.0, val metric: 0.074\n",
            "INFO - root - epoch: 155, training loss: 0.108, val loss: 2.78977, patience: 86, training metric: 1.0, val metric: 0.0728\n",
            "INFO - root - epoch: 156, training loss: 0.10719, val loss: 2.78916, patience: 87, training metric: 1.0, val metric: 0.0596\n",
            "INFO - root - epoch: 157, training loss: 0.1069, val loss: 2.79288, patience: 88, training metric: 1.0, val metric: 0.0544\n",
            "INFO - root - epoch: 158, training loss: 0.10611, val loss: 2.7924, patience: 89, training metric: 1.0, val metric: 0.0648\n",
            "INFO - root - epoch: 159, training loss: 0.10489, val loss: 2.77945, patience: 90, training metric: 1.0, val metric: 0.062\n",
            "INFO - root - epoch: 160, training loss: 0.10434, val loss: 2.78234, patience: 91, training metric: 1.0, val metric: 0.068\n",
            "INFO - root - epoch: 161, training loss: 0.10336, val loss: 2.78402, patience: 92, training metric: 1.0, val metric: 0.0632\n",
            "INFO - root - epoch: 162, training loss: 0.10252, val loss: 2.80159, patience: 93, training metric: 1.0, val metric: 0.0672\n",
            "INFO - root - epoch: 163, training loss: 0.1018, val loss: 2.82397, patience: 94, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 164, training loss: 0.10147, val loss: 2.82685, patience: 95, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 165, training loss: 0.10046, val loss: 2.81737, patience: 96, training metric: 1.0, val metric: 0.044\n",
            "INFO - root - epoch: 166, training loss: 0.09964, val loss: 2.80649, patience: 97, training metric: 1.0, val metric: 0.0464\n",
            "INFO - root - epoch: 167, training loss: 0.0988, val loss: 2.7845, patience: 98, training metric: 1.0, val metric: 0.052\n",
            "INFO - root - epoch: 168, training loss: 0.09832, val loss: 2.78131, patience: 99, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 169, training loss: 0.09743, val loss: 2.80459, patience: 100, training metric: 1.0, val metric: 0.0304\n",
            "INFO - root - epoch: 170, training loss: 0.09701, val loss: 2.83165, patience: 101, training metric: 1.0, val metric: 0.0176\n",
            "INFO - root - epoch: 171, training loss: 0.09612, val loss: 2.84671, patience: 102, training metric: 1.0, val metric: 0.0172\n",
            "INFO - root - epoch: 172, training loss: 0.09547, val loss: 2.84268, patience: 103, training metric: 1.0, val metric: 0.0164\n",
            "INFO - root - epoch: 173, training loss: 0.09513, val loss: 2.8376, patience: 104, training metric: 1.0, val metric: 0.0188\n",
            "INFO - root - epoch: 174, training loss: 0.09499, val loss: 2.82805, patience: 105, training metric: 1.0, val metric: 0.0292\n",
            "INFO - root - epoch: 175, training loss: 0.0946, val loss: 2.84428, patience: 106, training metric: 1.0, val metric: 0.0316\n",
            "INFO - root - epoch: 176, training loss: 0.09412, val loss: 2.83862, patience: 107, training metric: 1.0, val metric: 0.032\n",
            "INFO - root - epoch: 177, training loss: 0.09412, val loss: 2.83883, patience: 108, training metric: 1.0, val metric: 0.0296\n",
            "INFO - root - epoch: 178, training loss: 0.09346, val loss: 2.84416, patience: 109, training metric: 1.0, val metric: 0.0312\n",
            "INFO - root - epoch: 179, training loss: 0.09294, val loss: 2.84206, patience: 110, training metric: 1.0, val metric: 0.0236\n",
            "INFO - root - epoch: 180, training loss: 0.09325, val loss: 2.83651, patience: 111, training metric: 1.0, val metric: 0.0188\n",
            "INFO - root - epoch: 181, training loss: 0.0923, val loss: 2.82353, patience: 112, training metric: 1.0, val metric: 0.0164\n",
            "INFO - root - epoch: 182, training loss: 0.09234, val loss: 2.82114, patience: 113, training metric: 1.0, val metric: 0.0136\n",
            "INFO - root - epoch: 183, training loss: 0.09174, val loss: 2.81754, patience: 114, training metric: 1.0, val metric: 0.024\n",
            "INFO - root - epoch: 184, training loss: 0.09181, val loss: 2.79546, patience: 115, training metric: 1.0, val metric: 0.0228\n",
            "INFO - root - epoch: 185, training loss: 0.09155, val loss: 2.78554, patience: 116, training metric: 1.0, val metric: 0.0288\n",
            "INFO - root - epoch: 186, training loss: 0.09153, val loss: 2.77532, patience: 117, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 187, training loss: 0.09109, val loss: 2.78891, patience: 118, training metric: 1.0, val metric: 0.0304\n",
            "INFO - root - epoch: 188, training loss: 0.09081, val loss: 2.78191, patience: 119, training metric: 1.0, val metric: 0.0312\n",
            "INFO - root - epoch: 189, training loss: 0.09016, val loss: 2.78689, patience: 120, training metric: 1.0, val metric: 0.0272\n",
            "INFO - root - epoch: 190, training loss: 0.09017, val loss: 2.7904, patience: 121, training metric: 1.0, val metric: 0.0324\n",
            "INFO - root - epoch: 191, training loss: 0.08939, val loss: 2.80024, patience: 122, training metric: 1.0, val metric: 0.0344\n",
            "INFO - root - epoch: 192, training loss: 0.08913, val loss: 2.79009, patience: 123, training metric: 1.0, val metric: 0.0444\n",
            "INFO - root - epoch: 193, training loss: 0.08882, val loss: 2.79242, patience: 124, training metric: 1.0, val metric: 0.0476\n",
            "INFO - root - epoch: 194, training loss: 0.08855, val loss: 2.78385, patience: 125, training metric: 1.0, val metric: 0.0484\n",
            "INFO - root - epoch: 195, training loss: 0.08814, val loss: 2.79755, patience: 126, training metric: 1.0, val metric: 0.0528\n",
            "INFO - root - epoch: 196, training loss: 0.08773, val loss: 2.79583, patience: 127, training metric: 1.0, val metric: 0.0488\n",
            "INFO - root - epoch: 197, training loss: 0.08746, val loss: 2.80759, patience: 128, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 198, training loss: 0.08739, val loss: 2.80901, patience: 129, training metric: 1.0, val metric: 0.0312\n",
            "INFO - root - epoch: 199, training loss: 0.0872, val loss: 2.82368, patience: 130, training metric: 1.0, val metric: 0.03\n",
            "INFO - root - epoch: 200, training loss: 0.08671, val loss: 2.84064, patience: 131, training metric: 1.0, val metric: 0.0168\n",
            "INFO - root - epoch: 201, training loss: 0.08634, val loss: 2.86244, patience: 132, training metric: 1.0, val metric: 0.0112\n",
            "INFO - root - epoch: 202, training loss: 0.08609, val loss: 2.86476, patience: 133, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 203, training loss: 0.08609, val loss: 2.86302, patience: 134, training metric: 1.0, val metric: 0.0012\n",
            "INFO - root - epoch: 204, training loss: 0.08564, val loss: 2.85886, patience: 135, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 205, training loss: 0.08552, val loss: 2.8584, patience: 136, training metric: 1.0, val metric: 0.0028\n",
            "INFO - root - epoch: 206, training loss: 0.0852, val loss: 2.8481, patience: 137, training metric: 1.0, val metric: 0.002\n",
            "INFO - root - epoch: 207, training loss: 0.085, val loss: 2.85052, patience: 138, training metric: 1.0, val metric: 0.0008\n",
            "INFO - root - epoch: 208, training loss: 0.08418, val loss: 2.84256, patience: 139, training metric: 1.0, val metric: 0.0016\n",
            "INFO - root - epoch: 209, training loss: 0.08447, val loss: 2.83107, patience: 140, training metric: 1.0, val metric: 0.002\n",
            "INFO - root - epoch: 210, training loss: 0.08431, val loss: 2.82736, patience: 141, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 211, training loss: 0.08391, val loss: 2.8245, patience: 142, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 212, training loss: 0.08337, val loss: 2.80461, patience: 143, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 213, training loss: 0.08322, val loss: 2.80164, patience: 144, training metric: 1.0, val metric: 0.016\n",
            "INFO - root - epoch: 214, training loss: 0.08296, val loss: 2.8064, patience: 145, training metric: 1.0, val metric: 0.0216\n",
            "INFO - root - epoch: 215, training loss: 0.08285, val loss: 2.80477, patience: 146, training metric: 1.0, val metric: 0.0296\n",
            "INFO - root - epoch: 216, training loss: 0.08195, val loss: 2.7946, patience: 147, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 217, training loss: 0.08169, val loss: 2.7947, patience: 148, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 218, training loss: 0.0813, val loss: 2.80051, patience: 149, training metric: 1.0, val metric: 0.0456\n",
            "INFO - root - epoch: 219, training loss: 0.08108, val loss: 2.79808, patience: 150, training metric: 1.0, val metric: 0.0528\n",
            "INFO - root - epoch: 220, training loss: 0.08117, val loss: 2.78876, patience: 151, training metric: 1.0, val metric: 0.0528\n",
            "INFO - root - epoch: 221, training loss: 0.08047, val loss: 2.7915, patience: 152, training metric: 1.0, val metric: 0.0536\n",
            "INFO - root - epoch: 222, training loss: 0.08091, val loss: 2.79714, patience: 153, training metric: 1.0, val metric: 0.0524\n",
            "INFO - root - epoch: 223, training loss: 0.08025, val loss: 2.80515, patience: 154, training metric: 1.0, val metric: 0.0456\n",
            "INFO - root - epoch: 224, training loss: 0.08026, val loss: 2.81654, patience: 155, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 225, training loss: 0.08021, val loss: 2.81014, patience: 156, training metric: 1.0, val metric: 0.0308\n",
            "INFO - root - epoch: 226, training loss: 0.08006, val loss: 2.80756, patience: 157, training metric: 1.0, val metric: 0.0248\n",
            "INFO - root - epoch: 227, training loss: 0.07974, val loss: 2.80251, patience: 158, training metric: 1.0, val metric: 0.024\n",
            "INFO - root - epoch: 228, training loss: 0.07977, val loss: 2.81379, patience: 159, training metric: 1.0, val metric: 0.0168\n",
            "INFO - root - epoch: 229, training loss: 0.07984, val loss: 2.80856, patience: 160, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 230, training loss: 0.07975, val loss: 2.80158, patience: 161, training metric: 1.0, val metric: 0.0132\n",
            "INFO - root - epoch: 231, training loss: 0.07932, val loss: 2.79647, patience: 162, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 232, training loss: 0.07895, val loss: 2.78598, patience: 163, training metric: 1.0, val metric: 0.0164\n",
            "INFO - root - epoch: 233, training loss: 0.07908, val loss: 2.79546, patience: 164, training metric: 1.0, val metric: 0.0184\n",
            "INFO - root - epoch: 234, training loss: 0.07878, val loss: 2.78772, patience: 165, training metric: 1.0, val metric: 0.0152\n",
            "INFO - root - epoch: 235, training loss: 0.07873, val loss: 2.78685, patience: 166, training metric: 1.0, val metric: 0.0188\n",
            "INFO - root - epoch: 236, training loss: 0.0784, val loss: 2.78658, patience: 167, training metric: 1.0, val metric: 0.0192\n",
            "INFO - root - epoch: 237, training loss: 0.07841, val loss: 2.78767, patience: 168, training metric: 1.0, val metric: 0.0184\n",
            "INFO - root - epoch: 238, training loss: 0.07831, val loss: 2.79334, patience: 169, training metric: 1.0, val metric: 0.0168\n",
            "INFO - root - epoch: 239, training loss: 0.07787, val loss: 2.79413, patience: 170, training metric: 1.0, val metric: 0.0188\n",
            "INFO - root - epoch: 240, training loss: 0.07832, val loss: 2.79497, patience: 171, training metric: 1.0, val metric: 0.0212\n",
            "INFO - root - epoch: 241, training loss: 0.07784, val loss: 2.80828, patience: 172, training metric: 1.0, val metric: 0.016\n",
            "INFO - root - epoch: 242, training loss: 0.07759, val loss: 2.81104, patience: 173, training metric: 1.0, val metric: 0.018\n",
            "INFO - root - epoch: 243, training loss: 0.0777, val loss: 2.81015, patience: 174, training metric: 1.0, val metric: 0.0248\n",
            "INFO - root - epoch: 244, training loss: 0.07767, val loss: 2.7973, patience: 175, training metric: 1.0, val metric: 0.0208\n",
            "INFO - root - epoch: 245, training loss: 0.07749, val loss: 2.80834, patience: 176, training metric: 1.0, val metric: 0.0284\n",
            "INFO - root - epoch: 246, training loss: 0.07738, val loss: 2.83122, patience: 177, training metric: 1.0, val metric: 0.0236\n",
            "INFO - root - epoch: 247, training loss: 0.07724, val loss: 2.81547, patience: 178, training metric: 1.0, val metric: 0.0244\n",
            "INFO - root - epoch: 248, training loss: 0.07703, val loss: 2.82125, patience: 179, training metric: 1.0, val metric: 0.0228\n",
            "INFO - root - epoch: 249, training loss: 0.07724, val loss: 2.82039, patience: 180, training metric: 1.0, val metric: 0.022\n",
            "INFO - root - epoch: 250, training loss: 0.07672, val loss: 2.83452, patience: 181, training metric: 1.0, val metric: 0.0168\n",
            "INFO - root - epoch: 251, training loss: 0.07671, val loss: 2.83239, patience: 182, training metric: 1.0, val metric: 0.0152\n",
            "INFO - root - epoch: 252, training loss: 0.0764, val loss: 2.84802, patience: 183, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 253, training loss: 0.07618, val loss: 2.8551, patience: 184, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 254, training loss: 0.076, val loss: 2.8606, patience: 185, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 255, training loss: 0.07617, val loss: 2.86515, patience: 186, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 256, training loss: 0.07649, val loss: 2.86709, patience: 187, training metric: 1.0, val metric: 0.0044\n",
            "INFO - root - epoch: 257, training loss: 0.07616, val loss: 2.86908, patience: 188, training metric: 1.0, val metric: 0.0028\n",
            "INFO - root - epoch: 258, training loss: 0.07609, val loss: 2.88031, patience: 189, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 259, training loss: 0.07594, val loss: 2.87952, patience: 190, training metric: 1.0, val metric: 0.0028\n",
            "INFO - root - epoch: 260, training loss: 0.07542, val loss: 2.87117, patience: 191, training metric: 1.0, val metric: 0.002\n",
            "INFO - root - epoch: 261, training loss: 0.07565, val loss: 2.87035, patience: 192, training metric: 1.0, val metric: 0.0012\n",
            "INFO - root - epoch: 262, training loss: 0.07531, val loss: 2.86314, patience: 193, training metric: 1.0, val metric: 0.0024\n",
            "INFO - root - epoch: 263, training loss: 0.07546, val loss: 2.86097, patience: 194, training metric: 1.0, val metric: 0.0016\n",
            "INFO - root - epoch: 264, training loss: 0.07534, val loss: 2.85928, patience: 195, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 265, training loss: 0.07513, val loss: 2.84923, patience: 196, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 266, training loss: 0.07491, val loss: 2.84979, patience: 197, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 267, training loss: 0.07482, val loss: 2.8467, patience: 198, training metric: 1.0, val metric: 0.0036\n",
            "INFO - root - epoch: 268, training loss: 0.07432, val loss: 2.8395, patience: 199, training metric: 1.0, val metric: 0.0144\n",
            "INFO - root - epoch: 269, training loss: 0.0744, val loss: 2.83414, patience: 200, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 270, training loss: 0.0739, val loss: 2.83695, patience: 201, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 271, training loss: 0.07427, val loss: 2.83889, patience: 202, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 272, training loss: 0.07406, val loss: 2.83225, patience: 203, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 273, training loss: 0.07373, val loss: 2.84065, patience: 204, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 274, training loss: 0.07347, val loss: 2.83988, patience: 205, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 275, training loss: 0.07342, val loss: 2.84884, patience: 206, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 276, training loss: 0.07323, val loss: 2.83629, patience: 207, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 277, training loss: 0.07347, val loss: 2.83667, patience: 208, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 278, training loss: 0.07312, val loss: 2.83465, patience: 209, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 279, training loss: 0.07335, val loss: 2.83498, patience: 210, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 280, training loss: 0.07305, val loss: 2.82956, patience: 211, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 281, training loss: 0.07294, val loss: 2.83537, patience: 212, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 282, training loss: 0.07298, val loss: 2.83637, patience: 213, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 283, training loss: 0.07292, val loss: 2.83474, patience: 214, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 284, training loss: 0.07321, val loss: 2.83281, patience: 215, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 285, training loss: 0.07289, val loss: 2.83132, patience: 216, training metric: 1.0, val metric: 0.004\n",
            "INFO - root - epoch: 286, training loss: 0.07286, val loss: 2.83224, patience: 217, training metric: 1.0, val metric: 0.0028\n",
            "INFO - root - epoch: 287, training loss: 0.07283, val loss: 2.83271, patience: 218, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 288, training loss: 0.07247, val loss: 2.83654, patience: 219, training metric: 1.0, val metric: 0.004\n",
            "INFO - root - epoch: 289, training loss: 0.07277, val loss: 2.84019, patience: 220, training metric: 1.0, val metric: 0.0028\n",
            "INFO - root - epoch: 290, training loss: 0.07284, val loss: 2.83338, patience: 221, training metric: 1.0, val metric: 0.0036\n",
            "INFO - root - epoch: 291, training loss: 0.07258, val loss: 2.83823, patience: 222, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 292, training loss: 0.07252, val loss: 2.84724, patience: 223, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 293, training loss: 0.07248, val loss: 2.84878, patience: 224, training metric: 1.0, val metric: 0.004\n",
            "INFO - root - epoch: 294, training loss: 0.07223, val loss: 2.84545, patience: 225, training metric: 1.0, val metric: 0.0012\n",
            "INFO - root - epoch: 295, training loss: 0.07225, val loss: 2.83735, patience: 226, training metric: 1.0, val metric: 0.002\n",
            "INFO - root - epoch: 296, training loss: 0.07224, val loss: 2.83709, patience: 227, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 297, training loss: 0.07212, val loss: 2.83837, patience: 228, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 298, training loss: 0.07214, val loss: 2.84284, patience: 229, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 299, training loss: 0.07215, val loss: 2.83531, patience: 230, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 300, training loss: 0.07215, val loss: 2.84286, patience: 231, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 301, training loss: 0.0717, val loss: 2.83167, patience: 232, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 302, training loss: 0.07205, val loss: 2.83316, patience: 233, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 303, training loss: 0.07188, val loss: 2.83365, patience: 234, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 304, training loss: 0.07191, val loss: 2.82757, patience: 235, training metric: 1.0, val metric: 0.0128\n",
            "INFO - root - epoch: 305, training loss: 0.07187, val loss: 2.82997, patience: 236, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 306, training loss: 0.07189, val loss: 2.83194, patience: 237, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 307, training loss: 0.07188, val loss: 2.84178, patience: 238, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 308, training loss: 0.07162, val loss: 2.84045, patience: 239, training metric: 1.0, val metric: 0.014\n",
            "INFO - root - epoch: 309, training loss: 0.07136, val loss: 2.84662, patience: 240, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 310, training loss: 0.07127, val loss: 2.8376, patience: 241, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 311, training loss: 0.07157, val loss: 2.84509, patience: 242, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 312, training loss: 0.07135, val loss: 2.84091, patience: 243, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 313, training loss: 0.07119, val loss: 2.84108, patience: 244, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 314, training loss: 0.07154, val loss: 2.84466, patience: 245, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 315, training loss: 0.07142, val loss: 2.84431, patience: 246, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 316, training loss: 0.0712, val loss: 2.84587, patience: 247, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 317, training loss: 0.07104, val loss: 2.83479, patience: 248, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 318, training loss: 0.07091, val loss: 2.85087, patience: 249, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 319, training loss: 0.07106, val loss: 2.84279, patience: 250, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 320, training loss: 0.07102, val loss: 2.84279, patience: 251, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 321, training loss: 0.07088, val loss: 2.84508, patience: 252, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 322, training loss: 0.07041, val loss: 2.84121, patience: 253, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 323, training loss: 0.07052, val loss: 2.83635, patience: 254, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 324, training loss: 0.07086, val loss: 2.84293, patience: 255, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 325, training loss: 0.07046, val loss: 2.842, patience: 256, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 326, training loss: 0.07053, val loss: 2.83505, patience: 257, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 327, training loss: 0.07058, val loss: 2.82953, patience: 258, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 328, training loss: 0.07026, val loss: 2.83206, patience: 259, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 329, training loss: 0.0702, val loss: 2.83512, patience: 260, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 330, training loss: 0.07017, val loss: 2.82433, patience: 261, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 331, training loss: 0.07021, val loss: 2.8276, patience: 262, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 332, training loss: 0.07026, val loss: 2.83467, patience: 263, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 333, training loss: 0.07015, val loss: 2.82567, patience: 264, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 334, training loss: 0.0703, val loss: 2.82631, patience: 265, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 335, training loss: 0.07014, val loss: 2.82381, patience: 266, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 336, training loss: 0.0703, val loss: 2.83877, patience: 267, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 337, training loss: 0.07023, val loss: 2.82627, patience: 268, training metric: 1.0, val metric: 0.0152\n",
            "INFO - root - epoch: 338, training loss: 0.0701, val loss: 2.82258, patience: 269, training metric: 1.0, val metric: 0.0112\n",
            "INFO - root - epoch: 339, training loss: 0.07012, val loss: 2.82444, patience: 270, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 340, training loss: 0.06973, val loss: 2.82567, patience: 271, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 341, training loss: 0.07021, val loss: 2.82873, patience: 272, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 342, training loss: 0.07051, val loss: 2.82741, patience: 273, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 343, training loss: 0.07009, val loss: 2.8281, patience: 274, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 344, training loss: 0.06998, val loss: 2.82658, patience: 275, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 345, training loss: 0.06996, val loss: 2.83496, patience: 276, training metric: 1.0, val metric: 0.0044\n",
            "INFO - root - epoch: 346, training loss: 0.06996, val loss: 2.83942, patience: 277, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 347, training loss: 0.0701, val loss: 2.83971, patience: 278, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 348, training loss: 0.06989, val loss: 2.84185, patience: 279, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 349, training loss: 0.06996, val loss: 2.84174, patience: 280, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 350, training loss: 0.06975, val loss: 2.84569, patience: 281, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 351, training loss: 0.06953, val loss: 2.84406, patience: 282, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 352, training loss: 0.06957, val loss: 2.84287, patience: 283, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 353, training loss: 0.0695, val loss: 2.84288, patience: 284, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 354, training loss: 0.06969, val loss: 2.83628, patience: 285, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 355, training loss: 0.0695, val loss: 2.84331, patience: 286, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 356, training loss: 0.06905, val loss: 2.83937, patience: 287, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 357, training loss: 0.06932, val loss: 2.8392, patience: 288, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 358, training loss: 0.06917, val loss: 2.83943, patience: 289, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 359, training loss: 0.06962, val loss: 2.83758, patience: 290, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 360, training loss: 0.06894, val loss: 2.84376, patience: 291, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 361, training loss: 0.0692, val loss: 2.84577, patience: 292, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 362, training loss: 0.06925, val loss: 2.84108, patience: 293, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 363, training loss: 0.06971, val loss: 2.84216, patience: 294, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 364, training loss: 0.0692, val loss: 2.83788, patience: 295, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 365, training loss: 0.0691, val loss: 2.84424, patience: 296, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 366, training loss: 0.06911, val loss: 2.84752, patience: 297, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 367, training loss: 0.06911, val loss: 2.84769, patience: 298, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 368, training loss: 0.06931, val loss: 2.84783, patience: 299, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 369, training loss: 0.0692, val loss: 2.83952, patience: 300, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 370, training loss: 0.0689, val loss: 2.84305, patience: 301, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 371, training loss: 0.06891, val loss: 2.8536, patience: 302, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 372, training loss: 0.06893, val loss: 2.84146, patience: 303, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 373, training loss: 0.06876, val loss: 2.85511, patience: 304, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 374, training loss: 0.06894, val loss: 2.84811, patience: 305, training metric: 1.0, val metric: 0.0044\n",
            "INFO - root - epoch: 375, training loss: 0.06879, val loss: 2.85686, patience: 306, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 376, training loss: 0.06892, val loss: 2.84802, patience: 307, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 377, training loss: 0.06873, val loss: 2.84239, patience: 308, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 378, training loss: 0.06874, val loss: 2.85348, patience: 309, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 379, training loss: 0.06857, val loss: 2.85133, patience: 310, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 380, training loss: 0.06863, val loss: 2.84081, patience: 311, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 381, training loss: 0.06872, val loss: 2.85148, patience: 312, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 382, training loss: 0.06854, val loss: 2.84819, patience: 313, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 383, training loss: 0.06855, val loss: 2.84784, patience: 314, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 384, training loss: 0.0685, val loss: 2.84885, patience: 315, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 385, training loss: 0.06841, val loss: 2.8412, patience: 316, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 386, training loss: 0.06864, val loss: 2.84591, patience: 317, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 387, training loss: 0.06825, val loss: 2.84458, patience: 318, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 388, training loss: 0.0686, val loss: 2.84917, patience: 319, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 389, training loss: 0.06845, val loss: 2.85239, patience: 320, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 390, training loss: 0.06851, val loss: 2.84503, patience: 321, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 391, training loss: 0.06824, val loss: 2.84606, patience: 322, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 392, training loss: 0.06837, val loss: 2.83869, patience: 323, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 393, training loss: 0.06831, val loss: 2.8415, patience: 324, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 394, training loss: 0.06867, val loss: 2.84983, patience: 325, training metric: 1.0, val metric: 0.0112\n",
            "INFO - root - epoch: 395, training loss: 0.06889, val loss: 2.84929, patience: 326, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 396, training loss: 0.06865, val loss: 2.8428, patience: 327, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 397, training loss: 0.06851, val loss: 2.84746, patience: 328, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 398, training loss: 0.06833, val loss: 2.83165, patience: 329, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 399, training loss: 0.0683, val loss: 2.84854, patience: 330, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 400, training loss: 0.06824, val loss: 2.84587, patience: 331, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 401, training loss: 0.06845, val loss: 2.84966, patience: 332, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 402, training loss: 0.06835, val loss: 2.84367, patience: 333, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 403, training loss: 0.06821, val loss: 2.83875, patience: 334, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 404, training loss: 0.06833, val loss: 2.84584, patience: 335, training metric: 1.0, val metric: 0.0144\n",
            "INFO - root - epoch: 405, training loss: 0.06835, val loss: 2.84023, patience: 336, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 406, training loss: 0.06824, val loss: 2.83852, patience: 337, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 407, training loss: 0.06828, val loss: 2.84992, patience: 338, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 408, training loss: 0.0682, val loss: 2.85281, patience: 339, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 409, training loss: 0.06843, val loss: 2.84488, patience: 340, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 410, training loss: 0.06822, val loss: 2.85487, patience: 341, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 411, training loss: 0.06822, val loss: 2.84872, patience: 342, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 412, training loss: 0.06823, val loss: 2.84768, patience: 343, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 413, training loss: 0.06821, val loss: 2.84955, patience: 344, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 414, training loss: 0.0681, val loss: 2.85419, patience: 345, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 415, training loss: 0.06812, val loss: 2.84457, patience: 346, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 416, training loss: 0.06801, val loss: 2.85655, patience: 347, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 417, training loss: 0.06823, val loss: 2.84271, patience: 348, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 418, training loss: 0.06822, val loss: 2.84303, patience: 349, training metric: 1.0, val metric: 0.0128\n",
            "INFO - root - epoch: 419, training loss: 0.06816, val loss: 2.85259, patience: 350, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 420, training loss: 0.06807, val loss: 2.84376, patience: 351, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 421, training loss: 0.0683, val loss: 2.85272, patience: 352, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 422, training loss: 0.06829, val loss: 2.84953, patience: 353, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 423, training loss: 0.06795, val loss: 2.85267, patience: 354, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 424, training loss: 0.06819, val loss: 2.84821, patience: 355, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 425, training loss: 0.06809, val loss: 2.84445, patience: 356, training metric: 1.0, val metric: 0.0112\n",
            "INFO - root - epoch: 426, training loss: 0.06802, val loss: 2.8489, patience: 357, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 427, training loss: 0.06797, val loss: 2.84847, patience: 358, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 428, training loss: 0.06799, val loss: 2.84711, patience: 359, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 429, training loss: 0.06809, val loss: 2.8478, patience: 360, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 430, training loss: 0.06809, val loss: 2.84714, patience: 361, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 431, training loss: 0.06782, val loss: 2.84393, patience: 362, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 432, training loss: 0.06795, val loss: 2.84571, patience: 363, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 433, training loss: 0.06806, val loss: 2.85174, patience: 364, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 434, training loss: 0.06783, val loss: 2.84399, patience: 365, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 435, training loss: 0.0679, val loss: 2.83921, patience: 366, training metric: 1.0, val metric: 0.0136\n",
            "INFO - root - epoch: 436, training loss: 0.06794, val loss: 2.83865, patience: 367, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 437, training loss: 0.06784, val loss: 2.84766, patience: 368, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 438, training loss: 0.06811, val loss: 2.83488, patience: 369, training metric: 1.0, val metric: 0.0112\n",
            "INFO - root - epoch: 439, training loss: 0.06788, val loss: 2.84341, patience: 370, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 440, training loss: 0.06757, val loss: 2.8455, patience: 371, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 441, training loss: 0.06779, val loss: 2.8467, patience: 372, training metric: 1.0, val metric: 0.0128\n",
            "INFO - root - epoch: 442, training loss: 0.06797, val loss: 2.84184, patience: 373, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 443, training loss: 0.06792, val loss: 2.84809, patience: 374, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 444, training loss: 0.06765, val loss: 2.85312, patience: 375, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 445, training loss: 0.06787, val loss: 2.84384, patience: 376, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 446, training loss: 0.0679, val loss: 2.84568, patience: 377, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 447, training loss: 0.06779, val loss: 2.84615, patience: 378, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 448, training loss: 0.0676, val loss: 2.84846, patience: 379, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 449, training loss: 0.06757, val loss: 2.84782, patience: 380, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 450, training loss: 0.06787, val loss: 2.84544, patience: 381, training metric: 1.0, val metric: 0.0128\n",
            "INFO - root - epoch: 451, training loss: 0.06781, val loss: 2.85073, patience: 382, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 452, training loss: 0.06789, val loss: 2.85261, patience: 383, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 453, training loss: 0.0676, val loss: 2.849, patience: 384, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 454, training loss: 0.06748, val loss: 2.84889, patience: 385, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 455, training loss: 0.06798, val loss: 2.84074, patience: 386, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 456, training loss: 0.06781, val loss: 2.84597, patience: 387, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 457, training loss: 0.06774, val loss: 2.84488, patience: 388, training metric: 1.0, val metric: 0.0112\n",
            "INFO - root - epoch: 458, training loss: 0.0681, val loss: 2.84487, patience: 389, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 459, training loss: 0.06787, val loss: 2.83961, patience: 390, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 460, training loss: 0.06772, val loss: 2.84738, patience: 391, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 461, training loss: 0.06749, val loss: 2.84773, patience: 392, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 462, training loss: 0.06747, val loss: 2.8475, patience: 393, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 463, training loss: 0.06763, val loss: 2.84435, patience: 394, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 464, training loss: 0.0678, val loss: 2.84519, patience: 395, training metric: 1.0, val metric: 0.0112\n",
            "INFO - root - epoch: 465, training loss: 0.06764, val loss: 2.85452, patience: 396, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 466, training loss: 0.06749, val loss: 2.84806, patience: 397, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 467, training loss: 0.06759, val loss: 2.85194, patience: 398, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 468, training loss: 0.06762, val loss: 2.84034, patience: 399, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 469, training loss: 0.06745, val loss: 2.8471, patience: 400, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 470, training loss: 0.06733, val loss: 2.84929, patience: 401, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 471, training loss: 0.06746, val loss: 2.84853, patience: 402, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 472, training loss: 0.06739, val loss: 2.84764, patience: 403, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 473, training loss: 0.06735, val loss: 2.85855, patience: 404, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 474, training loss: 0.06741, val loss: 2.84972, patience: 405, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 475, training loss: 0.06741, val loss: 2.85098, patience: 406, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 476, training loss: 0.06756, val loss: 2.85076, patience: 407, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 477, training loss: 0.06733, val loss: 2.8522, patience: 408, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 478, training loss: 0.06741, val loss: 2.84519, patience: 409, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 479, training loss: 0.06743, val loss: 2.85317, patience: 410, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 480, training loss: 0.0672, val loss: 2.84559, patience: 411, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 481, training loss: 0.06751, val loss: 2.85414, patience: 412, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 482, training loss: 0.06733, val loss: 2.84559, patience: 413, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 483, training loss: 0.06732, val loss: 2.85075, patience: 414, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 484, training loss: 0.06724, val loss: 2.85462, patience: 415, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 485, training loss: 0.06729, val loss: 2.84498, patience: 416, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 486, training loss: 0.06731, val loss: 2.85454, patience: 417, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 487, training loss: 0.0671, val loss: 2.8499, patience: 418, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 488, training loss: 0.06757, val loss: 2.84866, patience: 419, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 489, training loss: 0.06713, val loss: 2.85461, patience: 420, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 490, training loss: 0.06774, val loss: 2.85438, patience: 421, training metric: 1.0, val metric: 0.0112\n",
            "INFO - root - epoch: 491, training loss: 0.06728, val loss: 2.85444, patience: 422, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 492, training loss: 0.06714, val loss: 2.85795, patience: 423, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 493, training loss: 0.06734, val loss: 2.85341, patience: 424, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 494, training loss: 0.06743, val loss: 2.85395, patience: 425, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 495, training loss: 0.06725, val loss: 2.85376, patience: 426, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 496, training loss: 0.06741, val loss: 2.85618, patience: 427, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 497, training loss: 0.06739, val loss: 2.85373, patience: 428, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 498, training loss: 0.0673, val loss: 2.85144, patience: 429, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 499, training loss: 0.0674, val loss: 2.85085, patience: 430, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 500, training loss: 0.06724, val loss: 2.85364, patience: 431, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 501, training loss: 0.06729, val loss: 2.85718, patience: 432, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 502, training loss: 0.06725, val loss: 2.84714, patience: 433, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 503, training loss: 0.06723, val loss: 2.85168, patience: 434, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 504, training loss: 0.06715, val loss: 2.85775, patience: 435, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 505, training loss: 0.06714, val loss: 2.83845, patience: 436, training metric: 1.0, val metric: 0.0128\n",
            "INFO - root - epoch: 506, training loss: 0.06704, val loss: 2.8489, patience: 437, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 507, training loss: 0.06719, val loss: 2.85542, patience: 438, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 508, training loss: 0.06719, val loss: 2.85475, patience: 439, training metric: 1.0, val metric: 0.0112\n",
            "INFO - root - epoch: 509, training loss: 0.0671, val loss: 2.84805, patience: 440, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 510, training loss: 0.06717, val loss: 2.85978, patience: 441, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 511, training loss: 0.06688, val loss: 2.84581, patience: 442, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 512, training loss: 0.067, val loss: 2.85441, patience: 443, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 513, training loss: 0.06688, val loss: 2.85645, patience: 444, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 514, training loss: 0.0671, val loss: 2.8553, patience: 445, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 515, training loss: 0.06691, val loss: 2.85133, patience: 446, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 516, training loss: 0.06715, val loss: 2.85443, patience: 447, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 517, training loss: 0.06693, val loss: 2.85131, patience: 448, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 518, training loss: 0.06684, val loss: 2.84708, patience: 449, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 519, training loss: 0.0667, val loss: 2.85132, patience: 450, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 520, training loss: 0.06669, val loss: 2.85581, patience: 451, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 521, training loss: 0.06673, val loss: 2.85157, patience: 452, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 522, training loss: 0.06689, val loss: 2.8478, patience: 453, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 523, training loss: 0.06699, val loss: 2.85836, patience: 454, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 524, training loss: 0.0669, val loss: 2.85353, patience: 455, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 525, training loss: 0.06693, val loss: 2.85279, patience: 456, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 526, training loss: 0.06668, val loss: 2.85739, patience: 457, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 527, training loss: 0.06715, val loss: 2.85037, patience: 458, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 528, training loss: 0.06716, val loss: 2.85668, patience: 459, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 529, training loss: 0.06701, val loss: 2.84954, patience: 460, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 530, training loss: 0.06717, val loss: 2.85135, patience: 461, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 531, training loss: 0.06689, val loss: 2.84528, patience: 462, training metric: 1.0, val metric: 0.0112\n",
            "INFO - root - epoch: 532, training loss: 0.06698, val loss: 2.85162, patience: 463, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 533, training loss: 0.06706, val loss: 2.84602, patience: 464, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 534, training loss: 0.06692, val loss: 2.8576, patience: 465, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 535, training loss: 0.067, val loss: 2.85549, patience: 466, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 536, training loss: 0.06686, val loss: 2.86233, patience: 467, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 537, training loss: 0.06688, val loss: 2.84999, patience: 468, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 538, training loss: 0.06685, val loss: 2.84974, patience: 469, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 539, training loss: 0.06674, val loss: 2.84857, patience: 470, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 540, training loss: 0.06678, val loss: 2.85191, patience: 471, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 541, training loss: 0.0668, val loss: 2.86043, patience: 472, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 542, training loss: 0.06692, val loss: 2.8546, patience: 473, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 543, training loss: 0.06699, val loss: 2.85598, patience: 474, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 544, training loss: 0.06677, val loss: 2.85306, patience: 475, training metric: 1.0, val metric: 0.0152\n",
            "INFO - root - epoch: 545, training loss: 0.06679, val loss: 2.85246, patience: 476, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 546, training loss: 0.06682, val loss: 2.84975, patience: 477, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 547, training loss: 0.06656, val loss: 2.85549, patience: 478, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 548, training loss: 0.06655, val loss: 2.85422, patience: 479, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 549, training loss: 0.06688, val loss: 2.85387, patience: 480, training metric: 1.0, val metric: 0.0112\n",
            "INFO - root - epoch: 550, training loss: 0.06658, val loss: 2.84995, patience: 481, training metric: 1.0, val metric: 0.0148\n",
            "INFO - root - epoch: 551, training loss: 0.06661, val loss: 2.85507, patience: 482, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 552, training loss: 0.06634, val loss: 2.84158, patience: 483, training metric: 1.0, val metric: 0.0164\n",
            "INFO - root - epoch: 553, training loss: 0.06638, val loss: 2.84224, patience: 484, training metric: 1.0, val metric: 0.0128\n",
            "INFO - root - epoch: 554, training loss: 0.06642, val loss: 2.84414, patience: 485, training metric: 1.0, val metric: 0.0132\n",
            "INFO - root - epoch: 555, training loss: 0.06689, val loss: 2.85031, patience: 486, training metric: 1.0, val metric: 0.0128\n",
            "INFO - root - epoch: 556, training loss: 0.06684, val loss: 2.84432, patience: 487, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 557, training loss: 0.06678, val loss: 2.84905, patience: 488, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 558, training loss: 0.0664, val loss: 2.84824, patience: 489, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 559, training loss: 0.06675, val loss: 2.84126, patience: 490, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 560, training loss: 0.06658, val loss: 2.84549, patience: 491, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 561, training loss: 0.06664, val loss: 2.8426, patience: 492, training metric: 1.0, val metric: 0.0172\n",
            "INFO - root - epoch: 562, training loss: 0.06673, val loss: 2.8484, patience: 493, training metric: 1.0, val metric: 0.014\n",
            "INFO - root - epoch: 563, training loss: 0.06658, val loss: 2.85372, patience: 494, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 564, training loss: 0.06666, val loss: 2.84228, patience: 495, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 565, training loss: 0.06678, val loss: 2.84225, patience: 496, training metric: 1.0, val metric: 0.012\n",
            "INFO - root - epoch: 566, training loss: 0.06644, val loss: 2.84473, patience: 497, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 567, training loss: 0.06634, val loss: 2.83888, patience: 498, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 568, training loss: 0.06631, val loss: 2.84923, patience: 499, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 569, training loss: 0.06638, val loss: 2.84687, patience: 500, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 570, training loss: 0.06648, val loss: 2.84142, patience: 501, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 571, training loss: 0.06635, val loss: 2.84359, patience: 502, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 572, training loss: 0.06638, val loss: 2.84596, patience: 503, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 573, training loss: 0.06644, val loss: 2.84172, patience: 504, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 574, training loss: 0.06628, val loss: 2.85505, patience: 505, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 575, training loss: 0.06645, val loss: 2.84413, patience: 506, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 576, training loss: 0.06618, val loss: 2.85029, patience: 507, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 577, training loss: 0.06652, val loss: 2.85176, patience: 508, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 578, training loss: 0.06633, val loss: 2.84682, patience: 509, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 579, training loss: 0.06619, val loss: 2.84716, patience: 510, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 580, training loss: 0.06641, val loss: 2.85474, patience: 511, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 581, training loss: 0.06676, val loss: 2.85395, patience: 512, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 582, training loss: 0.06701, val loss: 2.84625, patience: 513, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 583, training loss: 0.06675, val loss: 2.85057, patience: 514, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 584, training loss: 0.06668, val loss: 2.85755, patience: 515, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 585, training loss: 0.06627, val loss: 2.84929, patience: 516, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 586, training loss: 0.06646, val loss: 2.84446, patience: 517, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 587, training loss: 0.06655, val loss: 2.85266, patience: 518, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 588, training loss: 0.0664, val loss: 2.85364, patience: 519, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 589, training loss: 0.06631, val loss: 2.86024, patience: 520, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 590, training loss: 0.06622, val loss: 2.85556, patience: 521, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 591, training loss: 0.06635, val loss: 2.85385, patience: 522, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 592, training loss: 0.0666, val loss: 2.85297, patience: 523, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 593, training loss: 0.06645, val loss: 2.86191, patience: 524, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 594, training loss: 0.06637, val loss: 2.85379, patience: 525, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 595, training loss: 0.06663, val loss: 2.85333, patience: 526, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 596, training loss: 0.06621, val loss: 2.86144, patience: 527, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 597, training loss: 0.0664, val loss: 2.86102, patience: 528, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 598, training loss: 0.06613, val loss: 2.849, patience: 529, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 599, training loss: 0.06616, val loss: 2.85553, patience: 530, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 600, training loss: 0.06625, val loss: 2.86521, patience: 531, training metric: 1.0, val metric: 0.0044\n",
            "INFO - root - epoch: 601, training loss: 0.06606, val loss: 2.86069, patience: 532, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 602, training loss: 0.0665, val loss: 2.8562, patience: 533, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 603, training loss: 0.06614, val loss: 2.8607, patience: 534, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 604, training loss: 0.06617, val loss: 2.85071, patience: 535, training metric: 1.0, val metric: 0.0104\n",
            "INFO - root - epoch: 605, training loss: 0.06603, val loss: 2.85322, patience: 536, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 606, training loss: 0.06608, val loss: 2.85392, patience: 537, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 607, training loss: 0.0662, val loss: 2.87202, patience: 538, training metric: 1.0, val metric: 0.0036\n",
            "INFO - root - epoch: 608, training loss: 0.06624, val loss: 2.85747, patience: 539, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 609, training loss: 0.06616, val loss: 2.86206, patience: 540, training metric: 1.0, val metric: 0.0036\n",
            "INFO - root - epoch: 610, training loss: 0.06613, val loss: 2.85436, patience: 541, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 611, training loss: 0.06589, val loss: 2.85306, patience: 542, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 612, training loss: 0.06604, val loss: 2.85988, patience: 543, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 613, training loss: 0.06604, val loss: 2.85503, patience: 544, training metric: 1.0, val metric: 0.0036\n",
            "INFO - root - epoch: 614, training loss: 0.06615, val loss: 2.86243, patience: 545, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 615, training loss: 0.06583, val loss: 2.85821, patience: 546, training metric: 1.0, val metric: 0.004\n",
            "INFO - root - epoch: 616, training loss: 0.06575, val loss: 2.859, patience: 547, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 617, training loss: 0.06598, val loss: 2.85067, patience: 548, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 618, training loss: 0.06576, val loss: 2.85101, patience: 549, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 619, training loss: 0.06603, val loss: 2.86006, patience: 550, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 620, training loss: 0.06608, val loss: 2.86403, patience: 551, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 621, training loss: 0.06572, val loss: 2.85893, patience: 552, training metric: 1.0, val metric: 0.0028\n",
            "INFO - root - epoch: 622, training loss: 0.06614, val loss: 2.85776, patience: 553, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 623, training loss: 0.06612, val loss: 2.85297, patience: 554, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 624, training loss: 0.06606, val loss: 2.85419, patience: 555, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 625, training loss: 0.06624, val loss: 2.85277, patience: 556, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 626, training loss: 0.06598, val loss: 2.85053, patience: 557, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 627, training loss: 0.06595, val loss: 2.8549, patience: 558, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 628, training loss: 0.0659, val loss: 2.8432, patience: 559, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 629, training loss: 0.06578, val loss: 2.85806, patience: 560, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 630, training loss: 0.06592, val loss: 2.8547, patience: 561, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 631, training loss: 0.06584, val loss: 2.84664, patience: 562, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 632, training loss: 0.06545, val loss: 2.85793, patience: 563, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 633, training loss: 0.06582, val loss: 2.84742, patience: 564, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 634, training loss: 0.06563, val loss: 2.85641, patience: 565, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 635, training loss: 0.06575, val loss: 2.85169, patience: 566, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 636, training loss: 0.06575, val loss: 2.84538, patience: 567, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 637, training loss: 0.0657, val loss: 2.84518, patience: 568, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 638, training loss: 0.06558, val loss: 2.84889, patience: 569, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 639, training loss: 0.0656, val loss: 2.8562, patience: 570, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 640, training loss: 0.06544, val loss: 2.84416, patience: 571, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 641, training loss: 0.06554, val loss: 2.84888, patience: 572, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 642, training loss: 0.06578, val loss: 2.86031, patience: 573, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 643, training loss: 0.06604, val loss: 2.84405, patience: 574, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 644, training loss: 0.06567, val loss: 2.84609, patience: 575, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 645, training loss: 0.06566, val loss: 2.85163, patience: 576, training metric: 1.0, val metric: 0.004\n",
            "INFO - root - epoch: 646, training loss: 0.06572, val loss: 2.84949, patience: 577, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 647, training loss: 0.06542, val loss: 2.84658, patience: 578, training metric: 1.0, val metric: 0.01\n",
            "INFO - root - epoch: 648, training loss: 0.06543, val loss: 2.84929, patience: 579, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 649, training loss: 0.06537, val loss: 2.84823, patience: 580, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 650, training loss: 0.06565, val loss: 2.84976, patience: 581, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 651, training loss: 0.0659, val loss: 2.85526, patience: 582, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 652, training loss: 0.06577, val loss: 2.85218, patience: 583, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 653, training loss: 0.06541, val loss: 2.84311, patience: 584, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 654, training loss: 0.06545, val loss: 2.85892, patience: 585, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 655, training loss: 0.06546, val loss: 2.84719, patience: 586, training metric: 1.0, val metric: 0.004\n",
            "INFO - root - epoch: 656, training loss: 0.06524, val loss: 2.8468, patience: 587, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 657, training loss: 0.06535, val loss: 2.85085, patience: 588, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 658, training loss: 0.0654, val loss: 2.85465, patience: 589, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 659, training loss: 0.06537, val loss: 2.85661, patience: 590, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 660, training loss: 0.06542, val loss: 2.85373, patience: 591, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 661, training loss: 0.06511, val loss: 2.85309, patience: 592, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 662, training loss: 0.06553, val loss: 2.85041, patience: 593, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 663, training loss: 0.0656, val loss: 2.85863, patience: 594, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 664, training loss: 0.0653, val loss: 2.86206, patience: 595, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 665, training loss: 0.06518, val loss: 2.85502, patience: 596, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 666, training loss: 0.06544, val loss: 2.85062, patience: 597, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 667, training loss: 0.06542, val loss: 2.85496, patience: 598, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 668, training loss: 0.06522, val loss: 2.85129, patience: 599, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 669, training loss: 0.06514, val loss: 2.86082, patience: 600, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 670, training loss: 0.06516, val loss: 2.8533, patience: 601, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 671, training loss: 0.06537, val loss: 2.85877, patience: 602, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 672, training loss: 0.06496, val loss: 2.85736, patience: 603, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 673, training loss: 0.0652, val loss: 2.85202, patience: 604, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 674, training loss: 0.06532, val loss: 2.85335, patience: 605, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 675, training loss: 0.06482, val loss: 2.86101, patience: 606, training metric: 1.0, val metric: 0.0068\n",
            "INFO - root - epoch: 676, training loss: 0.06508, val loss: 2.85135, patience: 607, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 677, training loss: 0.06528, val loss: 2.85572, patience: 608, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 678, training loss: 0.06518, val loss: 2.85476, patience: 609, training metric: 1.0, val metric: 0.0044\n",
            "INFO - root - epoch: 679, training loss: 0.0654, val loss: 2.85833, patience: 610, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 680, training loss: 0.06524, val loss: 2.85036, patience: 611, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 681, training loss: 0.06544, val loss: 2.84938, patience: 612, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 682, training loss: 0.06591, val loss: 2.86334, patience: 613, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 683, training loss: 0.06537, val loss: 2.86035, patience: 614, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 684, training loss: 0.06512, val loss: 2.85032, patience: 615, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 685, training loss: 0.06522, val loss: 2.85346, patience: 616, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 686, training loss: 0.0652, val loss: 2.85605, patience: 617, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 687, training loss: 0.065, val loss: 2.85548, patience: 618, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 688, training loss: 0.06518, val loss: 2.8502, patience: 619, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 689, training loss: 0.06551, val loss: 2.85331, patience: 620, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 690, training loss: 0.06533, val loss: 2.84209, patience: 621, training metric: 1.0, val metric: 0.0084\n",
            "INFO - root - epoch: 691, training loss: 0.06498, val loss: 2.8575, patience: 622, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - epoch: 692, training loss: 0.06505, val loss: 2.85308, patience: 623, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 693, training loss: 0.0651, val loss: 2.85007, patience: 624, training metric: 1.0, val metric: 0.0088\n",
            "INFO - root - epoch: 694, training loss: 0.0652, val loss: 2.8576, patience: 625, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 695, training loss: 0.06499, val loss: 2.86115, patience: 626, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 696, training loss: 0.06497, val loss: 2.85872, patience: 627, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 697, training loss: 0.06508, val loss: 2.84914, patience: 628, training metric: 1.0, val metric: 0.0064\n",
            "INFO - root - epoch: 698, training loss: 0.06523, val loss: 2.85113, patience: 629, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 699, training loss: 0.06485, val loss: 2.85896, patience: 630, training metric: 1.0, val metric: 0.008\n",
            "INFO - root - loaded best model at epoch 69\n",
            "INFO - root - Best val loss: 2.3080098628997803\n",
            "INFO - root - Best val metric: 0.2084\n",
            "INFO - root - test loss: 4.707716941833496\n",
            "INFO - root - test metric: 0.016\n",
            "INFO - root - epoch: 0, training loss: 2.35974, val loss: 2.30685, patience: 0, training metric: 0.0392, val metric: 0.1\n",
            "INFO - root - epoch: 1, training loss: 2.21944, val loss: 2.30686, patience: 1, training metric: 0.1012, val metric: 0.1\n",
            "INFO - root - epoch: 2, training loss: 2.15489, val loss: 2.3067, patience: 2, training metric: 0.168, val metric: 0.1\n",
            "INFO - root - epoch: 3, training loss: 2.1033, val loss: 2.30662, patience: 3, training metric: 0.204, val metric: 0.1\n",
            "INFO - root - epoch: 4, training loss: 2.07366, val loss: 2.30672, patience: 4, training metric: 0.2412, val metric: 0.1\n",
            "INFO - root - epoch: 5, training loss: 2.04846, val loss: 2.30701, patience: 5, training metric: 0.2852, val metric: 0.1\n",
            "INFO - root - epoch: 6, training loss: 2.00937, val loss: 2.30756, patience: 6, training metric: 0.3276, val metric: 0.1\n",
            "INFO - root - epoch: 7, training loss: 1.9801, val loss: 2.30814, patience: 7, training metric: 0.374, val metric: 0.1\n",
            "INFO - root - epoch: 8, training loss: 1.9395, val loss: 2.30887, patience: 8, training metric: 0.4316, val metric: 0.1\n",
            "INFO - root - epoch: 9, training loss: 1.9093, val loss: 2.30971, patience: 9, training metric: 0.4976, val metric: 0.1\n",
            "INFO - root - epoch: 10, training loss: 1.87408, val loss: 2.3107, patience: 10, training metric: 0.5664, val metric: 0.1\n",
            "INFO - root - epoch: 11, training loss: 1.85106, val loss: 2.31178, patience: 11, training metric: 0.5836, val metric: 0.1\n",
            "INFO - root - epoch: 12, training loss: 1.8161, val loss: 2.31285, patience: 12, training metric: 0.61, val metric: 0.1\n",
            "INFO - root - epoch: 13, training loss: 1.79596, val loss: 2.31392, patience: 13, training metric: 0.6548, val metric: 0.1\n",
            "INFO - root - epoch: 14, training loss: 1.7585, val loss: 2.3151, patience: 14, training metric: 0.7124, val metric: 0.1\n",
            "INFO - root - epoch: 15, training loss: 1.73499, val loss: 2.31616, patience: 15, training metric: 0.7332, val metric: 0.1\n",
            "INFO - root - epoch: 16, training loss: 1.70626, val loss: 2.317, patience: 16, training metric: 0.7496, val metric: 0.1\n",
            "INFO - root - epoch: 17, training loss: 1.677, val loss: 2.31762, patience: 17, training metric: 0.7884, val metric: 0.1\n",
            "INFO - root - epoch: 18, training loss: 1.66555, val loss: 2.31793, patience: 18, training metric: 0.7828, val metric: 0.1\n",
            "INFO - root - epoch: 19, training loss: 1.63952, val loss: 2.31796, patience: 19, training metric: 0.7884, val metric: 0.1\n",
            "INFO - root - epoch: 20, training loss: 1.61097, val loss: 2.31777, patience: 20, training metric: 0.8176, val metric: 0.1\n",
            "INFO - root - epoch: 21, training loss: 1.57873, val loss: 2.31734, patience: 21, training metric: 0.824, val metric: 0.1\n",
            "INFO - root - epoch: 22, training loss: 1.55868, val loss: 2.3165, patience: 22, training metric: 0.8152, val metric: 0.1\n",
            "INFO - root - epoch: 23, training loss: 1.53329, val loss: 2.31502, patience: 23, training metric: 0.8412, val metric: 0.1\n",
            "INFO - root - epoch: 24, training loss: 1.50795, val loss: 2.31336, patience: 24, training metric: 0.838, val metric: 0.1\n",
            "INFO - root - epoch: 25, training loss: 1.48063, val loss: 2.31194, patience: 25, training metric: 0.856, val metric: 0.1\n",
            "INFO - root - epoch: 26, training loss: 1.46054, val loss: 2.31182, patience: 26, training metric: 0.8712, val metric: 0.1\n",
            "INFO - root - epoch: 27, training loss: 1.44027, val loss: 2.31301, patience: 27, training metric: 0.8788, val metric: 0.1\n",
            "INFO - root - epoch: 28, training loss: 1.41132, val loss: 2.3168, patience: 0, training metric: 0.8836, val metric: 0.1168\n",
            "INFO - root - epoch: 29, training loss: 1.38911, val loss: 2.32295, patience: 1, training metric: 0.8792, val metric: 0.1004\n",
            "INFO - root - epoch: 30, training loss: 1.36623, val loss: 2.33359, patience: 2, training metric: 0.896, val metric: 0.1\n",
            "INFO - root - epoch: 31, training loss: 1.34912, val loss: 2.34782, patience: 3, training metric: 0.88, val metric: 0.1\n",
            "INFO - root - epoch: 32, training loss: 1.32011, val loss: 2.36174, patience: 4, training metric: 0.9036, val metric: 0.1\n",
            "INFO - root - epoch: 33, training loss: 1.29726, val loss: 2.37593, patience: 5, training metric: 0.9124, val metric: 0.1\n",
            "INFO - root - epoch: 34, training loss: 1.27506, val loss: 2.38962, patience: 6, training metric: 0.9268, val metric: 0.1\n",
            "INFO - root - epoch: 35, training loss: 1.25013, val loss: 2.40512, patience: 7, training metric: 0.9264, val metric: 0.1\n",
            "INFO - root - epoch: 36, training loss: 1.22905, val loss: 2.42486, patience: 8, training metric: 0.9388, val metric: 0.1\n",
            "INFO - root - epoch: 37, training loss: 1.20079, val loss: 2.44713, patience: 9, training metric: 0.9468, val metric: 0.1\n",
            "INFO - root - epoch: 38, training loss: 1.18199, val loss: 2.47356, patience: 10, training metric: 0.9444, val metric: 0.1\n",
            "INFO - root - epoch: 39, training loss: 1.17125, val loss: 2.5023, patience: 11, training metric: 0.9392, val metric: 0.0748\n",
            "INFO - root - epoch: 40, training loss: 1.14459, val loss: 2.54444, patience: 12, training metric: 0.9524, val metric: 0.1\n",
            "INFO - root - epoch: 41, training loss: 1.12495, val loss: 2.59548, patience: 13, training metric: 0.954, val metric: 0.1\n",
            "INFO - root - epoch: 42, training loss: 1.10784, val loss: 2.66366, patience: 14, training metric: 0.9604, val metric: 0.1\n",
            "INFO - root - epoch: 43, training loss: 1.08149, val loss: 2.73162, patience: 15, training metric: 0.9652, val metric: 0.1\n",
            "INFO - root - epoch: 44, training loss: 1.05654, val loss: 2.78215, patience: 16, training metric: 0.9752, val metric: 0.1\n",
            "INFO - root - epoch: 45, training loss: 1.04416, val loss: 2.84749, patience: 17, training metric: 0.9708, val metric: 0.1\n",
            "INFO - root - epoch: 46, training loss: 1.01578, val loss: 2.91981, patience: 18, training metric: 0.9792, val metric: 0.0996\n",
            "INFO - root - epoch: 47, training loss: 0.99448, val loss: 3.00982, patience: 19, training metric: 0.9804, val metric: 0.0992\n",
            "INFO - root - epoch: 48, training loss: 0.9848, val loss: 3.0802, patience: 20, training metric: 0.9832, val metric: 0.1\n",
            "INFO - root - epoch: 49, training loss: 0.9678, val loss: 3.09784, patience: 0, training metric: 0.988, val metric: 0.1228\n",
            "INFO - root - epoch: 50, training loss: 0.94327, val loss: 3.0942, patience: 1, training metric: 0.9892, val metric: 0.1212\n",
            "INFO - root - epoch: 51, training loss: 0.92206, val loss: 3.11358, patience: 2, training metric: 0.9908, val metric: 0.0996\n",
            "INFO - root - epoch: 52, training loss: 0.89948, val loss: 3.24467, patience: 3, training metric: 0.988, val metric: 0.1\n",
            "INFO - root - epoch: 53, training loss: 0.88409, val loss: 3.57544, patience: 4, training metric: 0.9908, val metric: 0.1\n",
            "INFO - root - epoch: 54, training loss: 0.86525, val loss: 4.15323, patience: 5, training metric: 0.9892, val metric: 0.1\n",
            "INFO - root - epoch: 55, training loss: 0.84463, val loss: 4.77007, patience: 6, training metric: 0.9892, val metric: 0.1\n",
            "INFO - root - epoch: 56, training loss: 0.83592, val loss: 4.94852, patience: 7, training metric: 0.994, val metric: 0.1\n",
            "INFO - root - epoch: 57, training loss: 0.81234, val loss: 4.67712, patience: 8, training metric: 0.9936, val metric: 0.1\n",
            "INFO - root - epoch: 58, training loss: 0.79377, val loss: 4.09669, patience: 9, training metric: 0.9964, val metric: 0.1\n",
            "INFO - root - epoch: 59, training loss: 0.76994, val loss: 3.70452, patience: 10, training metric: 0.9956, val metric: 0.1\n",
            "INFO - root - epoch: 60, training loss: 0.75257, val loss: 3.55438, patience: 11, training metric: 0.9964, val metric: 0.102\n",
            "INFO - root - epoch: 61, training loss: 0.73237, val loss: 4.30697, patience: 12, training metric: 0.9956, val metric: 0.1\n",
            "INFO - root - epoch: 62, training loss: 0.72055, val loss: 5.08183, patience: 13, training metric: 0.9948, val metric: 0.1\n",
            "INFO - root - epoch: 63, training loss: 0.6997, val loss: 5.19224, patience: 14, training metric: 0.9952, val metric: 0.1\n",
            "INFO - root - epoch: 64, training loss: 0.67743, val loss: 4.75481, patience: 15, training metric: 0.996, val metric: 0.1\n",
            "INFO - root - epoch: 65, training loss: 0.67221, val loss: 3.6721, patience: 0, training metric: 0.9968, val metric: 0.1476\n",
            "INFO - root - epoch: 66, training loss: 0.65604, val loss: 3.07151, patience: 1, training metric: 0.9972, val metric: 0.1392\n",
            "INFO - root - epoch: 67, training loss: 0.63441, val loss: 2.89001, patience: 2, training metric: 0.9976, val metric: 0.1272\n",
            "INFO - root - epoch: 68, training loss: 0.62285, val loss: 2.77986, patience: 3, training metric: 0.998, val metric: 0.1364\n",
            "INFO - root - epoch: 69, training loss: 0.60837, val loss: 2.87654, patience: 0, training metric: 0.9976, val metric: 0.1568\n",
            "INFO - root - epoch: 70, training loss: 0.58981, val loss: 3.16005, patience: 1, training metric: 0.9984, val metric: 0.1432\n",
            "INFO - root - epoch: 71, training loss: 0.57707, val loss: 3.42978, patience: 2, training metric: 0.9988, val metric: 0.1092\n",
            "INFO - root - epoch: 72, training loss: 0.55957, val loss: 3.57395, patience: 3, training metric: 0.998, val metric: 0.1036\n",
            "INFO - root - epoch: 73, training loss: 0.54828, val loss: 3.5064, patience: 0, training metric: 0.9976, val metric: 0.1676\n",
            "INFO - root - epoch: 74, training loss: 0.53463, val loss: 3.49739, patience: 1, training metric: 0.9992, val metric: 0.1008\n",
            "INFO - root - epoch: 75, training loss: 0.51816, val loss: 4.33139, patience: 2, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 76, training loss: 0.50473, val loss: 5.89213, patience: 3, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 77, training loss: 0.48978, val loss: 5.75819, patience: 4, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 78, training loss: 0.47577, val loss: 5.04506, patience: 5, training metric: 0.9988, val metric: 0.1\n",
            "INFO - root - epoch: 79, training loss: 0.4639, val loss: 5.27067, patience: 6, training metric: 0.9988, val metric: 0.1\n",
            "INFO - root - epoch: 80, training loss: 0.45092, val loss: 6.89068, patience: 7, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 81, training loss: 0.4397, val loss: 7.63365, patience: 8, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 82, training loss: 0.42497, val loss: 6.54915, patience: 9, training metric: 0.9988, val metric: 0.1\n",
            "INFO - root - epoch: 83, training loss: 0.41485, val loss: 5.56859, patience: 10, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 84, training loss: 0.40094, val loss: 5.13334, patience: 11, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 85, training loss: 0.39011, val loss: 5.11475, patience: 12, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 86, training loss: 0.38021, val loss: 4.74761, patience: 13, training metric: 0.9988, val metric: 0.1\n",
            "INFO - root - epoch: 87, training loss: 0.36987, val loss: 4.52029, patience: 14, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 88, training loss: 0.36199, val loss: 4.56383, patience: 15, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 89, training loss: 0.34803, val loss: 4.71858, patience: 16, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 90, training loss: 0.34088, val loss: 4.7726, patience: 17, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 91, training loss: 0.3319, val loss: 4.85383, patience: 18, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 92, training loss: 0.32237, val loss: 5.14111, patience: 19, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 93, training loss: 0.31438, val loss: 5.30542, patience: 20, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 94, training loss: 0.30582, val loss: 4.93852, patience: 21, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 95, training loss: 0.2994, val loss: 4.59286, patience: 22, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 96, training loss: 0.29104, val loss: 4.56868, patience: 23, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 97, training loss: 0.2825, val loss: 4.56127, patience: 24, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 98, training loss: 0.27497, val loss: 4.46434, patience: 25, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 99, training loss: 0.27025, val loss: 4.59847, patience: 26, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 100, training loss: 0.26244, val loss: 4.46698, patience: 27, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 101, training loss: 0.25592, val loss: 4.40398, patience: 28, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 102, training loss: 0.24919, val loss: 4.39143, patience: 29, training metric: 1.0, val metric: 0.1108\n",
            "INFO - root - epoch: 103, training loss: 0.24526, val loss: 4.55369, patience: 30, training metric: 1.0, val metric: 0.1304\n",
            "INFO - root - epoch: 104, training loss: 0.23726, val loss: 4.58515, patience: 0, training metric: 1.0, val metric: 0.1904\n",
            "INFO - root - epoch: 105, training loss: 0.23139, val loss: 4.61975, patience: 0, training metric: 1.0, val metric: 0.1972\n",
            "INFO - root - epoch: 106, training loss: 0.22727, val loss: 4.54111, patience: 1, training metric: 0.9996, val metric: 0.1968\n",
            "INFO - root - epoch: 107, training loss: 0.2222, val loss: 4.35425, patience: 0, training metric: 0.9996, val metric: 0.1992\n",
            "INFO - root - epoch: 108, training loss: 0.21704, val loss: 4.13245, patience: 0, training metric: 1.0, val metric: 0.1996\n",
            "INFO - root - epoch: 109, training loss: 0.21267, val loss: 4.09093, patience: 1, training metric: 1.0, val metric: 0.166\n",
            "INFO - root - epoch: 110, training loss: 0.20743, val loss: 4.33109, patience: 2, training metric: 0.9996, val metric: 0.11\n",
            "INFO - root - epoch: 111, training loss: 0.20196, val loss: 4.31633, patience: 3, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 112, training loss: 0.19623, val loss: 4.3763, patience: 4, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 113, training loss: 0.19214, val loss: 4.37227, patience: 5, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 114, training loss: 0.18927, val loss: 4.5231, patience: 6, training metric: 1.0, val metric: 0.0872\n",
            "INFO - root - epoch: 115, training loss: 0.18416, val loss: 4.63656, patience: 7, training metric: 1.0, val metric: 0.088\n",
            "INFO - root - epoch: 116, training loss: 0.18123, val loss: 4.87017, patience: 8, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 117, training loss: 0.17638, val loss: 5.00599, patience: 9, training metric: 1.0, val metric: 0.0796\n",
            "INFO - root - epoch: 118, training loss: 0.17311, val loss: 4.84065, patience: 10, training metric: 1.0, val metric: 0.0248\n",
            "INFO - root - epoch: 119, training loss: 0.16933, val loss: 4.49463, patience: 11, training metric: 1.0, val metric: 0.036\n",
            "INFO - root - epoch: 120, training loss: 0.16507, val loss: 4.28154, patience: 12, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 121, training loss: 0.1644, val loss: 4.04393, patience: 13, training metric: 0.9996, val metric: 0.0988\n",
            "INFO - root - epoch: 122, training loss: 0.15885, val loss: 3.90042, patience: 14, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 123, training loss: 0.15656, val loss: 3.62235, patience: 15, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 124, training loss: 0.15316, val loss: 3.45587, patience: 16, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 125, training loss: 0.15025, val loss: 3.39221, patience: 17, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 126, training loss: 0.14771, val loss: 3.54191, patience: 18, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 127, training loss: 0.14476, val loss: 3.87943, patience: 19, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 128, training loss: 0.14109, val loss: 3.97639, patience: 20, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 129, training loss: 0.13812, val loss: 3.88135, patience: 21, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 130, training loss: 0.13602, val loss: 3.72737, patience: 22, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 131, training loss: 0.13357, val loss: 3.52196, patience: 23, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 132, training loss: 0.13102, val loss: 3.31835, patience: 24, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 133, training loss: 0.12817, val loss: 3.19513, patience: 25, training metric: 1.0, val metric: 0.094\n",
            "INFO - root - epoch: 134, training loss: 0.12746, val loss: 3.14598, patience: 26, training metric: 1.0, val metric: 0.0704\n",
            "INFO - root - epoch: 135, training loss: 0.12526, val loss: 3.16007, patience: 27, training metric: 1.0, val metric: 0.0644\n",
            "INFO - root - epoch: 136, training loss: 0.12212, val loss: 3.2456, patience: 28, training metric: 1.0, val metric: 0.0632\n",
            "INFO - root - epoch: 137, training loss: 0.11958, val loss: 3.32343, patience: 29, training metric: 1.0, val metric: 0.058\n",
            "INFO - root - epoch: 138, training loss: 0.11868, val loss: 3.29383, patience: 30, training metric: 1.0, val metric: 0.0492\n",
            "INFO - root - epoch: 139, training loss: 0.11622, val loss: 3.25327, patience: 31, training metric: 1.0, val metric: 0.0644\n",
            "INFO - root - epoch: 140, training loss: 0.11413, val loss: 3.23397, patience: 32, training metric: 1.0, val metric: 0.0764\n",
            "INFO - root - epoch: 141, training loss: 0.11249, val loss: 3.22481, patience: 33, training metric: 1.0, val metric: 0.0948\n",
            "INFO - root - epoch: 142, training loss: 0.11042, val loss: 3.27659, patience: 34, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 143, training loss: 0.10805, val loss: 3.41499, patience: 35, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 144, training loss: 0.10715, val loss: 3.56909, patience: 36, training metric: 1.0, val metric: 0.088\n",
            "INFO - root - epoch: 145, training loss: 0.10672, val loss: 3.60929, patience: 37, training metric: 1.0, val metric: 0.08\n",
            "INFO - root - epoch: 146, training loss: 0.10312, val loss: 3.55877, patience: 38, training metric: 1.0, val metric: 0.0688\n",
            "INFO - root - epoch: 147, training loss: 0.1023, val loss: 3.42366, patience: 39, training metric: 1.0, val metric: 0.0504\n",
            "INFO - root - epoch: 148, training loss: 0.10059, val loss: 3.25716, patience: 40, training metric: 1.0, val metric: 0.0496\n",
            "INFO - root - epoch: 149, training loss: 0.09919, val loss: 3.15572, patience: 41, training metric: 1.0, val metric: 0.0472\n",
            "INFO - root - epoch: 150, training loss: 0.09745, val loss: 3.14642, patience: 42, training metric: 1.0, val metric: 0.0292\n",
            "INFO - root - epoch: 151, training loss: 0.09587, val loss: 3.13319, patience: 43, training metric: 1.0, val metric: 0.0132\n",
            "INFO - root - epoch: 152, training loss: 0.09446, val loss: 3.12406, patience: 44, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 153, training loss: 0.09354, val loss: 3.11212, patience: 45, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 154, training loss: 0.09163, val loss: 3.08858, patience: 46, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 155, training loss: 0.08949, val loss: 3.07257, patience: 47, training metric: 1.0, val metric: 0.0648\n",
            "INFO - root - epoch: 156, training loss: 0.0888, val loss: 3.08083, patience: 48, training metric: 1.0, val metric: 0.0504\n",
            "INFO - root - epoch: 157, training loss: 0.08713, val loss: 3.07734, patience: 49, training metric: 1.0, val metric: 0.016\n",
            "INFO - root - epoch: 158, training loss: 0.08619, val loss: 3.1017, patience: 50, training metric: 1.0, val metric: 0.0012\n",
            "INFO - root - epoch: 159, training loss: 0.08499, val loss: 3.1174, patience: 51, training metric: 1.0, val metric: 0.0\n",
            "INFO - root - epoch: 160, training loss: 0.08326, val loss: 3.12146, patience: 52, training metric: 1.0, val metric: 0.0008\n",
            "INFO - root - epoch: 161, training loss: 0.08269, val loss: 3.12652, patience: 53, training metric: 1.0, val metric: 0.0016\n",
            "INFO - root - epoch: 162, training loss: 0.08225, val loss: 3.12379, patience: 54, training metric: 1.0, val metric: 0.0024\n",
            "INFO - root - epoch: 163, training loss: 0.08172, val loss: 3.12066, patience: 55, training metric: 1.0, val metric: 0.0016\n",
            "INFO - root - epoch: 164, training loss: 0.08146, val loss: 3.1043, patience: 56, training metric: 1.0, val metric: 0.0012\n",
            "INFO - root - epoch: 165, training loss: 0.0811, val loss: 3.09756, patience: 57, training metric: 1.0, val metric: 0.006\n",
            "INFO - root - epoch: 166, training loss: 0.0804, val loss: 3.09508, patience: 58, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 167, training loss: 0.0798, val loss: 3.09497, patience: 59, training metric: 1.0, val metric: 0.014\n",
            "INFO - root - epoch: 168, training loss: 0.07887, val loss: 3.09004, patience: 60, training metric: 1.0, val metric: 0.0224\n",
            "INFO - root - epoch: 169, training loss: 0.07892, val loss: 3.09786, patience: 61, training metric: 1.0, val metric: 0.0232\n",
            "INFO - root - epoch: 170, training loss: 0.07799, val loss: 3.11342, patience: 62, training metric: 1.0, val metric: 0.0124\n",
            "INFO - root - epoch: 171, training loss: 0.0781, val loss: 3.15396, patience: 63, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 172, training loss: 0.07702, val loss: 3.21042, patience: 64, training metric: 1.0, val metric: 0.0108\n",
            "INFO - root - epoch: 173, training loss: 0.07698, val loss: 3.26827, patience: 65, training metric: 1.0, val metric: 0.0076\n",
            "INFO - root - epoch: 174, training loss: 0.0764, val loss: 3.30215, patience: 66, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 175, training loss: 0.07593, val loss: 3.3465, patience: 67, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 176, training loss: 0.07561, val loss: 3.35443, patience: 68, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 177, training loss: 0.07449, val loss: 3.35805, patience: 69, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 178, training loss: 0.0746, val loss: 3.34818, patience: 70, training metric: 1.0, val metric: 0.0\n",
            "INFO - root - epoch: 179, training loss: 0.07354, val loss: 3.3307, patience: 71, training metric: 1.0, val metric: 0.002\n",
            "INFO - root - epoch: 180, training loss: 0.07288, val loss: 3.29949, patience: 72, training metric: 1.0, val metric: 0.0008\n",
            "INFO - root - epoch: 181, training loss: 0.07241, val loss: 3.28662, patience: 73, training metric: 1.0, val metric: 0.0032\n",
            "INFO - root - epoch: 182, training loss: 0.07213, val loss: 3.26138, patience: 74, training metric: 1.0, val metric: 0.0016\n",
            "INFO - root - epoch: 183, training loss: 0.0718, val loss: 3.23687, patience: 75, training metric: 1.0, val metric: 0.0048\n",
            "INFO - root - epoch: 184, training loss: 0.0713, val loss: 3.21153, patience: 76, training metric: 1.0, val metric: 0.0052\n",
            "INFO - root - epoch: 185, training loss: 0.07128, val loss: 3.18953, patience: 77, training metric: 1.0, val metric: 0.0056\n",
            "INFO - root - epoch: 186, training loss: 0.07032, val loss: 3.16287, patience: 78, training metric: 1.0, val metric: 0.0136\n",
            "INFO - root - epoch: 187, training loss: 0.07012, val loss: 3.15298, patience: 79, training metric: 1.0, val metric: 0.022\n",
            "INFO - root - epoch: 188, training loss: 0.07003, val loss: 3.15454, patience: 80, training metric: 1.0, val metric: 0.0288\n",
            "INFO - root - epoch: 189, training loss: 0.06931, val loss: 3.15807, patience: 81, training metric: 1.0, val metric: 0.0352\n",
            "INFO - root - epoch: 190, training loss: 0.06911, val loss: 3.16111, patience: 82, training metric: 1.0, val metric: 0.046\n",
            "INFO - root - epoch: 191, training loss: 0.06844, val loss: 3.19209, patience: 83, training metric: 1.0, val metric: 0.0344\n",
            "INFO - root - epoch: 192, training loss: 0.06789, val loss: 3.19632, patience: 84, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 193, training loss: 0.06719, val loss: 3.22153, patience: 85, training metric: 1.0, val metric: 0.046\n",
            "INFO - root - epoch: 194, training loss: 0.06746, val loss: 3.22449, patience: 86, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 195, training loss: 0.06686, val loss: 3.23048, patience: 87, training metric: 1.0, val metric: 0.0608\n",
            "INFO - root - epoch: 196, training loss: 0.06603, val loss: 3.26529, patience: 88, training metric: 1.0, val metric: 0.0624\n",
            "INFO - root - epoch: 197, training loss: 0.06585, val loss: 3.31177, patience: 89, training metric: 1.0, val metric: 0.0524\n",
            "INFO - root - epoch: 198, training loss: 0.06563, val loss: 3.33403, patience: 90, training metric: 1.0, val metric: 0.034\n",
            "INFO - root - epoch: 199, training loss: 0.06512, val loss: 3.34994, patience: 91, training metric: 1.0, val metric: 0.0192\n",
            "INFO - root - epoch: 200, training loss: 0.06528, val loss: 3.35001, patience: 92, training metric: 1.0, val metric: 0.0164\n",
            "INFO - root - epoch: 201, training loss: 0.06416, val loss: 3.33059, patience: 93, training metric: 1.0, val metric: 0.0092\n",
            "INFO - root - epoch: 202, training loss: 0.06405, val loss: 3.31206, patience: 94, training metric: 1.0, val metric: 0.0072\n",
            "INFO - root - epoch: 203, training loss: 0.06356, val loss: 3.28773, patience: 95, training metric: 1.0, val metric: 0.0116\n",
            "INFO - root - epoch: 204, training loss: 0.06335, val loss: 3.28422, patience: 96, training metric: 1.0, val metric: 0.022\n",
            "INFO - root - epoch: 205, training loss: 0.06237, val loss: 3.27711, patience: 97, training metric: 1.0, val metric: 0.0256\n",
            "INFO - root - epoch: 206, training loss: 0.06287, val loss: 3.29196, patience: 98, training metric: 1.0, val metric: 0.0284\n",
            "INFO - root - epoch: 207, training loss: 0.06181, val loss: 3.29214, patience: 99, training metric: 1.0, val metric: 0.0288\n",
            "INFO - root - epoch: 208, training loss: 0.06144, val loss: 3.30392, patience: 100, training metric: 1.0, val metric: 0.0348\n",
            "INFO - root - epoch: 209, training loss: 0.06102, val loss: 3.3246, patience: 101, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 210, training loss: 0.06088, val loss: 3.34474, patience: 102, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 211, training loss: 0.06009, val loss: 3.35001, patience: 103, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 212, training loss: 0.06027, val loss: 3.36884, patience: 104, training metric: 1.0, val metric: 0.0444\n",
            "INFO - root - epoch: 213, training loss: 0.0603, val loss: 3.38173, patience: 105, training metric: 1.0, val metric: 0.0484\n",
            "INFO - root - epoch: 214, training loss: 0.06021, val loss: 3.39447, patience: 106, training metric: 1.0, val metric: 0.056\n",
            "INFO - root - epoch: 215, training loss: 0.0595, val loss: 3.38557, patience: 107, training metric: 1.0, val metric: 0.0568\n",
            "INFO - root - epoch: 216, training loss: 0.05935, val loss: 3.39095, patience: 108, training metric: 1.0, val metric: 0.054\n",
            "INFO - root - epoch: 217, training loss: 0.05955, val loss: 3.39222, patience: 109, training metric: 1.0, val metric: 0.0516\n",
            "INFO - root - epoch: 218, training loss: 0.05923, val loss: 3.37832, patience: 110, training metric: 1.0, val metric: 0.0548\n",
            "INFO - root - epoch: 219, training loss: 0.05915, val loss: 3.37323, patience: 111, training metric: 1.0, val metric: 0.0464\n",
            "INFO - root - epoch: 220, training loss: 0.05874, val loss: 3.35891, patience: 112, training metric: 1.0, val metric: 0.0492\n",
            "INFO - root - epoch: 221, training loss: 0.05864, val loss: 3.34026, patience: 113, training metric: 1.0, val metric: 0.0492\n",
            "INFO - root - epoch: 222, training loss: 0.05809, val loss: 3.3355, patience: 114, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 223, training loss: 0.05829, val loss: 3.34442, patience: 115, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 224, training loss: 0.05819, val loss: 3.34919, patience: 116, training metric: 1.0, val metric: 0.0348\n",
            "INFO - root - epoch: 225, training loss: 0.0579, val loss: 3.35201, patience: 117, training metric: 1.0, val metric: 0.0324\n",
            "INFO - root - epoch: 226, training loss: 0.05793, val loss: 3.37088, patience: 118, training metric: 1.0, val metric: 0.0312\n",
            "INFO - root - epoch: 227, training loss: 0.0579, val loss: 3.36564, patience: 119, training metric: 1.0, val metric: 0.0332\n",
            "INFO - root - epoch: 228, training loss: 0.0576, val loss: 3.37564, patience: 120, training metric: 1.0, val metric: 0.0268\n",
            "INFO - root - epoch: 229, training loss: 0.05725, val loss: 3.36691, patience: 121, training metric: 1.0, val metric: 0.0292\n",
            "INFO - root - epoch: 230, training loss: 0.05714, val loss: 3.35855, patience: 122, training metric: 1.0, val metric: 0.0216\n",
            "INFO - root - epoch: 231, training loss: 0.05691, val loss: 3.35828, patience: 123, training metric: 1.0, val metric: 0.0176\n",
            "INFO - root - epoch: 232, training loss: 0.05681, val loss: 3.34585, patience: 124, training metric: 1.0, val metric: 0.0216\n",
            "INFO - root - epoch: 233, training loss: 0.05659, val loss: 3.34989, patience: 125, training metric: 1.0, val metric: 0.0248\n",
            "INFO - root - epoch: 234, training loss: 0.05669, val loss: 3.35843, patience: 126, training metric: 1.0, val metric: 0.0288\n",
            "INFO - root - epoch: 235, training loss: 0.05648, val loss: 3.34605, patience: 127, training metric: 1.0, val metric: 0.0296\n",
            "INFO - root - epoch: 236, training loss: 0.05635, val loss: 3.35033, patience: 128, training metric: 1.0, val metric: 0.0368\n",
            "INFO - root - epoch: 237, training loss: 0.0563, val loss: 3.34786, patience: 129, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 238, training loss: 0.0559, val loss: 3.36419, patience: 130, training metric: 1.0, val metric: 0.0452\n",
            "INFO - root - epoch: 239, training loss: 0.05571, val loss: 3.38071, patience: 131, training metric: 1.0, val metric: 0.0504\n",
            "INFO - root - epoch: 240, training loss: 0.05587, val loss: 3.36789, patience: 132, training metric: 1.0, val metric: 0.0604\n",
            "INFO - root - epoch: 241, training loss: 0.05553, val loss: 3.371, patience: 133, training metric: 1.0, val metric: 0.0588\n",
            "INFO - root - epoch: 242, training loss: 0.05536, val loss: 3.36549, patience: 134, training metric: 1.0, val metric: 0.0604\n",
            "INFO - root - epoch: 243, training loss: 0.05539, val loss: 3.34991, patience: 135, training metric: 1.0, val metric: 0.0628\n",
            "INFO - root - epoch: 244, training loss: 0.05515, val loss: 3.34239, patience: 136, training metric: 1.0, val metric: 0.062\n",
            "INFO - root - epoch: 245, training loss: 0.05521, val loss: 3.33543, patience: 137, training metric: 1.0, val metric: 0.0616\n",
            "INFO - root - epoch: 246, training loss: 0.05477, val loss: 3.32718, patience: 138, training metric: 1.0, val metric: 0.0596\n",
            "INFO - root - epoch: 247, training loss: 0.05441, val loss: 3.31673, patience: 139, training metric: 1.0, val metric: 0.058\n",
            "INFO - root - epoch: 248, training loss: 0.05418, val loss: 3.33385, patience: 140, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 249, training loss: 0.05455, val loss: 3.34198, patience: 141, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 250, training loss: 0.05385, val loss: 3.34419, patience: 142, training metric: 1.0, val metric: 0.0472\n",
            "INFO - root - epoch: 251, training loss: 0.05405, val loss: 3.35744, patience: 143, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 252, training loss: 0.05364, val loss: 3.37644, patience: 144, training metric: 1.0, val metric: 0.03\n",
            "INFO - root - epoch: 253, training loss: 0.05383, val loss: 3.3972, patience: 145, training metric: 1.0, val metric: 0.0284\n",
            "INFO - root - epoch: 254, training loss: 0.0533, val loss: 3.41971, patience: 146, training metric: 1.0, val metric: 0.0296\n",
            "INFO - root - epoch: 255, training loss: 0.05295, val loss: 3.43491, patience: 147, training metric: 1.0, val metric: 0.0216\n",
            "INFO - root - epoch: 256, training loss: 0.05292, val loss: 3.46325, patience: 148, training metric: 1.0, val metric: 0.0208\n",
            "INFO - root - epoch: 257, training loss: 0.05293, val loss: 3.48535, patience: 149, training metric: 1.0, val metric: 0.0212\n",
            "INFO - root - epoch: 258, training loss: 0.05278, val loss: 3.50479, patience: 150, training metric: 1.0, val metric: 0.0232\n",
            "INFO - root - epoch: 259, training loss: 0.05262, val loss: 3.52226, patience: 151, training metric: 1.0, val metric: 0.0236\n",
            "INFO - root - epoch: 260, training loss: 0.05263, val loss: 3.5233, patience: 152, training metric: 1.0, val metric: 0.0228\n",
            "INFO - root - epoch: 261, training loss: 0.05214, val loss: 3.52766, patience: 153, training metric: 1.0, val metric: 0.0248\n",
            "INFO - root - epoch: 262, training loss: 0.05209, val loss: 3.5191, patience: 154, training metric: 1.0, val metric: 0.0268\n",
            "INFO - root - epoch: 263, training loss: 0.05224, val loss: 3.51294, patience: 155, training metric: 1.0, val metric: 0.0288\n",
            "INFO - root - epoch: 264, training loss: 0.05213, val loss: 3.50316, patience: 156, training metric: 1.0, val metric: 0.0252\n",
            "INFO - root - epoch: 265, training loss: 0.05196, val loss: 3.49003, patience: 157, training metric: 1.0, val metric: 0.0276\n",
            "INFO - root - epoch: 266, training loss: 0.05223, val loss: 3.47946, patience: 158, training metric: 1.0, val metric: 0.024\n",
            "INFO - root - epoch: 267, training loss: 0.05187, val loss: 3.45687, patience: 159, training metric: 1.0, val metric: 0.0296\n",
            "INFO - root - epoch: 268, training loss: 0.05185, val loss: 3.43695, patience: 160, training metric: 1.0, val metric: 0.0296\n",
            "INFO - root - epoch: 269, training loss: 0.05194, val loss: 3.41924, patience: 161, training metric: 1.0, val metric: 0.036\n",
            "INFO - root - epoch: 270, training loss: 0.05181, val loss: 3.40429, patience: 162, training metric: 1.0, val metric: 0.0364\n",
            "INFO - root - epoch: 271, training loss: 0.05118, val loss: 3.39459, patience: 163, training metric: 1.0, val metric: 0.0344\n",
            "INFO - root - epoch: 272, training loss: 0.05159, val loss: 3.39373, patience: 164, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 273, training loss: 0.05135, val loss: 3.37278, patience: 165, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 274, training loss: 0.05129, val loss: 3.37148, patience: 166, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 275, training loss: 0.05131, val loss: 3.36021, patience: 167, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 276, training loss: 0.05133, val loss: 3.34208, patience: 168, training metric: 1.0, val metric: 0.0464\n",
            "INFO - root - epoch: 277, training loss: 0.05091, val loss: 3.34497, patience: 169, training metric: 1.0, val metric: 0.0484\n",
            "INFO - root - epoch: 278, training loss: 0.05074, val loss: 3.34014, patience: 170, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 279, training loss: 0.05096, val loss: 3.33575, patience: 171, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 280, training loss: 0.05082, val loss: 3.34669, patience: 172, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 281, training loss: 0.05047, val loss: 3.33748, patience: 173, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 282, training loss: 0.0506, val loss: 3.34886, patience: 174, training metric: 1.0, val metric: 0.0344\n",
            "INFO - root - epoch: 283, training loss: 0.05072, val loss: 3.33749, patience: 175, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 284, training loss: 0.05053, val loss: 3.34873, patience: 176, training metric: 1.0, val metric: 0.032\n",
            "INFO - root - epoch: 285, training loss: 0.0504, val loss: 3.3484, patience: 177, training metric: 1.0, val metric: 0.036\n",
            "INFO - root - epoch: 286, training loss: 0.05023, val loss: 3.35709, patience: 178, training metric: 1.0, val metric: 0.036\n",
            "INFO - root - epoch: 287, training loss: 0.05051, val loss: 3.35491, patience: 179, training metric: 1.0, val metric: 0.0308\n",
            "INFO - root - epoch: 288, training loss: 0.0503, val loss: 3.36037, patience: 180, training metric: 1.0, val metric: 0.032\n",
            "INFO - root - epoch: 289, training loss: 0.0503, val loss: 3.36396, patience: 181, training metric: 1.0, val metric: 0.0368\n",
            "INFO - root - epoch: 290, training loss: 0.0503, val loss: 3.37647, patience: 182, training metric: 1.0, val metric: 0.0304\n",
            "INFO - root - epoch: 291, training loss: 0.04995, val loss: 3.37852, patience: 183, training metric: 1.0, val metric: 0.0368\n",
            "INFO - root - epoch: 292, training loss: 0.04996, val loss: 3.38514, patience: 184, training metric: 1.0, val metric: 0.0336\n",
            "INFO - root - epoch: 293, training loss: 0.0501, val loss: 3.39053, patience: 185, training metric: 1.0, val metric: 0.0336\n",
            "INFO - root - epoch: 294, training loss: 0.04967, val loss: 3.39726, patience: 186, training metric: 1.0, val metric: 0.034\n",
            "INFO - root - epoch: 295, training loss: 0.04959, val loss: 3.40335, patience: 187, training metric: 1.0, val metric: 0.032\n",
            "INFO - root - epoch: 296, training loss: 0.04953, val loss: 3.40757, patience: 188, training metric: 1.0, val metric: 0.0312\n",
            "INFO - root - epoch: 297, training loss: 0.04951, val loss: 3.41604, patience: 189, training metric: 1.0, val metric: 0.0292\n",
            "INFO - root - epoch: 298, training loss: 0.04942, val loss: 3.40867, patience: 190, training metric: 1.0, val metric: 0.0344\n",
            "INFO - root - epoch: 299, training loss: 0.0495, val loss: 3.42085, patience: 191, training metric: 1.0, val metric: 0.03\n",
            "INFO - root - epoch: 300, training loss: 0.04907, val loss: 3.42627, patience: 192, training metric: 1.0, val metric: 0.0252\n",
            "INFO - root - epoch: 301, training loss: 0.04933, val loss: 3.4274, patience: 193, training metric: 1.0, val metric: 0.024\n",
            "INFO - root - epoch: 302, training loss: 0.04909, val loss: 3.42718, patience: 194, training metric: 1.0, val metric: 0.0256\n",
            "INFO - root - epoch: 303, training loss: 0.04904, val loss: 3.43505, patience: 195, training metric: 1.0, val metric: 0.0316\n",
            "INFO - root - epoch: 304, training loss: 0.04905, val loss: 3.43815, patience: 196, training metric: 1.0, val metric: 0.0236\n",
            "INFO - root - epoch: 305, training loss: 0.04877, val loss: 3.44337, patience: 197, training metric: 1.0, val metric: 0.0232\n",
            "INFO - root - epoch: 306, training loss: 0.04894, val loss: 3.43868, patience: 198, training metric: 1.0, val metric: 0.0284\n",
            "INFO - root - epoch: 307, training loss: 0.04903, val loss: 3.42821, patience: 199, training metric: 1.0, val metric: 0.022\n",
            "INFO - root - epoch: 308, training loss: 0.04869, val loss: 3.4331, patience: 200, training metric: 1.0, val metric: 0.0324\n",
            "INFO - root - epoch: 309, training loss: 0.04874, val loss: 3.42563, patience: 201, training metric: 1.0, val metric: 0.0268\n",
            "INFO - root - epoch: 310, training loss: 0.04872, val loss: 3.41971, patience: 202, training metric: 1.0, val metric: 0.0272\n",
            "INFO - root - epoch: 311, training loss: 0.04856, val loss: 3.4215, patience: 203, training metric: 1.0, val metric: 0.0248\n",
            "INFO - root - epoch: 312, training loss: 0.049, val loss: 3.41316, patience: 204, training metric: 1.0, val metric: 0.03\n",
            "INFO - root - epoch: 313, training loss: 0.0484, val loss: 3.41877, patience: 205, training metric: 1.0, val metric: 0.0276\n",
            "INFO - root - epoch: 314, training loss: 0.04864, val loss: 3.40909, patience: 206, training metric: 1.0, val metric: 0.0308\n",
            "INFO - root - epoch: 315, training loss: 0.04865, val loss: 3.39232, patience: 207, training metric: 1.0, val metric: 0.0328\n",
            "INFO - root - epoch: 316, training loss: 0.04869, val loss: 3.40113, patience: 208, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 317, training loss: 0.04851, val loss: 3.39314, patience: 209, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 318, training loss: 0.04829, val loss: 3.38867, patience: 210, training metric: 1.0, val metric: 0.036\n",
            "INFO - root - epoch: 319, training loss: 0.0485, val loss: 3.38805, patience: 211, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 320, training loss: 0.04817, val loss: 3.38388, patience: 212, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 321, training loss: 0.04817, val loss: 3.38722, patience: 213, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 322, training loss: 0.04822, val loss: 3.39156, patience: 214, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 323, training loss: 0.04815, val loss: 3.39229, patience: 215, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 324, training loss: 0.04813, val loss: 3.38644, patience: 216, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 325, training loss: 0.04813, val loss: 3.3872, patience: 217, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 326, training loss: 0.0478, val loss: 3.38974, patience: 218, training metric: 1.0, val metric: 0.0332\n",
            "INFO - root - epoch: 327, training loss: 0.04835, val loss: 3.39417, patience: 219, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 328, training loss: 0.04807, val loss: 3.40241, patience: 220, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 329, training loss: 0.04829, val loss: 3.40043, patience: 221, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 330, training loss: 0.04786, val loss: 3.40122, patience: 222, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 331, training loss: 0.04792, val loss: 3.40165, patience: 223, training metric: 1.0, val metric: 0.0484\n",
            "INFO - root - epoch: 332, training loss: 0.04769, val loss: 3.40567, patience: 224, training metric: 1.0, val metric: 0.046\n",
            "INFO - root - epoch: 333, training loss: 0.04801, val loss: 3.41311, patience: 225, training metric: 1.0, val metric: 0.0464\n",
            "INFO - root - epoch: 334, training loss: 0.04785, val loss: 3.39467, patience: 226, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 335, training loss: 0.04784, val loss: 3.40199, patience: 227, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 336, training loss: 0.0478, val loss: 3.39778, patience: 228, training metric: 1.0, val metric: 0.0536\n",
            "INFO - root - epoch: 337, training loss: 0.04739, val loss: 3.41359, patience: 229, training metric: 1.0, val metric: 0.0472\n",
            "INFO - root - epoch: 338, training loss: 0.04772, val loss: 3.41805, patience: 230, training metric: 1.0, val metric: 0.0444\n",
            "INFO - root - epoch: 339, training loss: 0.04777, val loss: 3.4162, patience: 231, training metric: 1.0, val metric: 0.0508\n",
            "INFO - root - epoch: 340, training loss: 0.04739, val loss: 3.4209, patience: 232, training metric: 1.0, val metric: 0.0464\n",
            "INFO - root - epoch: 341, training loss: 0.04748, val loss: 3.42133, patience: 233, training metric: 1.0, val metric: 0.0484\n",
            "INFO - root - epoch: 342, training loss: 0.04733, val loss: 3.4219, patience: 234, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 343, training loss: 0.0477, val loss: 3.42711, patience: 235, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 344, training loss: 0.04782, val loss: 3.42368, patience: 236, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 345, training loss: 0.04736, val loss: 3.4317, patience: 237, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 346, training loss: 0.04721, val loss: 3.44019, patience: 238, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 347, training loss: 0.04719, val loss: 3.42814, patience: 239, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 348, training loss: 0.04743, val loss: 3.4344, patience: 240, training metric: 1.0, val metric: 0.048\n",
            "INFO - root - epoch: 349, training loss: 0.0473, val loss: 3.43052, patience: 241, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 350, training loss: 0.0474, val loss: 3.43773, patience: 242, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 351, training loss: 0.04689, val loss: 3.43752, patience: 243, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 352, training loss: 0.04754, val loss: 3.42232, patience: 244, training metric: 1.0, val metric: 0.0484\n",
            "INFO - root - epoch: 353, training loss: 0.04724, val loss: 3.42997, patience: 245, training metric: 1.0, val metric: 0.0484\n",
            "INFO - root - epoch: 354, training loss: 0.04707, val loss: 3.42622, patience: 246, training metric: 1.0, val metric: 0.0464\n",
            "INFO - root - epoch: 355, training loss: 0.04685, val loss: 3.43199, patience: 247, training metric: 1.0, val metric: 0.0492\n",
            "INFO - root - epoch: 356, training loss: 0.04741, val loss: 3.41931, patience: 248, training metric: 1.0, val metric: 0.0548\n",
            "INFO - root - epoch: 357, training loss: 0.0469, val loss: 3.42475, patience: 249, training metric: 1.0, val metric: 0.0496\n",
            "INFO - root - epoch: 358, training loss: 0.04716, val loss: 3.41475, patience: 250, training metric: 1.0, val metric: 0.0552\n",
            "INFO - root - epoch: 359, training loss: 0.04726, val loss: 3.41947, patience: 251, training metric: 1.0, val metric: 0.0564\n",
            "INFO - root - epoch: 360, training loss: 0.04685, val loss: 3.41345, patience: 252, training metric: 1.0, val metric: 0.0544\n",
            "INFO - root - epoch: 361, training loss: 0.047, val loss: 3.41063, patience: 253, training metric: 1.0, val metric: 0.0528\n",
            "INFO - root - epoch: 362, training loss: 0.04665, val loss: 3.41564, patience: 254, training metric: 1.0, val metric: 0.0544\n",
            "INFO - root - epoch: 363, training loss: 0.04687, val loss: 3.40687, patience: 255, training metric: 1.0, val metric: 0.0504\n",
            "INFO - root - epoch: 364, training loss: 0.04666, val loss: 3.41102, patience: 256, training metric: 1.0, val metric: 0.0512\n",
            "INFO - root - epoch: 365, training loss: 0.04631, val loss: 3.40684, patience: 257, training metric: 1.0, val metric: 0.0552\n",
            "INFO - root - epoch: 366, training loss: 0.04645, val loss: 3.39918, patience: 258, training metric: 1.0, val metric: 0.0492\n",
            "INFO - root - epoch: 367, training loss: 0.04667, val loss: 3.40299, patience: 259, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 368, training loss: 0.04667, val loss: 3.39664, patience: 260, training metric: 1.0, val metric: 0.0484\n",
            "INFO - root - epoch: 369, training loss: 0.04653, val loss: 3.40695, patience: 261, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 370, training loss: 0.04662, val loss: 3.40193, patience: 262, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 371, training loss: 0.04634, val loss: 3.41121, patience: 263, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 372, training loss: 0.04656, val loss: 3.40034, patience: 264, training metric: 1.0, val metric: 0.0472\n",
            "INFO - root - epoch: 373, training loss: 0.04659, val loss: 3.39313, patience: 265, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 374, training loss: 0.04637, val loss: 3.39643, patience: 266, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 375, training loss: 0.04648, val loss: 3.39909, patience: 267, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 376, training loss: 0.04658, val loss: 3.39879, patience: 268, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 377, training loss: 0.04659, val loss: 3.39003, patience: 269, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 378, training loss: 0.04657, val loss: 3.3916, patience: 270, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 379, training loss: 0.04663, val loss: 3.39618, patience: 271, training metric: 1.0, val metric: 0.0408\n",
            "INFO - root - epoch: 380, training loss: 0.04643, val loss: 3.39204, patience: 272, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 381, training loss: 0.04631, val loss: 3.39359, patience: 273, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 382, training loss: 0.04624, val loss: 3.39435, patience: 274, training metric: 1.0, val metric: 0.0368\n",
            "INFO - root - epoch: 383, training loss: 0.04646, val loss: 3.38159, patience: 275, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 384, training loss: 0.04637, val loss: 3.39249, patience: 276, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 385, training loss: 0.04647, val loss: 3.38624, patience: 277, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 386, training loss: 0.04606, val loss: 3.39381, patience: 278, training metric: 1.0, val metric: 0.0368\n",
            "INFO - root - epoch: 387, training loss: 0.0464, val loss: 3.39168, patience: 279, training metric: 1.0, val metric: 0.0368\n",
            "INFO - root - epoch: 388, training loss: 0.04605, val loss: 3.38878, patience: 280, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 389, training loss: 0.04628, val loss: 3.39718, patience: 281, training metric: 1.0, val metric: 0.0304\n",
            "INFO - root - epoch: 390, training loss: 0.04619, val loss: 3.40041, patience: 282, training metric: 1.0, val metric: 0.034\n",
            "INFO - root - epoch: 391, training loss: 0.04632, val loss: 3.39626, patience: 283, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 392, training loss: 0.0462, val loss: 3.39938, patience: 284, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 393, training loss: 0.04606, val loss: 3.39938, patience: 285, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 394, training loss: 0.04601, val loss: 3.4009, patience: 286, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 395, training loss: 0.04609, val loss: 3.3944, patience: 287, training metric: 1.0, val metric: 0.0328\n",
            "INFO - root - epoch: 396, training loss: 0.04616, val loss: 3.40214, patience: 288, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 397, training loss: 0.04617, val loss: 3.40227, patience: 289, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 398, training loss: 0.0463, val loss: 3.40605, patience: 290, training metric: 1.0, val metric: 0.036\n",
            "INFO - root - epoch: 399, training loss: 0.04597, val loss: 3.40714, patience: 291, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 400, training loss: 0.046, val loss: 3.42132, patience: 292, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 401, training loss: 0.04603, val loss: 3.41458, patience: 293, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 402, training loss: 0.04587, val loss: 3.41323, patience: 294, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 403, training loss: 0.04604, val loss: 3.41714, patience: 295, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 404, training loss: 0.04563, val loss: 3.41889, patience: 296, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 405, training loss: 0.04606, val loss: 3.42335, patience: 297, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 406, training loss: 0.04583, val loss: 3.41905, patience: 298, training metric: 1.0, val metric: 0.044\n",
            "INFO - root - epoch: 407, training loss: 0.04564, val loss: 3.41352, patience: 299, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 408, training loss: 0.04604, val loss: 3.41631, patience: 300, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 409, training loss: 0.04583, val loss: 3.42556, patience: 301, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 410, training loss: 0.04613, val loss: 3.42418, patience: 302, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 411, training loss: 0.04569, val loss: 3.43089, patience: 303, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 412, training loss: 0.0458, val loss: 3.41824, patience: 304, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 413, training loss: 0.0456, val loss: 3.43129, patience: 305, training metric: 1.0, val metric: 0.0364\n",
            "INFO - root - epoch: 414, training loss: 0.04585, val loss: 3.43701, patience: 306, training metric: 1.0, val metric: 0.048\n",
            "INFO - root - epoch: 415, training loss: 0.04566, val loss: 3.43314, patience: 307, training metric: 1.0, val metric: 0.036\n",
            "INFO - root - epoch: 416, training loss: 0.04564, val loss: 3.43063, patience: 308, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 417, training loss: 0.04578, val loss: 3.42247, patience: 309, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 418, training loss: 0.04576, val loss: 3.41876, patience: 310, training metric: 1.0, val metric: 0.0364\n",
            "INFO - root - epoch: 419, training loss: 0.04587, val loss: 3.43145, patience: 311, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 420, training loss: 0.04559, val loss: 3.42255, patience: 312, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 421, training loss: 0.04591, val loss: 3.4194, patience: 313, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 422, training loss: 0.04573, val loss: 3.42132, patience: 314, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 423, training loss: 0.04566, val loss: 3.41823, patience: 315, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 424, training loss: 0.04573, val loss: 3.40656, patience: 316, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 425, training loss: 0.04571, val loss: 3.41475, patience: 317, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 426, training loss: 0.04557, val loss: 3.41667, patience: 318, training metric: 1.0, val metric: 0.0352\n",
            "INFO - root - epoch: 427, training loss: 0.04587, val loss: 3.41233, patience: 319, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 428, training loss: 0.04563, val loss: 3.40988, patience: 320, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 429, training loss: 0.04569, val loss: 3.42251, patience: 321, training metric: 1.0, val metric: 0.034\n",
            "INFO - root - epoch: 430, training loss: 0.04581, val loss: 3.41566, patience: 322, training metric: 1.0, val metric: 0.032\n",
            "INFO - root - epoch: 431, training loss: 0.04573, val loss: 3.41555, patience: 323, training metric: 1.0, val metric: 0.0444\n",
            "INFO - root - epoch: 432, training loss: 0.04565, val loss: 3.41837, patience: 324, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 433, training loss: 0.04578, val loss: 3.41102, patience: 325, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 434, training loss: 0.04565, val loss: 3.41822, patience: 326, training metric: 1.0, val metric: 0.0352\n",
            "INFO - root - epoch: 435, training loss: 0.04592, val loss: 3.41801, patience: 327, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 436, training loss: 0.04558, val loss: 3.42188, patience: 328, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 437, training loss: 0.04572, val loss: 3.41867, patience: 329, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 438, training loss: 0.04555, val loss: 3.41981, patience: 330, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 439, training loss: 0.04586, val loss: 3.41321, patience: 331, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 440, training loss: 0.04554, val loss: 3.41885, patience: 332, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 441, training loss: 0.04555, val loss: 3.41231, patience: 333, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 442, training loss: 0.04549, val loss: 3.4155, patience: 334, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 443, training loss: 0.0455, val loss: 3.42097, patience: 335, training metric: 1.0, val metric: 0.044\n",
            "INFO - root - epoch: 444, training loss: 0.04545, val loss: 3.42753, patience: 336, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 445, training loss: 0.04559, val loss: 3.42076, patience: 337, training metric: 1.0, val metric: 0.044\n",
            "INFO - root - epoch: 446, training loss: 0.04553, val loss: 3.41329, patience: 338, training metric: 1.0, val metric: 0.046\n",
            "INFO - root - epoch: 447, training loss: 0.04527, val loss: 3.41205, patience: 339, training metric: 1.0, val metric: 0.0464\n",
            "INFO - root - epoch: 448, training loss: 0.04533, val loss: 3.42201, patience: 340, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 449, training loss: 0.04547, val loss: 3.42164, patience: 341, training metric: 1.0, val metric: 0.0444\n",
            "INFO - root - epoch: 450, training loss: 0.04542, val loss: 3.41487, patience: 342, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 451, training loss: 0.04519, val loss: 3.41762, patience: 343, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 452, training loss: 0.0456, val loss: 3.42065, patience: 344, training metric: 1.0, val metric: 0.048\n",
            "INFO - root - epoch: 453, training loss: 0.04521, val loss: 3.41216, patience: 345, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 454, training loss: 0.04536, val loss: 3.41715, patience: 346, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 455, training loss: 0.04561, val loss: 3.41498, patience: 347, training metric: 1.0, val metric: 0.048\n",
            "INFO - root - epoch: 456, training loss: 0.04547, val loss: 3.4089, patience: 348, training metric: 1.0, val metric: 0.052\n",
            "INFO - root - epoch: 457, training loss: 0.04532, val loss: 3.40706, patience: 349, training metric: 1.0, val metric: 0.046\n",
            "INFO - root - epoch: 458, training loss: 0.04556, val loss: 3.41607, patience: 350, training metric: 1.0, val metric: 0.0456\n",
            "INFO - root - epoch: 459, training loss: 0.04497, val loss: 3.42228, patience: 351, training metric: 1.0, val metric: 0.0456\n",
            "INFO - root - epoch: 460, training loss: 0.04535, val loss: 3.41361, patience: 352, training metric: 1.0, val metric: 0.0444\n",
            "INFO - root - epoch: 461, training loss: 0.0454, val loss: 3.41344, patience: 353, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 462, training loss: 0.04517, val loss: 3.40469, patience: 354, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 463, training loss: 0.04554, val loss: 3.41576, patience: 355, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 464, training loss: 0.04556, val loss: 3.41304, patience: 356, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 465, training loss: 0.04532, val loss: 3.42162, patience: 357, training metric: 1.0, val metric: 0.0472\n",
            "INFO - root - epoch: 466, training loss: 0.04527, val loss: 3.41339, patience: 358, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 467, training loss: 0.04501, val loss: 3.40845, patience: 359, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 468, training loss: 0.04511, val loss: 3.42034, patience: 360, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 469, training loss: 0.04536, val loss: 3.41881, patience: 361, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 470, training loss: 0.04523, val loss: 3.41437, patience: 362, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 471, training loss: 0.04556, val loss: 3.41759, patience: 363, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 472, training loss: 0.04534, val loss: 3.41269, patience: 364, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 473, training loss: 0.04523, val loss: 3.40815, patience: 365, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 474, training loss: 0.0453, val loss: 3.41663, patience: 366, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 475, training loss: 0.04538, val loss: 3.41838, patience: 367, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 476, training loss: 0.04531, val loss: 3.41701, patience: 368, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 477, training loss: 0.04517, val loss: 3.40857, patience: 369, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 478, training loss: 0.04547, val loss: 3.41457, patience: 370, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 479, training loss: 0.04536, val loss: 3.41146, patience: 371, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 480, training loss: 0.0451, val loss: 3.41281, patience: 372, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 481, training loss: 0.04526, val loss: 3.40871, patience: 373, training metric: 1.0, val metric: 0.048\n",
            "INFO - root - epoch: 482, training loss: 0.04526, val loss: 3.41742, patience: 374, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 483, training loss: 0.0453, val loss: 3.40791, patience: 375, training metric: 1.0, val metric: 0.0408\n",
            "INFO - root - epoch: 484, training loss: 0.04521, val loss: 3.41147, patience: 376, training metric: 1.0, val metric: 0.0408\n",
            "INFO - root - epoch: 485, training loss: 0.045, val loss: 3.41139, patience: 377, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 486, training loss: 0.04508, val loss: 3.41824, patience: 378, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 487, training loss: 0.04512, val loss: 3.41426, patience: 379, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 488, training loss: 0.0452, val loss: 3.40975, patience: 380, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 489, training loss: 0.04533, val loss: 3.41307, patience: 381, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 490, training loss: 0.04515, val loss: 3.40575, patience: 382, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 491, training loss: 0.04528, val loss: 3.41438, patience: 383, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 492, training loss: 0.04509, val loss: 3.41352, patience: 384, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 493, training loss: 0.04505, val loss: 3.41158, patience: 385, training metric: 1.0, val metric: 0.0348\n",
            "INFO - root - epoch: 494, training loss: 0.04505, val loss: 3.40153, patience: 386, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 495, training loss: 0.04509, val loss: 3.40134, patience: 387, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 496, training loss: 0.04527, val loss: 3.41262, patience: 388, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 497, training loss: 0.04521, val loss: 3.40611, patience: 389, training metric: 1.0, val metric: 0.0344\n",
            "INFO - root - epoch: 498, training loss: 0.04502, val loss: 3.41348, patience: 390, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 499, training loss: 0.04512, val loss: 3.39826, patience: 391, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 500, training loss: 0.04501, val loss: 3.39792, patience: 392, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 501, training loss: 0.04493, val loss: 3.40766, patience: 393, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 502, training loss: 0.04519, val loss: 3.40162, patience: 394, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 503, training loss: 0.04537, val loss: 3.40371, patience: 395, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 504, training loss: 0.04496, val loss: 3.41209, patience: 396, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 505, training loss: 0.04499, val loss: 3.40354, patience: 397, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 506, training loss: 0.04523, val loss: 3.40108, patience: 398, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 507, training loss: 0.04495, val loss: 3.40873, patience: 399, training metric: 1.0, val metric: 0.0344\n",
            "INFO - root - epoch: 508, training loss: 0.04499, val loss: 3.40419, patience: 400, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 509, training loss: 0.04517, val loss: 3.39855, patience: 401, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 510, training loss: 0.04494, val loss: 3.40755, patience: 402, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 511, training loss: 0.04498, val loss: 3.40068, patience: 403, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 512, training loss: 0.04494, val loss: 3.40385, patience: 404, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 513, training loss: 0.04487, val loss: 3.4012, patience: 405, training metric: 1.0, val metric: 0.0348\n",
            "INFO - root - epoch: 514, training loss: 0.04502, val loss: 3.40446, patience: 406, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 515, training loss: 0.04516, val loss: 3.40841, patience: 407, training metric: 1.0, val metric: 0.0364\n",
            "INFO - root - epoch: 516, training loss: 0.04491, val loss: 3.40522, patience: 408, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 517, training loss: 0.04499, val loss: 3.40262, patience: 409, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 518, training loss: 0.04511, val loss: 3.41193, patience: 410, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 519, training loss: 0.04505, val loss: 3.40378, patience: 411, training metric: 1.0, val metric: 0.034\n",
            "INFO - root - epoch: 520, training loss: 0.04491, val loss: 3.41168, patience: 412, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 521, training loss: 0.04496, val loss: 3.40978, patience: 413, training metric: 1.0, val metric: 0.0328\n",
            "INFO - root - epoch: 522, training loss: 0.04476, val loss: 3.41417, patience: 414, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 523, training loss: 0.04507, val loss: 3.40176, patience: 415, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 524, training loss: 0.04474, val loss: 3.40586, patience: 416, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 525, training loss: 0.04491, val loss: 3.41051, patience: 417, training metric: 1.0, val metric: 0.0348\n",
            "INFO - root - epoch: 526, training loss: 0.04506, val loss: 3.40708, patience: 418, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 527, training loss: 0.04527, val loss: 3.41529, patience: 419, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 528, training loss: 0.04507, val loss: 3.41571, patience: 420, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 529, training loss: 0.04498, val loss: 3.40485, patience: 421, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 530, training loss: 0.04471, val loss: 3.40902, patience: 422, training metric: 1.0, val metric: 0.0352\n",
            "INFO - root - epoch: 531, training loss: 0.0449, val loss: 3.40939, patience: 423, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 532, training loss: 0.04478, val loss: 3.41493, patience: 424, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 533, training loss: 0.04498, val loss: 3.41583, patience: 425, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 534, training loss: 0.04473, val loss: 3.42203, patience: 426, training metric: 1.0, val metric: 0.0364\n",
            "INFO - root - epoch: 535, training loss: 0.04517, val loss: 3.41682, patience: 427, training metric: 1.0, val metric: 0.0456\n",
            "INFO - root - epoch: 536, training loss: 0.04481, val loss: 3.41634, patience: 428, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 537, training loss: 0.04511, val loss: 3.41097, patience: 429, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 538, training loss: 0.04488, val loss: 3.415, patience: 430, training metric: 1.0, val metric: 0.044\n",
            "INFO - root - epoch: 539, training loss: 0.04478, val loss: 3.42323, patience: 431, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 540, training loss: 0.04495, val loss: 3.41135, patience: 432, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 541, training loss: 0.04495, val loss: 3.41063, patience: 433, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 542, training loss: 0.04477, val loss: 3.41643, patience: 434, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 543, training loss: 0.04476, val loss: 3.42111, patience: 435, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 544, training loss: 0.04487, val loss: 3.41455, patience: 436, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 545, training loss: 0.04482, val loss: 3.41606, patience: 437, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 546, training loss: 0.04479, val loss: 3.41488, patience: 438, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 547, training loss: 0.04473, val loss: 3.41686, patience: 439, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 548, training loss: 0.0447, val loss: 3.42415, patience: 440, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 549, training loss: 0.04462, val loss: 3.4203, patience: 441, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 550, training loss: 0.04475, val loss: 3.42441, patience: 442, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 551, training loss: 0.0449, val loss: 3.41759, patience: 443, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 552, training loss: 0.0447, val loss: 3.41401, patience: 444, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 553, training loss: 0.04474, val loss: 3.41879, patience: 445, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 554, training loss: 0.04476, val loss: 3.42804, patience: 446, training metric: 1.0, val metric: 0.0452\n",
            "INFO - root - epoch: 555, training loss: 0.04467, val loss: 3.42549, patience: 447, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 556, training loss: 0.04476, val loss: 3.42235, patience: 448, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 557, training loss: 0.04475, val loss: 3.43226, patience: 449, training metric: 1.0, val metric: 0.0364\n",
            "INFO - root - epoch: 558, training loss: 0.04473, val loss: 3.41765, patience: 450, training metric: 1.0, val metric: 0.0408\n",
            "INFO - root - epoch: 559, training loss: 0.0447, val loss: 3.4233, patience: 451, training metric: 1.0, val metric: 0.044\n",
            "INFO - root - epoch: 560, training loss: 0.04468, val loss: 3.42631, patience: 452, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 561, training loss: 0.04468, val loss: 3.42351, patience: 453, training metric: 1.0, val metric: 0.046\n",
            "INFO - root - epoch: 562, training loss: 0.04484, val loss: 3.42862, patience: 454, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 563, training loss: 0.04472, val loss: 3.41943, patience: 455, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 564, training loss: 0.04468, val loss: 3.42563, patience: 456, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 565, training loss: 0.04465, val loss: 3.42534, patience: 457, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 566, training loss: 0.04433, val loss: 3.42814, patience: 458, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 567, training loss: 0.04474, val loss: 3.42048, patience: 459, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 568, training loss: 0.04456, val loss: 3.43039, patience: 460, training metric: 1.0, val metric: 0.0344\n",
            "INFO - root - epoch: 569, training loss: 0.04462, val loss: 3.41556, patience: 461, training metric: 1.0, val metric: 0.0456\n",
            "INFO - root - epoch: 570, training loss: 0.0443, val loss: 3.42466, patience: 462, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 571, training loss: 0.04472, val loss: 3.43222, patience: 463, training metric: 1.0, val metric: 0.034\n",
            "INFO - root - epoch: 572, training loss: 0.04463, val loss: 3.43072, patience: 464, training metric: 1.0, val metric: 0.0328\n",
            "INFO - root - epoch: 573, training loss: 0.04464, val loss: 3.42355, patience: 465, training metric: 1.0, val metric: 0.0364\n",
            "INFO - root - epoch: 574, training loss: 0.04443, val loss: 3.41959, patience: 466, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 575, training loss: 0.0446, val loss: 3.41948, patience: 467, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 576, training loss: 0.04456, val loss: 3.42727, patience: 468, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 577, training loss: 0.04446, val loss: 3.42534, patience: 469, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 578, training loss: 0.04448, val loss: 3.43065, patience: 470, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 579, training loss: 0.04467, val loss: 3.43659, patience: 471, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 580, training loss: 0.04455, val loss: 3.42707, patience: 472, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 581, training loss: 0.04446, val loss: 3.43187, patience: 473, training metric: 1.0, val metric: 0.0364\n",
            "INFO - root - epoch: 582, training loss: 0.04443, val loss: 3.42966, patience: 474, training metric: 1.0, val metric: 0.0408\n",
            "INFO - root - epoch: 583, training loss: 0.04414, val loss: 3.43346, patience: 475, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 584, training loss: 0.04452, val loss: 3.42509, patience: 476, training metric: 1.0, val metric: 0.0352\n",
            "INFO - root - epoch: 585, training loss: 0.04453, val loss: 3.42673, patience: 477, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 586, training loss: 0.04444, val loss: 3.42462, patience: 478, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 587, training loss: 0.04444, val loss: 3.42699, patience: 479, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 588, training loss: 0.0446, val loss: 3.42973, patience: 480, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 589, training loss: 0.04448, val loss: 3.43039, patience: 481, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 590, training loss: 0.0444, val loss: 3.4295, patience: 482, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 591, training loss: 0.04482, val loss: 3.42855, patience: 483, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 592, training loss: 0.04447, val loss: 3.4205, patience: 484, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 593, training loss: 0.04439, val loss: 3.43104, patience: 485, training metric: 1.0, val metric: 0.034\n",
            "INFO - root - epoch: 594, training loss: 0.04436, val loss: 3.43303, patience: 486, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 595, training loss: 0.04443, val loss: 3.42567, patience: 487, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 596, training loss: 0.04426, val loss: 3.42881, patience: 488, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 597, training loss: 0.04437, val loss: 3.42412, patience: 489, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 598, training loss: 0.04449, val loss: 3.42442, patience: 490, training metric: 1.0, val metric: 0.0464\n",
            "INFO - root - epoch: 599, training loss: 0.0447, val loss: 3.42627, patience: 491, training metric: 1.0, val metric: 0.0472\n",
            "INFO - root - epoch: 600, training loss: 0.04446, val loss: 3.42833, patience: 492, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 601, training loss: 0.04437, val loss: 3.42842, patience: 493, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 602, training loss: 0.04432, val loss: 3.43016, patience: 494, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 603, training loss: 0.0442, val loss: 3.4261, patience: 495, training metric: 1.0, val metric: 0.0456\n",
            "INFO - root - epoch: 604, training loss: 0.04444, val loss: 3.41877, patience: 496, training metric: 1.0, val metric: 0.0456\n",
            "INFO - root - epoch: 605, training loss: 0.04433, val loss: 3.44029, patience: 497, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 606, training loss: 0.04452, val loss: 3.42774, patience: 498, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 607, training loss: 0.0444, val loss: 3.4329, patience: 499, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 608, training loss: 0.0442, val loss: 3.42114, patience: 500, training metric: 1.0, val metric: 0.0484\n",
            "INFO - root - epoch: 609, training loss: 0.04441, val loss: 3.43523, patience: 501, training metric: 1.0, val metric: 0.0456\n",
            "INFO - root - epoch: 610, training loss: 0.04424, val loss: 3.42828, patience: 502, training metric: 1.0, val metric: 0.0464\n",
            "INFO - root - epoch: 611, training loss: 0.04446, val loss: 3.43061, patience: 503, training metric: 1.0, val metric: 0.0516\n",
            "INFO - root - epoch: 612, training loss: 0.04409, val loss: 3.42776, patience: 504, training metric: 1.0, val metric: 0.048\n",
            "INFO - root - epoch: 613, training loss: 0.0444, val loss: 3.43551, patience: 505, training metric: 1.0, val metric: 0.0408\n",
            "INFO - root - epoch: 614, training loss: 0.04445, val loss: 3.41986, patience: 506, training metric: 1.0, val metric: 0.0456\n",
            "INFO - root - epoch: 615, training loss: 0.04419, val loss: 3.42547, patience: 507, training metric: 1.0, val metric: 0.0444\n",
            "INFO - root - epoch: 616, training loss: 0.04429, val loss: 3.43449, patience: 508, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 617, training loss: 0.04433, val loss: 3.42995, patience: 509, training metric: 1.0, val metric: 0.0472\n",
            "INFO - root - epoch: 618, training loss: 0.04436, val loss: 3.42236, patience: 510, training metric: 1.0, val metric: 0.0408\n",
            "INFO - root - epoch: 619, training loss: 0.04438, val loss: 3.42756, patience: 511, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 620, training loss: 0.04439, val loss: 3.42272, patience: 512, training metric: 1.0, val metric: 0.0496\n",
            "INFO - root - epoch: 621, training loss: 0.04452, val loss: 3.4242, patience: 513, training metric: 1.0, val metric: 0.0452\n",
            "INFO - root - epoch: 622, training loss: 0.04426, val loss: 3.43036, patience: 514, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 623, training loss: 0.04452, val loss: 3.43047, patience: 515, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 624, training loss: 0.04424, val loss: 3.43944, patience: 516, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 625, training loss: 0.04439, val loss: 3.42444, patience: 517, training metric: 1.0, val metric: 0.0464\n",
            "INFO - root - epoch: 626, training loss: 0.04446, val loss: 3.43002, patience: 518, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 627, training loss: 0.04398, val loss: 3.42259, patience: 519, training metric: 1.0, val metric: 0.0408\n",
            "INFO - root - epoch: 628, training loss: 0.04424, val loss: 3.42499, patience: 520, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 629, training loss: 0.04451, val loss: 3.42051, patience: 521, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 630, training loss: 0.04394, val loss: 3.41696, patience: 522, training metric: 1.0, val metric: 0.0336\n",
            "INFO - root - epoch: 631, training loss: 0.04433, val loss: 3.42958, patience: 523, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 632, training loss: 0.04424, val loss: 3.4285, patience: 524, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 633, training loss: 0.0442, val loss: 3.43436, patience: 525, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 634, training loss: 0.04414, val loss: 3.43743, patience: 526, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 635, training loss: 0.04433, val loss: 3.42458, patience: 527, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 636, training loss: 0.04437, val loss: 3.4232, patience: 528, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 637, training loss: 0.04417, val loss: 3.43274, patience: 529, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 638, training loss: 0.04421, val loss: 3.42929, patience: 530, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 639, training loss: 0.044, val loss: 3.43539, patience: 531, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 640, training loss: 0.04417, val loss: 3.43589, patience: 532, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 641, training loss: 0.04454, val loss: 3.4323, patience: 533, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 642, training loss: 0.04434, val loss: 3.42637, patience: 534, training metric: 1.0, val metric: 0.048\n",
            "INFO - root - epoch: 643, training loss: 0.04406, val loss: 3.4335, patience: 535, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 644, training loss: 0.04439, val loss: 3.43644, patience: 536, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 645, training loss: 0.04436, val loss: 3.4309, patience: 537, training metric: 1.0, val metric: 0.0412\n",
            "INFO - root - epoch: 646, training loss: 0.04422, val loss: 3.43787, patience: 538, training metric: 1.0, val metric: 0.0464\n",
            "INFO - root - epoch: 647, training loss: 0.04443, val loss: 3.43643, patience: 539, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 648, training loss: 0.04427, val loss: 3.42283, patience: 540, training metric: 1.0, val metric: 0.046\n",
            "INFO - root - epoch: 649, training loss: 0.04409, val loss: 3.42675, patience: 541, training metric: 1.0, val metric: 0.0496\n",
            "INFO - root - epoch: 650, training loss: 0.04406, val loss: 3.42785, patience: 542, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 651, training loss: 0.04419, val loss: 3.42914, patience: 543, training metric: 1.0, val metric: 0.0392\n",
            "INFO - root - epoch: 652, training loss: 0.04412, val loss: 3.43448, patience: 544, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 653, training loss: 0.04405, val loss: 3.42635, patience: 545, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 654, training loss: 0.04429, val loss: 3.43967, patience: 546, training metric: 1.0, val metric: 0.0356\n",
            "INFO - root - epoch: 655, training loss: 0.04405, val loss: 3.42701, patience: 547, training metric: 1.0, val metric: 0.0452\n",
            "INFO - root - epoch: 656, training loss: 0.04416, val loss: 3.43111, patience: 548, training metric: 1.0, val metric: 0.034\n",
            "INFO - root - epoch: 657, training loss: 0.04411, val loss: 3.44023, patience: 549, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 658, training loss: 0.04404, val loss: 3.43289, patience: 550, training metric: 1.0, val metric: 0.0456\n",
            "INFO - root - epoch: 659, training loss: 0.04424, val loss: 3.42435, patience: 551, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 660, training loss: 0.04393, val loss: 3.42519, patience: 552, training metric: 1.0, val metric: 0.0368\n",
            "INFO - root - epoch: 661, training loss: 0.04381, val loss: 3.42139, patience: 553, training metric: 1.0, val metric: 0.0332\n",
            "INFO - root - epoch: 662, training loss: 0.04426, val loss: 3.42299, patience: 554, training metric: 1.0, val metric: 0.0408\n",
            "INFO - root - epoch: 663, training loss: 0.04409, val loss: 3.42221, patience: 555, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 664, training loss: 0.04395, val loss: 3.42778, patience: 556, training metric: 1.0, val metric: 0.0348\n",
            "INFO - root - epoch: 665, training loss: 0.04396, val loss: 3.42503, patience: 557, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 666, training loss: 0.04398, val loss: 3.42202, patience: 558, training metric: 1.0, val metric: 0.0376\n",
            "INFO - root - epoch: 667, training loss: 0.04383, val loss: 3.42545, patience: 559, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 668, training loss: 0.04388, val loss: 3.41406, patience: 560, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 669, training loss: 0.04413, val loss: 3.42429, patience: 561, training metric: 1.0, val metric: 0.04\n",
            "INFO - root - epoch: 670, training loss: 0.04407, val loss: 3.41254, patience: 562, training metric: 1.0, val metric: 0.046\n",
            "INFO - root - epoch: 671, training loss: 0.04391, val loss: 3.41156, patience: 563, training metric: 1.0, val metric: 0.0424\n",
            "INFO - root - epoch: 672, training loss: 0.04409, val loss: 3.43215, patience: 564, training metric: 1.0, val metric: 0.0372\n",
            "INFO - root - epoch: 673, training loss: 0.044, val loss: 3.43052, patience: 565, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 674, training loss: 0.04376, val loss: 3.42778, patience: 566, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 675, training loss: 0.04403, val loss: 3.41992, patience: 567, training metric: 1.0, val metric: 0.0452\n",
            "INFO - root - epoch: 676, training loss: 0.04409, val loss: 3.42513, patience: 568, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 677, training loss: 0.04382, val loss: 3.42052, patience: 569, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 678, training loss: 0.0441, val loss: 3.42239, patience: 570, training metric: 1.0, val metric: 0.0468\n",
            "INFO - root - epoch: 679, training loss: 0.04378, val loss: 3.41775, patience: 571, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 680, training loss: 0.04377, val loss: 3.4245, patience: 572, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 681, training loss: 0.04385, val loss: 3.41933, patience: 573, training metric: 1.0, val metric: 0.0404\n",
            "INFO - root - epoch: 682, training loss: 0.04421, val loss: 3.41726, patience: 574, training metric: 1.0, val metric: 0.046\n",
            "INFO - root - epoch: 683, training loss: 0.04372, val loss: 3.42543, patience: 575, training metric: 1.0, val metric: 0.044\n",
            "INFO - root - epoch: 684, training loss: 0.04371, val loss: 3.43275, patience: 576, training metric: 1.0, val metric: 0.0416\n",
            "INFO - root - epoch: 685, training loss: 0.04379, val loss: 3.43039, patience: 577, training metric: 1.0, val metric: 0.0428\n",
            "INFO - root - epoch: 686, training loss: 0.04417, val loss: 3.429, patience: 578, training metric: 1.0, val metric: 0.0396\n",
            "INFO - root - epoch: 687, training loss: 0.04376, val loss: 3.42293, patience: 579, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 688, training loss: 0.04392, val loss: 3.41695, patience: 580, training metric: 1.0, val metric: 0.042\n",
            "INFO - root - epoch: 689, training loss: 0.04388, val loss: 3.4225, patience: 581, training metric: 1.0, val metric: 0.0408\n",
            "INFO - root - epoch: 690, training loss: 0.04362, val loss: 3.42958, patience: 582, training metric: 1.0, val metric: 0.0432\n",
            "INFO - root - epoch: 691, training loss: 0.04384, val loss: 3.41788, patience: 583, training metric: 1.0, val metric: 0.044\n",
            "INFO - root - epoch: 692, training loss: 0.04388, val loss: 3.42458, patience: 584, training metric: 1.0, val metric: 0.0452\n",
            "INFO - root - epoch: 693, training loss: 0.04366, val loss: 3.43003, patience: 585, training metric: 1.0, val metric: 0.0436\n",
            "INFO - root - epoch: 694, training loss: 0.04367, val loss: 3.43021, patience: 586, training metric: 1.0, val metric: 0.0388\n",
            "INFO - root - epoch: 695, training loss: 0.04377, val loss: 3.4302, patience: 587, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 696, training loss: 0.04386, val loss: 3.43414, patience: 588, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - epoch: 697, training loss: 0.04365, val loss: 3.43805, patience: 589, training metric: 1.0, val metric: 0.0384\n",
            "INFO - root - epoch: 698, training loss: 0.04366, val loss: 3.43106, patience: 590, training metric: 1.0, val metric: 0.044\n",
            "INFO - root - epoch: 699, training loss: 0.04378, val loss: 3.43735, patience: 591, training metric: 1.0, val metric: 0.0448\n",
            "INFO - root - loaded best model at epoch 108\n",
            "INFO - root - Best val loss: 2.3066160678863525\n",
            "INFO - root - Best val metric: 0.1996\n",
            "INFO - root - test loss: 3.109102249145508\n",
            "INFO - root - test metric: 0.0996\n",
            "INFO - root - epoch: 0, training loss: 2.52929, val loss: 2.30797, patience: 0, training metric: 0.0068, val metric: 0.1\n",
            "INFO - root - epoch: 1, training loss: 2.39163, val loss: 2.30787, patience: 1, training metric: 0.046, val metric: 0.1\n",
            "INFO - root - epoch: 2, training loss: 2.24875, val loss: 2.30778, patience: 2, training metric: 0.1264, val metric: 0.1\n",
            "INFO - root - epoch: 3, training loss: 2.13251, val loss: 2.30796, patience: 3, training metric: 0.1968, val metric: 0.1\n",
            "INFO - root - epoch: 4, training loss: 2.06016, val loss: 2.30855, patience: 4, training metric: 0.2048, val metric: 0.1\n",
            "INFO - root - epoch: 5, training loss: 2.00636, val loss: 2.30962, patience: 5, training metric: 0.2684, val metric: 0.1\n",
            "INFO - root - epoch: 6, training loss: 1.94482, val loss: 2.311, patience: 6, training metric: 0.3768, val metric: 0.1\n",
            "INFO - root - epoch: 7, training loss: 1.9022, val loss: 2.31259, patience: 7, training metric: 0.4344, val metric: 0.1\n",
            "INFO - root - epoch: 8, training loss: 1.8689, val loss: 2.31437, patience: 8, training metric: 0.4816, val metric: 0.1\n",
            "INFO - root - epoch: 9, training loss: 1.82735, val loss: 2.31624, patience: 9, training metric: 0.532, val metric: 0.1\n",
            "INFO - root - epoch: 10, training loss: 1.79985, val loss: 2.31825, patience: 10, training metric: 0.5932, val metric: 0.1\n",
            "INFO - root - epoch: 11, training loss: 1.76105, val loss: 2.3204, patience: 11, training metric: 0.6428, val metric: 0.1\n",
            "INFO - root - epoch: 12, training loss: 1.72949, val loss: 2.32268, patience: 12, training metric: 0.67, val metric: 0.1\n",
            "INFO - root - epoch: 13, training loss: 1.70011, val loss: 2.32488, patience: 13, training metric: 0.6816, val metric: 0.1\n",
            "INFO - root - epoch: 14, training loss: 1.67455, val loss: 2.32714, patience: 14, training metric: 0.7136, val metric: 0.1\n",
            "INFO - root - epoch: 15, training loss: 1.63358, val loss: 2.32948, patience: 15, training metric: 0.7468, val metric: 0.1\n",
            "INFO - root - epoch: 16, training loss: 1.60638, val loss: 2.33199, patience: 16, training metric: 0.7604, val metric: 0.1\n",
            "INFO - root - epoch: 17, training loss: 1.57069, val loss: 2.33425, patience: 17, training metric: 0.764, val metric: 0.1\n",
            "INFO - root - epoch: 18, training loss: 1.54151, val loss: 2.33641, patience: 18, training metric: 0.782, val metric: 0.1\n",
            "INFO - root - epoch: 19, training loss: 1.5207, val loss: 2.33788, patience: 19, training metric: 0.8092, val metric: 0.1\n",
            "INFO - root - epoch: 20, training loss: 1.49288, val loss: 2.33897, patience: 20, training metric: 0.818, val metric: 0.1\n",
            "INFO - root - epoch: 21, training loss: 1.45936, val loss: 2.34034, patience: 21, training metric: 0.8432, val metric: 0.1\n",
            "INFO - root - epoch: 22, training loss: 1.4272, val loss: 2.34143, patience: 22, training metric: 0.8512, val metric: 0.1\n",
            "INFO - root - epoch: 23, training loss: 1.39425, val loss: 2.34253, patience: 23, training metric: 0.866, val metric: 0.1\n",
            "INFO - root - epoch: 24, training loss: 1.37519, val loss: 2.34417, patience: 24, training metric: 0.8624, val metric: 0.1\n",
            "INFO - root - epoch: 25, training loss: 1.34597, val loss: 2.34591, patience: 25, training metric: 0.8928, val metric: 0.1\n",
            "INFO - root - epoch: 26, training loss: 1.32304, val loss: 2.34892, patience: 26, training metric: 0.9192, val metric: 0.1\n",
            "INFO - root - epoch: 27, training loss: 1.29732, val loss: 2.35306, patience: 27, training metric: 0.9136, val metric: 0.1\n",
            "INFO - root - epoch: 28, training loss: 1.26523, val loss: 2.35861, patience: 28, training metric: 0.9352, val metric: 0.1\n",
            "INFO - root - epoch: 29, training loss: 1.24556, val loss: 2.36638, patience: 29, training metric: 0.938, val metric: 0.1\n",
            "INFO - root - epoch: 30, training loss: 1.21547, val loss: 2.37855, patience: 30, training metric: 0.9556, val metric: 0.1\n",
            "INFO - root - epoch: 31, training loss: 1.2008, val loss: 2.39336, patience: 31, training metric: 0.9476, val metric: 0.1\n",
            "INFO - root - epoch: 32, training loss: 1.18477, val loss: 2.40967, patience: 0, training metric: 0.9408, val metric: 0.1004\n",
            "INFO - root - epoch: 33, training loss: 1.16073, val loss: 2.42986, patience: 1, training metric: 0.9504, val metric: 0.1\n",
            "INFO - root - epoch: 34, training loss: 1.13469, val loss: 2.45199, patience: 2, training metric: 0.9648, val metric: 0.1\n",
            "INFO - root - epoch: 35, training loss: 1.11185, val loss: 2.47694, patience: 3, training metric: 0.9684, val metric: 0.1\n",
            "INFO - root - epoch: 36, training loss: 1.09614, val loss: 2.50713, patience: 4, training metric: 0.9668, val metric: 0.1\n",
            "INFO - root - epoch: 37, training loss: 1.07587, val loss: 2.5396, patience: 5, training metric: 0.9656, val metric: 0.1\n",
            "INFO - root - epoch: 38, training loss: 1.05305, val loss: 2.57686, patience: 6, training metric: 0.9832, val metric: 0.1\n",
            "INFO - root - epoch: 39, training loss: 1.03653, val loss: 2.62328, patience: 7, training metric: 0.9856, val metric: 0.1\n",
            "INFO - root - epoch: 40, training loss: 1.01201, val loss: 2.67346, patience: 8, training metric: 0.9884, val metric: 0.1\n",
            "INFO - root - epoch: 41, training loss: 0.99477, val loss: 2.70813, patience: 9, training metric: 0.98, val metric: 0.1\n",
            "INFO - root - epoch: 42, training loss: 0.97359, val loss: 2.7364, patience: 10, training metric: 0.9852, val metric: 0.1\n",
            "INFO - root - epoch: 43, training loss: 0.96768, val loss: 2.75445, patience: 11, training metric: 0.9848, val metric: 0.1\n",
            "INFO - root - epoch: 44, training loss: 0.94221, val loss: 2.77957, patience: 12, training metric: 0.9908, val metric: 0.1\n",
            "INFO - root - epoch: 45, training loss: 0.92431, val loss: 2.81784, patience: 13, training metric: 0.9916, val metric: 0.1\n",
            "INFO - root - epoch: 46, training loss: 0.90934, val loss: 2.85889, patience: 14, training metric: 0.9948, val metric: 0.1\n",
            "INFO - root - epoch: 47, training loss: 0.89367, val loss: 2.90192, patience: 15, training metric: 0.9952, val metric: 0.1\n",
            "INFO - root - epoch: 48, training loss: 0.87343, val loss: 2.94186, patience: 16, training metric: 0.9952, val metric: 0.1\n",
            "INFO - root - epoch: 49, training loss: 0.8573, val loss: 2.97459, patience: 17, training metric: 0.9968, val metric: 0.1\n",
            "INFO - root - epoch: 50, training loss: 0.84078, val loss: 3.00565, patience: 18, training metric: 0.9964, val metric: 0.1\n",
            "INFO - root - epoch: 51, training loss: 0.82638, val loss: 3.04114, patience: 19, training metric: 0.994, val metric: 0.1\n",
            "INFO - root - epoch: 52, training loss: 0.80826, val loss: 3.09551, patience: 20, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 53, training loss: 0.78491, val loss: 3.15302, patience: 21, training metric: 0.9988, val metric: 0.1\n",
            "INFO - root - epoch: 54, training loss: 0.7702, val loss: 3.19193, patience: 22, training metric: 0.9984, val metric: 0.1\n",
            "INFO - root - epoch: 55, training loss: 0.75349, val loss: 3.21153, patience: 23, training metric: 0.9988, val metric: 0.1\n",
            "INFO - root - epoch: 56, training loss: 0.73916, val loss: 3.24916, patience: 24, training metric: 0.9996, val metric: 0.1\n",
            "INFO - root - epoch: 57, training loss: 0.72791, val loss: 3.29675, patience: 25, training metric: 0.998, val metric: 0.1\n",
            "INFO - root - epoch: 58, training loss: 0.70954, val loss: 3.37287, patience: 0, training metric: 1.0, val metric: 0.1268\n",
            "INFO - root - epoch: 59, training loss: 0.69387, val loss: 3.43125, patience: 0, training metric: 1.0, val metric: 0.1924\n",
            "INFO - root - epoch: 60, training loss: 0.67848, val loss: 3.45686, patience: 1, training metric: 0.9996, val metric: 0.1764\n",
            "INFO - root - epoch: 61, training loss: 0.6664, val loss: 3.43146, patience: 0, training metric: 0.9992, val metric: 0.2028\n",
            "INFO - root - epoch: 62, training loss: 0.6496, val loss: 3.4044, patience: 1, training metric: 1.0, val metric: 0.1224\n",
            "INFO - root - epoch: 63, training loss: 0.63219, val loss: 3.46761, patience: 2, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 64, training loss: 0.61665, val loss: 3.50707, patience: 3, training metric: 0.9996, val metric: 0.1004\n",
            "INFO - root - epoch: 65, training loss: 0.60307, val loss: 4.106, patience: 4, training metric: 1.0, val metric: 0.0728\n",
            "INFO - root - epoch: 66, training loss: 0.58659, val loss: 4.10527, patience: 5, training metric: 1.0, val metric: 0.0944\n",
            "INFO - root - epoch: 67, training loss: 0.57159, val loss: 3.9259, patience: 6, training metric: 1.0, val metric: 0.1288\n",
            "INFO - root - epoch: 68, training loss: 0.56539, val loss: 3.81215, patience: 7, training metric: 1.0, val metric: 0.146\n",
            "INFO - root - epoch: 69, training loss: 0.55278, val loss: 4.38352, patience: 8, training metric: 0.9996, val metric: 0.0856\n",
            "INFO - root - epoch: 70, training loss: 0.53667, val loss: 4.73402, patience: 9, training metric: 1.0, val metric: 0.0808\n",
            "INFO - root - epoch: 71, training loss: 0.52327, val loss: 4.17047, patience: 10, training metric: 1.0, val metric: 0.0096\n",
            "INFO - root - epoch: 72, training loss: 0.51132, val loss: 3.41663, patience: 0, training metric: 1.0, val metric: 0.2516\n",
            "INFO - root - epoch: 73, training loss: 0.5001, val loss: 3.40918, patience: 1, training metric: 1.0, val metric: 0.2012\n",
            "INFO - root - epoch: 74, training loss: 0.4889, val loss: 3.4512, patience: 2, training metric: 1.0, val metric: 0.1996\n",
            "INFO - root - epoch: 75, training loss: 0.47882, val loss: 5.50278, patience: 3, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 76, training loss: 0.46379, val loss: 7.23386, patience: 4, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 77, training loss: 0.45122, val loss: 6.97348, patience: 5, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 78, training loss: 0.44069, val loss: 7.03236, patience: 6, training metric: 1.0, val metric: 0.192\n",
            "INFO - root - epoch: 79, training loss: 0.4313, val loss: 8.0107, patience: 7, training metric: 1.0, val metric: 0.2\n",
            "INFO - root - epoch: 80, training loss: 0.42176, val loss: 7.82645, patience: 8, training metric: 1.0, val metric: 0.2\n",
            "INFO - root - epoch: 81, training loss: 0.41316, val loss: 7.53302, patience: 9, training metric: 1.0, val metric: 0.1468\n",
            "INFO - root - epoch: 82, training loss: 0.39927, val loss: 7.24209, patience: 10, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 83, training loss: 0.39016, val loss: 6.80002, patience: 11, training metric: 1.0, val metric: 0.1472\n",
            "INFO - root - epoch: 84, training loss: 0.38229, val loss: 7.23459, patience: 12, training metric: 1.0, val metric: 0.1528\n",
            "INFO - root - epoch: 85, training loss: 0.3713, val loss: 8.66822, patience: 13, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 86, training loss: 0.36268, val loss: 9.83272, patience: 14, training metric: 1.0, val metric: 0.1412\n",
            "INFO - root - epoch: 87, training loss: 0.35499, val loss: 8.61297, patience: 15, training metric: 1.0, val metric: 0.1636\n",
            "INFO - root - epoch: 88, training loss: 0.3458, val loss: 8.19629, patience: 16, training metric: 1.0, val metric: 0.186\n",
            "INFO - root - epoch: 89, training loss: 0.33936, val loss: 8.30585, patience: 17, training metric: 1.0, val metric: 0.1408\n",
            "INFO - root - epoch: 90, training loss: 0.33163, val loss: 7.74115, patience: 18, training metric: 1.0, val metric: 0.1736\n",
            "INFO - root - epoch: 91, training loss: 0.32341, val loss: 7.38947, patience: 19, training metric: 1.0, val metric: 0.1996\n",
            "INFO - root - epoch: 92, training loss: 0.31636, val loss: 7.98463, patience: 20, training metric: 1.0, val metric: 0.1376\n",
            "INFO - root - epoch: 93, training loss: 0.30851, val loss: 9.37226, patience: 21, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 94, training loss: 0.30125, val loss: 8.40643, patience: 22, training metric: 1.0, val metric: 0.1252\n",
            "INFO - root - epoch: 95, training loss: 0.29263, val loss: 6.88122, patience: 23, training metric: 1.0, val metric: 0.1772\n",
            "INFO - root - epoch: 96, training loss: 0.28709, val loss: 6.32217, patience: 24, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 97, training loss: 0.28243, val loss: 5.92134, patience: 25, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 98, training loss: 0.2757, val loss: 5.55199, patience: 26, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 99, training loss: 0.27104, val loss: 5.82649, patience: 27, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 100, training loss: 0.26561, val loss: 5.0244, patience: 28, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 101, training loss: 0.26054, val loss: 3.61686, patience: 29, training metric: 1.0, val metric: 0.1516\n",
            "INFO - root - epoch: 102, training loss: 0.25259, val loss: 2.91991, patience: 30, training metric: 1.0, val metric: 0.126\n",
            "INFO - root - epoch: 103, training loss: 0.247, val loss: 2.9195, patience: 31, training metric: 1.0, val metric: 0.2492\n",
            "INFO - root - epoch: 104, training loss: 0.24171, val loss: 2.99656, patience: 0, training metric: 1.0, val metric: 0.346\n",
            "INFO - root - epoch: 105, training loss: 0.23583, val loss: 3.19081, patience: 1, training metric: 1.0, val metric: 0.1908\n",
            "INFO - root - epoch: 106, training loss: 0.23105, val loss: 3.81989, patience: 2, training metric: 1.0, val metric: 0.0772\n",
            "INFO - root - epoch: 107, training loss: 0.22772, val loss: 4.58916, patience: 3, training metric: 1.0, val metric: 0.0748\n",
            "INFO - root - epoch: 108, training loss: 0.22253, val loss: 4.95359, patience: 4, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 109, training loss: 0.21729, val loss: 4.76706, patience: 5, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 110, training loss: 0.2143, val loss: 4.87593, patience: 6, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 111, training loss: 0.20799, val loss: 4.56659, patience: 7, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 112, training loss: 0.20363, val loss: 4.13966, patience: 8, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 113, training loss: 0.20063, val loss: 3.84931, patience: 9, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 114, training loss: 0.19538, val loss: 3.73957, patience: 10, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 115, training loss: 0.19392, val loss: 3.90125, patience: 11, training metric: 1.0, val metric: 0.1388\n",
            "INFO - root - epoch: 116, training loss: 0.18935, val loss: 4.59538, patience: 12, training metric: 1.0, val metric: 0.1528\n",
            "INFO - root - epoch: 117, training loss: 0.18443, val loss: 5.45087, patience: 13, training metric: 1.0, val metric: 0.1372\n",
            "INFO - root - epoch: 118, training loss: 0.18104, val loss: 5.83875, patience: 14, training metric: 1.0, val metric: 0.1256\n",
            "INFO - root - epoch: 119, training loss: 0.17817, val loss: 5.2668, patience: 15, training metric: 1.0, val metric: 0.1808\n",
            "INFO - root - epoch: 120, training loss: 0.17414, val loss: 4.0938, patience: 16, training metric: 1.0, val metric: 0.1912\n",
            "INFO - root - epoch: 121, training loss: 0.17191, val loss: 3.46528, patience: 17, training metric: 1.0, val metric: 0.1668\n",
            "INFO - root - epoch: 122, training loss: 0.16822, val loss: 3.38065, patience: 18, training metric: 1.0, val metric: 0.038\n",
            "INFO - root - epoch: 123, training loss: 0.16449, val loss: 3.53147, patience: 19, training metric: 1.0, val metric: 0.0952\n",
            "INFO - root - epoch: 124, training loss: 0.16198, val loss: 3.81934, patience: 20, training metric: 1.0, val metric: 0.0888\n",
            "INFO - root - epoch: 125, training loss: 0.15877, val loss: 4.09923, patience: 21, training metric: 1.0, val metric: 0.0588\n",
            "INFO - root - epoch: 126, training loss: 0.15626, val loss: 4.62291, patience: 22, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 127, training loss: 0.15347, val loss: 5.52143, patience: 23, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 128, training loss: 0.15067, val loss: 5.98498, patience: 24, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 129, training loss: 0.14735, val loss: 5.93542, patience: 25, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 130, training loss: 0.14511, val loss: 5.24174, patience: 26, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 131, training loss: 0.14198, val loss: 4.59517, patience: 27, training metric: 1.0, val metric: 0.1492\n",
            "INFO - root - epoch: 132, training loss: 0.14035, val loss: 3.77602, patience: 28, training metric: 1.0, val metric: 0.1888\n",
            "INFO - root - epoch: 133, training loss: 0.13751, val loss: 3.3433, patience: 29, training metric: 1.0, val metric: 0.1096\n",
            "INFO - root - epoch: 134, training loss: 0.13592, val loss: 3.20442, patience: 30, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 135, training loss: 0.1339, val loss: 3.27999, patience: 31, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 136, training loss: 0.13161, val loss: 3.48629, patience: 32, training metric: 1.0, val metric: 0.0836\n",
            "INFO - root - epoch: 137, training loss: 0.1285, val loss: 3.61804, patience: 33, training metric: 1.0, val metric: 0.066\n",
            "INFO - root - epoch: 138, training loss: 0.1271, val loss: 3.61948, patience: 34, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 139, training loss: 0.12526, val loss: 3.53895, patience: 35, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 140, training loss: 0.12325, val loss: 3.36038, patience: 36, training metric: 1.0, val metric: 0.1604\n",
            "INFO - root - epoch: 141, training loss: 0.12084, val loss: 3.19939, patience: 37, training metric: 1.0, val metric: 0.1968\n",
            "INFO - root - epoch: 142, training loss: 0.11893, val loss: 3.07333, patience: 38, training metric: 1.0, val metric: 0.188\n",
            "INFO - root - epoch: 143, training loss: 0.11823, val loss: 2.98092, patience: 39, training metric: 1.0, val metric: 0.1864\n",
            "INFO - root - epoch: 144, training loss: 0.11554, val loss: 2.86355, patience: 40, training metric: 1.0, val metric: 0.2372\n",
            "INFO - root - epoch: 145, training loss: 0.11328, val loss: 2.74866, patience: 41, training metric: 1.0, val metric: 0.1524\n",
            "INFO - root - epoch: 146, training loss: 0.11278, val loss: 2.71094, patience: 42, training metric: 1.0, val metric: 0.092\n",
            "INFO - root - epoch: 147, training loss: 0.1107, val loss: 2.67042, patience: 43, training metric: 1.0, val metric: 0.0864\n",
            "INFO - root - epoch: 148, training loss: 0.10909, val loss: 2.67129, patience: 44, training metric: 1.0, val metric: 0.1628\n",
            "INFO - root - epoch: 149, training loss: 0.10729, val loss: 2.6828, patience: 45, training metric: 1.0, val metric: 0.1896\n",
            "INFO - root - epoch: 150, training loss: 0.10581, val loss: 2.65858, patience: 46, training metric: 1.0, val metric: 0.1616\n",
            "INFO - root - epoch: 151, training loss: 0.10391, val loss: 2.65005, patience: 47, training metric: 1.0, val metric: 0.1228\n",
            "INFO - root - epoch: 152, training loss: 0.1021, val loss: 2.64976, patience: 48, training metric: 1.0, val metric: 0.1616\n",
            "INFO - root - epoch: 153, training loss: 0.10109, val loss: 2.64797, patience: 49, training metric: 1.0, val metric: 0.1712\n",
            "INFO - root - epoch: 154, training loss: 0.09959, val loss: 2.62059, patience: 50, training metric: 1.0, val metric: 0.132\n",
            "INFO - root - epoch: 155, training loss: 0.09889, val loss: 2.59524, patience: 51, training metric: 1.0, val metric: 0.1208\n",
            "INFO - root - epoch: 156, training loss: 0.09713, val loss: 2.59181, patience: 52, training metric: 1.0, val metric: 0.1376\n",
            "INFO - root - epoch: 157, training loss: 0.09641, val loss: 2.63264, patience: 53, training metric: 1.0, val metric: 0.1356\n",
            "INFO - root - epoch: 158, training loss: 0.09518, val loss: 2.6747, patience: 54, training metric: 1.0, val metric: 0.1376\n",
            "INFO - root - epoch: 159, training loss: 0.09452, val loss: 2.72467, patience: 55, training metric: 1.0, val metric: 0.1372\n",
            "INFO - root - epoch: 160, training loss: 0.09438, val loss: 2.7476, patience: 56, training metric: 1.0, val metric: 0.1188\n",
            "INFO - root - epoch: 161, training loss: 0.09319, val loss: 2.76969, patience: 57, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 162, training loss: 0.09344, val loss: 2.80018, patience: 58, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 163, training loss: 0.09253, val loss: 2.80982, patience: 59, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 164, training loss: 0.09197, val loss: 2.84045, patience: 60, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 165, training loss: 0.09103, val loss: 2.86206, patience: 61, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 166, training loss: 0.09028, val loss: 2.87802, patience: 62, training metric: 1.0, val metric: 0.1116\n",
            "INFO - root - epoch: 167, training loss: 0.09008, val loss: 2.87269, patience: 63, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 168, training loss: 0.08924, val loss: 2.85654, patience: 64, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 169, training loss: 0.08949, val loss: 2.81525, patience: 65, training metric: 1.0, val metric: 0.0704\n",
            "INFO - root - epoch: 170, training loss: 0.08801, val loss: 2.78243, patience: 66, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 171, training loss: 0.08798, val loss: 2.76987, patience: 67, training metric: 1.0, val metric: 0.1136\n",
            "INFO - root - epoch: 172, training loss: 0.08655, val loss: 2.76487, patience: 68, training metric: 1.0, val metric: 0.1072\n",
            "INFO - root - epoch: 173, training loss: 0.08694, val loss: 2.76145, patience: 69, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 174, training loss: 0.08612, val loss: 2.77917, patience: 70, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 175, training loss: 0.08484, val loss: 2.80133, patience: 71, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 176, training loss: 0.08485, val loss: 2.82367, patience: 72, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 177, training loss: 0.08415, val loss: 2.83661, patience: 73, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 178, training loss: 0.08385, val loss: 2.82767, patience: 74, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 179, training loss: 0.08317, val loss: 2.82418, patience: 75, training metric: 1.0, val metric: 0.1048\n",
            "INFO - root - epoch: 180, training loss: 0.08279, val loss: 2.79754, patience: 76, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 181, training loss: 0.08229, val loss: 2.78143, patience: 77, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 182, training loss: 0.08211, val loss: 2.76485, patience: 78, training metric: 1.0, val metric: 0.0768\n",
            "INFO - root - epoch: 183, training loss: 0.0817, val loss: 2.73961, patience: 79, training metric: 1.0, val metric: 0.0584\n",
            "INFO - root - epoch: 184, training loss: 0.08045, val loss: 2.71034, patience: 80, training metric: 1.0, val metric: 0.0692\n",
            "INFO - root - epoch: 185, training loss: 0.08004, val loss: 2.70346, patience: 81, training metric: 1.0, val metric: 0.0904\n",
            "INFO - root - epoch: 186, training loss: 0.07951, val loss: 2.6793, patience: 82, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 187, training loss: 0.07928, val loss: 2.66071, patience: 83, training metric: 1.0, val metric: 0.112\n",
            "INFO - root - epoch: 188, training loss: 0.07873, val loss: 2.65929, patience: 84, training metric: 1.0, val metric: 0.1232\n",
            "INFO - root - epoch: 189, training loss: 0.07826, val loss: 2.65396, patience: 85, training metric: 1.0, val metric: 0.1716\n",
            "INFO - root - epoch: 190, training loss: 0.07739, val loss: 2.68271, patience: 86, training metric: 1.0, val metric: 0.1904\n",
            "INFO - root - epoch: 191, training loss: 0.07749, val loss: 2.73075, patience: 87, training metric: 1.0, val metric: 0.1568\n",
            "INFO - root - epoch: 192, training loss: 0.07752, val loss: 2.77948, patience: 88, training metric: 1.0, val metric: 0.1136\n",
            "INFO - root - epoch: 193, training loss: 0.07663, val loss: 2.84194, patience: 89, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 194, training loss: 0.07614, val loss: 2.88718, patience: 90, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 195, training loss: 0.0754, val loss: 2.91239, patience: 91, training metric: 1.0, val metric: 0.1056\n",
            "INFO - root - epoch: 196, training loss: 0.07493, val loss: 2.93443, patience: 92, training metric: 1.0, val metric: 0.1132\n",
            "INFO - root - epoch: 197, training loss: 0.07458, val loss: 2.95561, patience: 93, training metric: 1.0, val metric: 0.1224\n",
            "INFO - root - epoch: 198, training loss: 0.07419, val loss: 2.95349, patience: 94, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 199, training loss: 0.07361, val loss: 2.93754, patience: 95, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 200, training loss: 0.07346, val loss: 2.92358, patience: 96, training metric: 1.0, val metric: 0.0936\n",
            "INFO - root - epoch: 201, training loss: 0.07289, val loss: 2.93766, patience: 97, training metric: 1.0, val metric: 0.088\n",
            "INFO - root - epoch: 202, training loss: 0.07263, val loss: 2.94254, patience: 98, training metric: 1.0, val metric: 0.0732\n",
            "INFO - root - epoch: 203, training loss: 0.0717, val loss: 2.94597, patience: 99, training metric: 1.0, val metric: 0.0688\n",
            "INFO - root - epoch: 204, training loss: 0.07162, val loss: 2.95803, patience: 100, training metric: 1.0, val metric: 0.0676\n",
            "INFO - root - epoch: 205, training loss: 0.07118, val loss: 2.96232, patience: 101, training metric: 1.0, val metric: 0.0724\n",
            "INFO - root - epoch: 206, training loss: 0.07106, val loss: 2.96266, patience: 102, training metric: 1.0, val metric: 0.064\n",
            "INFO - root - epoch: 207, training loss: 0.0705, val loss: 2.9734, patience: 103, training metric: 1.0, val metric: 0.0592\n",
            "INFO - root - epoch: 208, training loss: 0.07016, val loss: 2.97705, patience: 104, training metric: 1.0, val metric: 0.0604\n",
            "INFO - root - epoch: 209, training loss: 0.06975, val loss: 2.98037, patience: 105, training metric: 1.0, val metric: 0.0628\n",
            "INFO - root - epoch: 210, training loss: 0.06968, val loss: 2.98351, patience: 106, training metric: 1.0, val metric: 0.062\n",
            "INFO - root - epoch: 211, training loss: 0.06926, val loss: 2.96877, patience: 107, training metric: 1.0, val metric: 0.0624\n",
            "INFO - root - epoch: 212, training loss: 0.06906, val loss: 2.98846, patience: 108, training metric: 1.0, val metric: 0.052\n",
            "INFO - root - epoch: 213, training loss: 0.06894, val loss: 2.96442, patience: 109, training metric: 1.0, val metric: 0.0664\n",
            "INFO - root - epoch: 214, training loss: 0.0687, val loss: 2.97248, patience: 110, training metric: 1.0, val metric: 0.0592\n",
            "INFO - root - epoch: 215, training loss: 0.06844, val loss: 2.95846, patience: 111, training metric: 1.0, val metric: 0.0672\n",
            "INFO - root - epoch: 216, training loss: 0.06847, val loss: 2.95893, patience: 112, training metric: 1.0, val metric: 0.0792\n",
            "INFO - root - epoch: 217, training loss: 0.06779, val loss: 2.93805, patience: 113, training metric: 1.0, val metric: 0.0756\n",
            "INFO - root - epoch: 218, training loss: 0.06758, val loss: 2.94125, patience: 114, training metric: 1.0, val metric: 0.0816\n",
            "INFO - root - epoch: 219, training loss: 0.06718, val loss: 2.9317, patience: 115, training metric: 1.0, val metric: 0.0844\n",
            "INFO - root - epoch: 220, training loss: 0.06699, val loss: 2.91899, patience: 116, training metric: 1.0, val metric: 0.0912\n",
            "INFO - root - epoch: 221, training loss: 0.06714, val loss: 2.9203, patience: 117, training metric: 1.0, val metric: 0.1076\n",
            "INFO - root - epoch: 222, training loss: 0.06701, val loss: 2.92196, patience: 118, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 223, training loss: 0.06706, val loss: 2.92229, patience: 119, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 224, training loss: 0.06683, val loss: 2.93448, patience: 120, training metric: 1.0, val metric: 0.08\n",
            "INFO - root - epoch: 225, training loss: 0.06627, val loss: 2.91955, patience: 121, training metric: 1.0, val metric: 0.072\n",
            "INFO - root - epoch: 226, training loss: 0.06611, val loss: 2.9282, patience: 122, training metric: 1.0, val metric: 0.0736\n",
            "INFO - root - epoch: 227, training loss: 0.06605, val loss: 2.93425, patience: 123, training metric: 1.0, val metric: 0.08\n",
            "INFO - root - epoch: 228, training loss: 0.06566, val loss: 2.94295, patience: 124, training metric: 1.0, val metric: 0.0824\n",
            "INFO - root - epoch: 229, training loss: 0.06588, val loss: 2.93832, patience: 125, training metric: 1.0, val metric: 0.0892\n",
            "INFO - root - epoch: 230, training loss: 0.06562, val loss: 2.94388, patience: 126, training metric: 1.0, val metric: 0.0972\n",
            "INFO - root - epoch: 231, training loss: 0.06548, val loss: 2.94862, patience: 127, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 232, training loss: 0.06519, val loss: 2.94602, patience: 128, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 233, training loss: 0.06471, val loss: 2.95447, patience: 129, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 234, training loss: 0.06481, val loss: 2.94916, patience: 130, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 235, training loss: 0.06476, val loss: 2.96015, patience: 131, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 236, training loss: 0.0646, val loss: 2.96141, patience: 132, training metric: 1.0, val metric: 0.1168\n",
            "INFO - root - epoch: 237, training loss: 0.06427, val loss: 2.96638, patience: 133, training metric: 1.0, val metric: 0.1248\n",
            "INFO - root - epoch: 238, training loss: 0.0639, val loss: 2.96307, patience: 134, training metric: 1.0, val metric: 0.1316\n",
            "INFO - root - epoch: 239, training loss: 0.06364, val loss: 2.95543, patience: 135, training metric: 1.0, val metric: 0.1408\n",
            "INFO - root - epoch: 240, training loss: 0.06391, val loss: 2.9475, patience: 136, training metric: 1.0, val metric: 0.1436\n",
            "INFO - root - epoch: 241, training loss: 0.06361, val loss: 2.94752, patience: 137, training metric: 1.0, val metric: 0.1384\n",
            "INFO - root - epoch: 242, training loss: 0.06353, val loss: 2.93823, patience: 138, training metric: 1.0, val metric: 0.1208\n",
            "INFO - root - epoch: 243, training loss: 0.06333, val loss: 2.94062, patience: 139, training metric: 1.0, val metric: 0.108\n",
            "INFO - root - epoch: 244, training loss: 0.06271, val loss: 2.92838, patience: 140, training metric: 1.0, val metric: 0.0968\n",
            "INFO - root - epoch: 245, training loss: 0.06263, val loss: 2.92171, patience: 141, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 246, training loss: 0.06255, val loss: 2.91699, patience: 142, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 247, training loss: 0.06232, val loss: 2.9132, patience: 143, training metric: 1.0, val metric: 0.0976\n",
            "INFO - root - epoch: 248, training loss: 0.06209, val loss: 2.91207, patience: 144, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 249, training loss: 0.06186, val loss: 2.90572, patience: 145, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 250, training loss: 0.06158, val loss: 2.91436, patience: 146, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 251, training loss: 0.06179, val loss: 2.92365, patience: 147, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 252, training loss: 0.06151, val loss: 2.93281, patience: 148, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 253, training loss: 0.061, val loss: 2.93722, patience: 149, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 254, training loss: 0.06113, val loss: 2.95239, patience: 150, training metric: 1.0, val metric: 0.1068\n",
            "INFO - root - epoch: 255, training loss: 0.06075, val loss: 2.95905, patience: 151, training metric: 1.0, val metric: 0.1104\n",
            "INFO - root - epoch: 256, training loss: 0.06069, val loss: 2.99218, patience: 152, training metric: 1.0, val metric: 0.1128\n",
            "INFO - root - epoch: 257, training loss: 0.06045, val loss: 2.98762, patience: 153, training metric: 1.0, val metric: 0.1152\n",
            "INFO - root - epoch: 258, training loss: 0.06042, val loss: 2.99638, patience: 154, training metric: 1.0, val metric: 0.1172\n",
            "INFO - root - epoch: 259, training loss: 0.06028, val loss: 2.98305, patience: 155, training metric: 1.0, val metric: 0.1168\n",
            "INFO - root - epoch: 260, training loss: 0.06024, val loss: 2.99187, patience: 156, training metric: 1.0, val metric: 0.118\n",
            "INFO - root - epoch: 261, training loss: 0.06044, val loss: 2.9966, patience: 157, training metric: 1.0, val metric: 0.1092\n",
            "INFO - root - epoch: 262, training loss: 0.06011, val loss: 2.98598, patience: 158, training metric: 1.0, val metric: 0.1064\n",
            "INFO - root - epoch: 263, training loss: 0.06006, val loss: 2.97807, patience: 159, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 264, training loss: 0.06016, val loss: 2.97027, patience: 160, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 265, training loss: 0.06009, val loss: 2.95478, patience: 161, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 266, training loss: 0.05982, val loss: 2.94807, patience: 162, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 267, training loss: 0.05947, val loss: 2.94285, patience: 163, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 268, training loss: 0.05967, val loss: 2.93097, patience: 164, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 269, training loss: 0.05986, val loss: 2.92709, patience: 165, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 270, training loss: 0.0593, val loss: 2.91421, patience: 166, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 271, training loss: 0.05936, val loss: 2.91878, patience: 167, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 272, training loss: 0.05919, val loss: 2.92817, patience: 168, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 273, training loss: 0.05889, val loss: 2.92311, patience: 169, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 274, training loss: 0.05869, val loss: 2.92694, patience: 170, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 275, training loss: 0.05901, val loss: 2.92468, patience: 171, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 276, training loss: 0.05866, val loss: 2.93081, patience: 172, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 277, training loss: 0.05881, val loss: 2.91913, patience: 173, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 278, training loss: 0.05867, val loss: 2.91952, patience: 174, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 279, training loss: 0.05855, val loss: 2.91837, patience: 175, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 280, training loss: 0.05845, val loss: 2.92278, patience: 176, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 281, training loss: 0.05841, val loss: 2.93104, patience: 177, training metric: 1.0, val metric: 0.1044\n",
            "INFO - root - epoch: 282, training loss: 0.0584, val loss: 2.93765, patience: 178, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 283, training loss: 0.05827, val loss: 2.93264, patience: 179, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 284, training loss: 0.05825, val loss: 2.94818, patience: 180, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 285, training loss: 0.05803, val loss: 2.94977, patience: 181, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 286, training loss: 0.0579, val loss: 2.93388, patience: 182, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 287, training loss: 0.05794, val loss: 2.94888, patience: 183, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 288, training loss: 0.05797, val loss: 2.9501, patience: 184, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 289, training loss: 0.05775, val loss: 2.94201, patience: 185, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 290, training loss: 0.05778, val loss: 2.93714, patience: 186, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 291, training loss: 0.05744, val loss: 2.93331, patience: 187, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 292, training loss: 0.0575, val loss: 2.93025, patience: 188, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 293, training loss: 0.05759, val loss: 2.91656, patience: 189, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 294, training loss: 0.0573, val loss: 2.91944, patience: 190, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 295, training loss: 0.057, val loss: 2.90864, patience: 191, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 296, training loss: 0.05713, val loss: 2.91008, patience: 192, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 297, training loss: 0.05729, val loss: 2.90737, patience: 193, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 298, training loss: 0.05711, val loss: 2.90964, patience: 194, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 299, training loss: 0.05696, val loss: 2.89235, patience: 195, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 300, training loss: 0.057, val loss: 2.89779, patience: 196, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 301, training loss: 0.0569, val loss: 2.88998, patience: 197, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 302, training loss: 0.05704, val loss: 2.88763, patience: 198, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 303, training loss: 0.05673, val loss: 2.89565, patience: 199, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 304, training loss: 0.05661, val loss: 2.89496, patience: 200, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 305, training loss: 0.05631, val loss: 2.89885, patience: 201, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 306, training loss: 0.05643, val loss: 2.89966, patience: 202, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 307, training loss: 0.05622, val loss: 2.90002, patience: 203, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 308, training loss: 0.056, val loss: 2.90792, patience: 204, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 309, training loss: 0.05596, val loss: 2.91386, patience: 205, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 310, training loss: 0.05585, val loss: 2.92474, patience: 206, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 311, training loss: 0.05602, val loss: 2.9267, patience: 207, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 312, training loss: 0.05616, val loss: 2.91573, patience: 208, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 313, training loss: 0.05601, val loss: 2.93444, patience: 209, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 314, training loss: 0.05589, val loss: 2.92785, patience: 210, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 315, training loss: 0.05574, val loss: 2.92986, patience: 211, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 316, training loss: 0.05582, val loss: 2.94334, patience: 212, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 317, training loss: 0.05563, val loss: 2.94234, patience: 213, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 318, training loss: 0.05574, val loss: 2.9371, patience: 214, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 319, training loss: 0.05559, val loss: 2.93735, patience: 215, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 320, training loss: 0.05549, val loss: 2.93451, patience: 216, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 321, training loss: 0.05563, val loss: 2.94682, patience: 217, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 322, training loss: 0.05547, val loss: 2.94143, patience: 218, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 323, training loss: 0.05524, val loss: 2.96032, patience: 219, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 324, training loss: 0.05536, val loss: 2.94614, patience: 220, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 325, training loss: 0.05548, val loss: 2.94437, patience: 221, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 326, training loss: 0.05533, val loss: 2.94922, patience: 222, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 327, training loss: 0.05499, val loss: 2.95491, patience: 223, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 328, training loss: 0.05515, val loss: 2.95819, patience: 224, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 329, training loss: 0.05502, val loss: 2.96525, patience: 225, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 330, training loss: 0.05497, val loss: 2.95774, patience: 226, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 331, training loss: 0.05516, val loss: 2.96388, patience: 227, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 332, training loss: 0.05502, val loss: 2.975, patience: 228, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 333, training loss: 0.05491, val loss: 2.96864, patience: 229, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 334, training loss: 0.05519, val loss: 2.97017, patience: 230, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 335, training loss: 0.05508, val loss: 2.97428, patience: 231, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 336, training loss: 0.05482, val loss: 2.97056, patience: 232, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 337, training loss: 0.05472, val loss: 2.98418, patience: 233, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 338, training loss: 0.05466, val loss: 2.98839, patience: 234, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 339, training loss: 0.05505, val loss: 2.98246, patience: 235, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 340, training loss: 0.05466, val loss: 2.98274, patience: 236, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 341, training loss: 0.05467, val loss: 2.98557, patience: 237, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 342, training loss: 0.05456, val loss: 2.98558, patience: 238, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 343, training loss: 0.05459, val loss: 2.98409, patience: 239, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 344, training loss: 0.0545, val loss: 2.99606, patience: 240, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 345, training loss: 0.05467, val loss: 2.99211, patience: 241, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 346, training loss: 0.05464, val loss: 2.97321, patience: 242, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 347, training loss: 0.05448, val loss: 2.98861, patience: 243, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 348, training loss: 0.05453, val loss: 2.98586, patience: 244, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 349, training loss: 0.05433, val loss: 2.98642, patience: 245, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 350, training loss: 0.05454, val loss: 2.98176, patience: 246, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 351, training loss: 0.05436, val loss: 2.99357, patience: 247, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 352, training loss: 0.05429, val loss: 2.98167, patience: 248, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 353, training loss: 0.05445, val loss: 2.98427, patience: 249, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 354, training loss: 0.0543, val loss: 2.98522, patience: 250, training metric: 1.0, val metric: 0.1052\n",
            "INFO - root - epoch: 355, training loss: 0.05417, val loss: 2.99307, patience: 251, training metric: 1.0, val metric: 0.1032\n",
            "INFO - root - epoch: 356, training loss: 0.0539, val loss: 2.98679, patience: 252, training metric: 1.0, val metric: 0.104\n",
            "INFO - root - epoch: 357, training loss: 0.05406, val loss: 2.98484, patience: 253, training metric: 1.0, val metric: 0.1028\n",
            "INFO - root - epoch: 358, training loss: 0.05389, val loss: 2.98174, patience: 254, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 359, training loss: 0.05385, val loss: 2.98976, patience: 255, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 360, training loss: 0.05403, val loss: 2.99268, patience: 256, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 361, training loss: 0.05393, val loss: 2.98778, patience: 257, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 362, training loss: 0.05398, val loss: 2.97823, patience: 258, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 363, training loss: 0.05391, val loss: 2.98965, patience: 259, training metric: 1.0, val metric: 0.1036\n",
            "INFO - root - epoch: 364, training loss: 0.0542, val loss: 2.98353, patience: 260, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 365, training loss: 0.05393, val loss: 2.99667, patience: 261, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 366, training loss: 0.05404, val loss: 2.9891, patience: 262, training metric: 1.0, val metric: 0.0964\n",
            "INFO - root - epoch: 367, training loss: 0.05403, val loss: 2.97918, patience: 263, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 368, training loss: 0.05418, val loss: 2.9868, patience: 264, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 369, training loss: 0.05378, val loss: 2.97499, patience: 265, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 370, training loss: 0.05369, val loss: 2.97322, patience: 266, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 371, training loss: 0.0538, val loss: 2.98112, patience: 267, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 372, training loss: 0.05381, val loss: 2.98356, patience: 268, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 373, training loss: 0.05387, val loss: 2.96497, patience: 269, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 374, training loss: 0.05397, val loss: 2.97426, patience: 270, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 375, training loss: 0.05365, val loss: 2.96879, patience: 271, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 376, training loss: 0.05363, val loss: 2.96966, patience: 272, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 377, training loss: 0.05358, val loss: 2.97631, patience: 273, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 378, training loss: 0.05359, val loss: 2.96289, patience: 274, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 379, training loss: 0.05359, val loss: 2.97243, patience: 275, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 380, training loss: 0.0536, val loss: 2.96599, patience: 276, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 381, training loss: 0.05353, val loss: 2.97893, patience: 277, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 382, training loss: 0.05364, val loss: 2.98497, patience: 278, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 383, training loss: 0.05361, val loss: 2.98957, patience: 279, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 384, training loss: 0.05354, val loss: 2.98118, patience: 280, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 385, training loss: 0.05361, val loss: 2.9826, patience: 281, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 386, training loss: 0.05358, val loss: 2.9714, patience: 282, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 387, training loss: 0.05355, val loss: 2.98813, patience: 283, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 388, training loss: 0.05347, val loss: 2.97975, patience: 284, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 389, training loss: 0.05341, val loss: 2.98005, patience: 285, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 390, training loss: 0.05325, val loss: 2.99006, patience: 286, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 391, training loss: 0.05344, val loss: 2.99482, patience: 287, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 392, training loss: 0.05329, val loss: 2.98984, patience: 288, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 393, training loss: 0.05349, val loss: 2.98688, patience: 289, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 394, training loss: 0.0534, val loss: 2.99122, patience: 290, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 395, training loss: 0.05326, val loss: 2.99652, patience: 291, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 396, training loss: 0.0531, val loss: 2.98277, patience: 292, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 397, training loss: 0.05329, val loss: 2.98149, patience: 293, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 398, training loss: 0.05336, val loss: 2.97795, patience: 294, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 399, training loss: 0.05331, val loss: 2.98774, patience: 295, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 400, training loss: 0.05347, val loss: 2.99202, patience: 296, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 401, training loss: 0.05322, val loss: 2.98308, patience: 297, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 402, training loss: 0.05329, val loss: 2.98038, patience: 298, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 403, training loss: 0.05334, val loss: 2.97928, patience: 299, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 404, training loss: 0.05328, val loss: 2.97786, patience: 300, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 405, training loss: 0.0531, val loss: 2.98285, patience: 301, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 406, training loss: 0.05306, val loss: 2.99627, patience: 302, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 407, training loss: 0.05276, val loss: 2.97511, patience: 303, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 408, training loss: 0.05312, val loss: 2.98101, patience: 304, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 409, training loss: 0.05293, val loss: 2.98859, patience: 305, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 410, training loss: 0.05301, val loss: 2.98149, patience: 306, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 411, training loss: 0.05291, val loss: 2.98439, patience: 307, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 412, training loss: 0.05299, val loss: 2.98501, patience: 308, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 413, training loss: 0.05297, val loss: 2.98537, patience: 309, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 414, training loss: 0.05281, val loss: 2.98746, patience: 310, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 415, training loss: 0.05283, val loss: 2.98716, patience: 311, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 416, training loss: 0.05287, val loss: 2.99867, patience: 312, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 417, training loss: 0.05269, val loss: 2.99195, patience: 313, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 418, training loss: 0.05285, val loss: 2.98723, patience: 314, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 419, training loss: 0.05297, val loss: 2.98842, patience: 315, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 420, training loss: 0.0527, val loss: 2.99094, patience: 316, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 421, training loss: 0.05283, val loss: 2.98792, patience: 317, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 422, training loss: 0.05274, val loss: 2.99614, patience: 318, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 423, training loss: 0.05275, val loss: 2.99744, patience: 319, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 424, training loss: 0.05283, val loss: 2.99771, patience: 320, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 425, training loss: 0.0528, val loss: 2.99956, patience: 321, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 426, training loss: 0.05272, val loss: 2.99758, patience: 322, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 427, training loss: 0.05256, val loss: 3.00017, patience: 323, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 428, training loss: 0.05262, val loss: 2.99822, patience: 324, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 429, training loss: 0.05253, val loss: 2.9968, patience: 325, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 430, training loss: 0.05281, val loss: 3.00833, patience: 326, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 431, training loss: 0.05271, val loss: 3.00319, patience: 327, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 432, training loss: 0.05256, val loss: 3.01301, patience: 328, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 433, training loss: 0.05287, val loss: 2.9892, patience: 329, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 434, training loss: 0.05252, val loss: 2.99751, patience: 330, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 435, training loss: 0.0527, val loss: 2.99974, patience: 331, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 436, training loss: 0.05257, val loss: 3.00882, patience: 332, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 437, training loss: 0.05256, val loss: 3.00047, patience: 333, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 438, training loss: 0.05267, val loss: 3.009, patience: 334, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 439, training loss: 0.05274, val loss: 3.00841, patience: 335, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 440, training loss: 0.05259, val loss: 3.00052, patience: 336, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 441, training loss: 0.05272, val loss: 3.00455, patience: 337, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 442, training loss: 0.05248, val loss: 3.00658, patience: 338, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 443, training loss: 0.05251, val loss: 2.9941, patience: 339, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 444, training loss: 0.05243, val loss: 3.00139, patience: 340, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 445, training loss: 0.05266, val loss: 3.01076, patience: 341, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 446, training loss: 0.05264, val loss: 3.00754, patience: 342, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 447, training loss: 0.05269, val loss: 3.01727, patience: 343, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 448, training loss: 0.05245, val loss: 3.00259, patience: 344, training metric: 1.0, val metric: 0.1024\n",
            "INFO - root - epoch: 449, training loss: 0.05259, val loss: 3.00775, patience: 345, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 450, training loss: 0.05255, val loss: 3.00188, patience: 346, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 451, training loss: 0.05254, val loss: 3.00939, patience: 347, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 452, training loss: 0.05259, val loss: 3.0121, patience: 348, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 453, training loss: 0.05257, val loss: 2.99994, patience: 349, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 454, training loss: 0.05275, val loss: 3.01826, patience: 350, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 455, training loss: 0.05262, val loss: 3.01128, patience: 351, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 456, training loss: 0.05264, val loss: 2.99787, patience: 352, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 457, training loss: 0.05249, val loss: 2.99697, patience: 353, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 458, training loss: 0.05247, val loss: 2.99544, patience: 354, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 459, training loss: 0.05275, val loss: 3.00793, patience: 355, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 460, training loss: 0.05253, val loss: 2.98843, patience: 356, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 461, training loss: 0.0524, val loss: 3.00688, patience: 357, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 462, training loss: 0.05237, val loss: 3.00063, patience: 358, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 463, training loss: 0.05253, val loss: 2.99931, patience: 359, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 464, training loss: 0.0526, val loss: 2.99988, patience: 360, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 465, training loss: 0.05244, val loss: 2.99296, patience: 361, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 466, training loss: 0.05246, val loss: 3.0, patience: 362, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 467, training loss: 0.05221, val loss: 2.99689, patience: 363, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 468, training loss: 0.05244, val loss: 2.99535, patience: 364, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 469, training loss: 0.0522, val loss: 2.98722, patience: 365, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 470, training loss: 0.05228, val loss: 2.98628, patience: 366, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 471, training loss: 0.05233, val loss: 2.99595, patience: 367, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 472, training loss: 0.05234, val loss: 2.97768, patience: 368, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 473, training loss: 0.05266, val loss: 2.99367, patience: 369, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 474, training loss: 0.05218, val loss: 2.99383, patience: 370, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 475, training loss: 0.0525, val loss: 2.99852, patience: 371, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 476, training loss: 0.0525, val loss: 2.99349, patience: 372, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 477, training loss: 0.0523, val loss: 2.99726, patience: 373, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 478, training loss: 0.05254, val loss: 2.98727, patience: 374, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 479, training loss: 0.05229, val loss: 2.99487, patience: 375, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 480, training loss: 0.05244, val loss: 2.9986, patience: 376, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 481, training loss: 0.0523, val loss: 2.98937, patience: 377, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 482, training loss: 0.05223, val loss: 2.98448, patience: 378, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 483, training loss: 0.05222, val loss: 2.99183, patience: 379, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 484, training loss: 0.05232, val loss: 2.99756, patience: 380, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 485, training loss: 0.05228, val loss: 2.99239, patience: 381, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 486, training loss: 0.05214, val loss: 2.99577, patience: 382, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 487, training loss: 0.05231, val loss: 2.98934, patience: 383, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 488, training loss: 0.05216, val loss: 2.99997, patience: 384, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 489, training loss: 0.05217, val loss: 3.00136, patience: 385, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 490, training loss: 0.05225, val loss: 2.99395, patience: 386, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 491, training loss: 0.05234, val loss: 2.99516, patience: 387, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 492, training loss: 0.05236, val loss: 2.99534, patience: 388, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 493, training loss: 0.0521, val loss: 2.9996, patience: 389, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 494, training loss: 0.05203, val loss: 2.99386, patience: 390, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 495, training loss: 0.05219, val loss: 2.99304, patience: 391, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 496, training loss: 0.05233, val loss: 3.00113, patience: 392, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 497, training loss: 0.05199, val loss: 3.00283, patience: 393, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 498, training loss: 0.05233, val loss: 3.00408, patience: 394, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 499, training loss: 0.05212, val loss: 3.00474, patience: 395, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 500, training loss: 0.05217, val loss: 2.99902, patience: 396, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 501, training loss: 0.05208, val loss: 3.00126, patience: 397, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 502, training loss: 0.05205, val loss: 2.99837, patience: 398, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 503, training loss: 0.05216, val loss: 2.99365, patience: 399, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 504, training loss: 0.052, val loss: 2.99877, patience: 400, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 505, training loss: 0.05202, val loss: 3.00234, patience: 401, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 506, training loss: 0.05218, val loss: 2.99677, patience: 402, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 507, training loss: 0.05217, val loss: 2.99953, patience: 403, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 508, training loss: 0.05189, val loss: 2.99862, patience: 404, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 509, training loss: 0.05198, val loss: 2.99735, patience: 405, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 510, training loss: 0.05197, val loss: 2.99271, patience: 406, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 511, training loss: 0.052, val loss: 2.99957, patience: 407, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 512, training loss: 0.05209, val loss: 3.00243, patience: 408, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 513, training loss: 0.05201, val loss: 2.98879, patience: 409, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 514, training loss: 0.05196, val loss: 3.00307, patience: 410, training metric: 1.0, val metric: 0.0984\n",
            "INFO - root - epoch: 515, training loss: 0.05219, val loss: 2.98775, patience: 411, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 516, training loss: 0.05206, val loss: 2.98663, patience: 412, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 517, training loss: 0.05203, val loss: 2.99697, patience: 413, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 518, training loss: 0.0519, val loss: 3.00518, patience: 414, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 519, training loss: 0.05194, val loss: 2.99063, patience: 415, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 520, training loss: 0.05187, val loss: 2.99497, patience: 416, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 521, training loss: 0.05191, val loss: 2.98702, patience: 417, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 522, training loss: 0.05191, val loss: 2.99333, patience: 418, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 523, training loss: 0.05199, val loss: 2.99236, patience: 419, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 524, training loss: 0.05173, val loss: 2.98903, patience: 420, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 525, training loss: 0.05201, val loss: 2.99715, patience: 421, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 526, training loss: 0.0518, val loss: 2.99587, patience: 422, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 527, training loss: 0.05186, val loss: 2.99839, patience: 423, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 528, training loss: 0.05185, val loss: 2.99522, patience: 424, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 529, training loss: 0.05197, val loss: 2.98501, patience: 425, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 530, training loss: 0.05186, val loss: 3.0005, patience: 426, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 531, training loss: 0.05184, val loss: 2.99163, patience: 427, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 532, training loss: 0.05182, val loss: 2.99078, patience: 428, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 533, training loss: 0.05194, val loss: 2.99731, patience: 429, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 534, training loss: 0.05196, val loss: 2.98755, patience: 430, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 535, training loss: 0.05171, val loss: 2.99169, patience: 431, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 536, training loss: 0.05194, val loss: 2.98929, patience: 432, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 537, training loss: 0.05168, val loss: 2.98843, patience: 433, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 538, training loss: 0.05194, val loss: 3.00288, patience: 434, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 539, training loss: 0.05194, val loss: 3.00028, patience: 435, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 540, training loss: 0.05187, val loss: 2.99134, patience: 436, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 541, training loss: 0.05164, val loss: 2.99234, patience: 437, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 542, training loss: 0.05169, val loss: 2.9891, patience: 438, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 543, training loss: 0.05174, val loss: 3.00015, patience: 439, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 544, training loss: 0.05171, val loss: 2.99982, patience: 440, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 545, training loss: 0.05183, val loss: 2.99956, patience: 441, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 546, training loss: 0.05184, val loss: 3.004, patience: 442, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 547, training loss: 0.05158, val loss: 3.00738, patience: 443, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 548, training loss: 0.05171, val loss: 2.99568, patience: 444, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 549, training loss: 0.05185, val loss: 2.99715, patience: 445, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 550, training loss: 0.05158, val loss: 2.99649, patience: 446, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 551, training loss: 0.05185, val loss: 3.00065, patience: 447, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 552, training loss: 0.05167, val loss: 2.99973, patience: 448, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 553, training loss: 0.05195, val loss: 2.99885, patience: 449, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 554, training loss: 0.05165, val loss: 2.99609, patience: 450, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 555, training loss: 0.05168, val loss: 2.99992, patience: 451, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 556, training loss: 0.05163, val loss: 3.0051, patience: 452, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 557, training loss: 0.05165, val loss: 2.99506, patience: 453, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 558, training loss: 0.05161, val loss: 2.99838, patience: 454, training metric: 1.0, val metric: 0.102\n",
            "INFO - root - epoch: 559, training loss: 0.05158, val loss: 3.00148, patience: 455, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 560, training loss: 0.05157, val loss: 3.00544, patience: 456, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 561, training loss: 0.05185, val loss: 2.9994, patience: 457, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 562, training loss: 0.05157, val loss: 3.00236, patience: 458, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 563, training loss: 0.05175, val loss: 3.00161, patience: 459, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 564, training loss: 0.05176, val loss: 3.00944, patience: 460, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 565, training loss: 0.05149, val loss: 2.99226, patience: 461, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 566, training loss: 0.05171, val loss: 2.98866, patience: 462, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 567, training loss: 0.05157, val loss: 2.99943, patience: 463, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 568, training loss: 0.05172, val loss: 3.00091, patience: 464, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 569, training loss: 0.05163, val loss: 2.99793, patience: 465, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 570, training loss: 0.05146, val loss: 2.99807, patience: 466, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 571, training loss: 0.0515, val loss: 2.9995, patience: 467, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 572, training loss: 0.05167, val loss: 2.99973, patience: 468, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 573, training loss: 0.0517, val loss: 3.00632, patience: 469, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 574, training loss: 0.05156, val loss: 2.99749, patience: 470, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 575, training loss: 0.05148, val loss: 3.01058, patience: 471, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 576, training loss: 0.0516, val loss: 2.99746, patience: 472, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 577, training loss: 0.05154, val loss: 2.99974, patience: 473, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 578, training loss: 0.05173, val loss: 3.00778, patience: 474, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 579, training loss: 0.05157, val loss: 2.99996, patience: 475, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 580, training loss: 0.05155, val loss: 3.002, patience: 476, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 581, training loss: 0.05143, val loss: 3.00049, patience: 477, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 582, training loss: 0.05145, val loss: 2.99908, patience: 478, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 583, training loss: 0.05145, val loss: 3.00411, patience: 479, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 584, training loss: 0.05149, val loss: 3.00141, patience: 480, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 585, training loss: 0.05129, val loss: 2.99582, patience: 481, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 586, training loss: 0.05142, val loss: 3.00784, patience: 482, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 587, training loss: 0.05151, val loss: 3.00193, patience: 483, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 588, training loss: 0.05152, val loss: 3.00208, patience: 484, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 589, training loss: 0.05146, val loss: 3.00914, patience: 485, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 590, training loss: 0.05177, val loss: 3.00623, patience: 486, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 591, training loss: 0.0515, val loss: 3.01479, patience: 487, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 592, training loss: 0.05152, val loss: 3.0136, patience: 488, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 593, training loss: 0.05157, val loss: 3.00716, patience: 489, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 594, training loss: 0.05145, val loss: 3.00436, patience: 490, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 595, training loss: 0.05137, val loss: 3.01093, patience: 491, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 596, training loss: 0.05143, val loss: 2.99801, patience: 492, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 597, training loss: 0.05153, val loss: 3.00709, patience: 493, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 598, training loss: 0.05148, val loss: 3.00744, patience: 494, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 599, training loss: 0.05137, val loss: 3.004, patience: 495, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 600, training loss: 0.05133, val loss: 2.99856, patience: 496, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 601, training loss: 0.05145, val loss: 3.01281, patience: 497, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 602, training loss: 0.05134, val loss: 3.0135, patience: 498, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 603, training loss: 0.05119, val loss: 2.99971, patience: 499, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 604, training loss: 0.05149, val loss: 3.00307, patience: 500, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 605, training loss: 0.05128, val loss: 3.009, patience: 501, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 606, training loss: 0.05128, val loss: 2.99778, patience: 502, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 607, training loss: 0.05118, val loss: 3.00051, patience: 503, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 608, training loss: 0.05113, val loss: 3.0102, patience: 504, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 609, training loss: 0.05127, val loss: 2.99871, patience: 505, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 610, training loss: 0.0512, val loss: 3.00637, patience: 506, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 611, training loss: 0.05122, val loss: 2.99448, patience: 507, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 612, training loss: 0.05101, val loss: 3.00291, patience: 508, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 613, training loss: 0.05127, val loss: 2.99971, patience: 509, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 614, training loss: 0.05125, val loss: 3.00454, patience: 510, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 615, training loss: 0.05126, val loss: 2.99238, patience: 511, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 616, training loss: 0.05103, val loss: 2.99953, patience: 512, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 617, training loss: 0.05136, val loss: 2.99821, patience: 513, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 618, training loss: 0.05126, val loss: 2.99719, patience: 514, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 619, training loss: 0.05124, val loss: 2.99279, patience: 515, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 620, training loss: 0.05131, val loss: 2.99565, patience: 516, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 621, training loss: 0.05118, val loss: 2.99415, patience: 517, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 622, training loss: 0.05125, val loss: 3.00429, patience: 518, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 623, training loss: 0.0512, val loss: 2.99765, patience: 519, training metric: 1.0, val metric: 0.0988\n",
            "INFO - root - epoch: 624, training loss: 0.05113, val loss: 2.99546, patience: 520, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 625, training loss: 0.0511, val loss: 2.99736, patience: 521, training metric: 1.0, val metric: 0.098\n",
            "INFO - root - epoch: 626, training loss: 0.05117, val loss: 3.00116, patience: 522, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 627, training loss: 0.05089, val loss: 3.0083, patience: 523, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 628, training loss: 0.0511, val loss: 3.00376, patience: 524, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 629, training loss: 0.05117, val loss: 3.00664, patience: 525, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 630, training loss: 0.05116, val loss: 2.99904, patience: 526, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 631, training loss: 0.05115, val loss: 3.01585, patience: 527, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 632, training loss: 0.05122, val loss: 2.99886, patience: 528, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 633, training loss: 0.05105, val loss: 3.01357, patience: 529, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 634, training loss: 0.05107, val loss: 3.00301, patience: 530, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 635, training loss: 0.05115, val loss: 3.00529, patience: 531, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 636, training loss: 0.05106, val loss: 2.99738, patience: 532, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 637, training loss: 0.05117, val loss: 2.98874, patience: 533, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 638, training loss: 0.05111, val loss: 3.0064, patience: 534, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 639, training loss: 0.05117, val loss: 2.9954, patience: 535, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 640, training loss: 0.05098, val loss: 3.00774, patience: 536, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 641, training loss: 0.05113, val loss: 3.00266, patience: 537, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 642, training loss: 0.05112, val loss: 3.00361, patience: 538, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 643, training loss: 0.05117, val loss: 3.01346, patience: 539, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 644, training loss: 0.05099, val loss: 3.00909, patience: 540, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 645, training loss: 0.0509, val loss: 3.00687, patience: 541, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 646, training loss: 0.05115, val loss: 3.0055, patience: 542, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 647, training loss: 0.05076, val loss: 2.9965, patience: 543, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 648, training loss: 0.05093, val loss: 3.00726, patience: 544, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 649, training loss: 0.05086, val loss: 3.00685, patience: 545, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 650, training loss: 0.05087, val loss: 3.00085, patience: 546, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 651, training loss: 0.05084, val loss: 2.99552, patience: 547, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 652, training loss: 0.05071, val loss: 2.99846, patience: 548, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 653, training loss: 0.05082, val loss: 3.0022, patience: 549, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 654, training loss: 0.05087, val loss: 3.01176, patience: 550, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 655, training loss: 0.05106, val loss: 3.0118, patience: 551, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 656, training loss: 0.05084, val loss: 2.9959, patience: 552, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 657, training loss: 0.05081, val loss: 2.99941, patience: 553, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 658, training loss: 0.05093, val loss: 3.00545, patience: 554, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 659, training loss: 0.051, val loss: 2.9936, patience: 555, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 660, training loss: 0.05085, val loss: 3.00429, patience: 556, training metric: 1.0, val metric: 0.1016\n",
            "INFO - root - epoch: 661, training loss: 0.05086, val loss: 3.00731, patience: 557, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 662, training loss: 0.0508, val loss: 3.00614, patience: 558, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 663, training loss: 0.05103, val loss: 3.01199, patience: 559, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 664, training loss: 0.05077, val loss: 3.00501, patience: 560, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 665, training loss: 0.05076, val loss: 2.99996, patience: 561, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 666, training loss: 0.05088, val loss: 2.99036, patience: 562, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 667, training loss: 0.05091, val loss: 3.0065, patience: 563, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 668, training loss: 0.05079, val loss: 3.00468, patience: 564, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 669, training loss: 0.05073, val loss: 3.01245, patience: 565, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 670, training loss: 0.05066, val loss: 3.00663, patience: 566, training metric: 1.0, val metric: 0.1012\n",
            "INFO - root - epoch: 671, training loss: 0.05075, val loss: 3.0002, patience: 567, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 672, training loss: 0.05063, val loss: 3.00619, patience: 568, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 673, training loss: 0.05055, val loss: 2.99381, patience: 569, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 674, training loss: 0.05064, val loss: 3.01041, patience: 570, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 675, training loss: 0.05078, val loss: 3.00173, patience: 571, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 676, training loss: 0.05078, val loss: 3.00596, patience: 572, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 677, training loss: 0.0507, val loss: 3.01094, patience: 573, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 678, training loss: 0.05051, val loss: 3.00911, patience: 574, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 679, training loss: 0.05069, val loss: 3.00532, patience: 575, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 680, training loss: 0.05055, val loss: 3.00579, patience: 576, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 681, training loss: 0.05069, val loss: 2.99981, patience: 577, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 682, training loss: 0.05068, val loss: 3.00337, patience: 578, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 683, training loss: 0.05061, val loss: 3.00791, patience: 579, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 684, training loss: 0.05068, val loss: 3.01121, patience: 580, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 685, training loss: 0.0506, val loss: 3.0128, patience: 581, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 686, training loss: 0.0508, val loss: 3.00545, patience: 582, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 687, training loss: 0.0507, val loss: 3.01295, patience: 583, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 688, training loss: 0.05068, val loss: 3.00233, patience: 584, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 689, training loss: 0.05058, val loss: 3.00451, patience: 585, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 690, training loss: 0.05052, val loss: 3.00782, patience: 586, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 691, training loss: 0.05072, val loss: 3.00263, patience: 587, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 692, training loss: 0.05066, val loss: 2.99925, patience: 588, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 693, training loss: 0.05064, val loss: 2.99885, patience: 589, training metric: 1.0, val metric: 0.0996\n",
            "INFO - root - epoch: 694, training loss: 0.05078, val loss: 3.00206, patience: 590, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 695, training loss: 0.05053, val loss: 3.0052, patience: 591, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - epoch: 696, training loss: 0.05052, val loss: 3.00776, patience: 592, training metric: 1.0, val metric: 0.1\n",
            "INFO - root - epoch: 697, training loss: 0.05069, val loss: 3.00794, patience: 593, training metric: 1.0, val metric: 0.0992\n",
            "INFO - root - epoch: 698, training loss: 0.05044, val loss: 3.01735, patience: 594, training metric: 1.0, val metric: 0.1008\n",
            "INFO - root - epoch: 699, training loss: 0.05056, val loss: 2.9996, patience: 595, training metric: 1.0, val metric: 0.1004\n",
            "INFO - root - loaded best model at epoch 104\n",
            "INFO - root - Best val loss: 2.3077800273895264\n",
            "INFO - root - Best val metric: 0.346\n",
            "INFO - root - test loss: 3.005398988723755\n",
            "INFO - root - test metric: 0.0172\n",
            "INFO - run - Completed after 0:29:39\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mprior5_skipcircles-mean\u001b[0m at: \u001b[34mhttps://wandb.ai/GRL_miniproject/grl-miniproject-skipcircles/runs/3v3buwaj\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250104_020136-3v3buwaj/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python run.py with /content/PR-MPNN/configs/sym_4cycles/prior5_skipcircles.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compress and download test results\n"
      ],
      "metadata": {
        "id": "HWdptb1HQP-L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5wr0rtR4-WK",
        "outputId": "ef597845-fd01-4c2c-9bf9-143a4f3bfabe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PR-MPNN\n",
            "  adding: logs/ (stored 0%)\n",
            "  adding: logs/prior5_skipcircles/ (stored 0%)\n",
            "  adding: logs/prior5_skipcircles/config.yaml (deflated 51%)\n",
            "  adding: logs/prior5-4cycle-sum/ (stored 0%)\n",
            "  adding: logs/prior5-4cycle-sum/result.yaml (deflated 19%)\n",
            "  adding: logs/prior5-4cycle-sum/model_best_3_0.pt (deflated 53%)\n",
            "  adding: logs/prior5-4cycle-sum/model_best_4_0.pt (deflated 54%)\n",
            "  adding: logs/prior5-4cycle-sum/model_best_0_0.pt (deflated 54%)\n",
            "  adding: logs/prior5-4cycle-sum/model_best_1_0.pt (deflated 54%)\n",
            "  adding: logs/prior5-4cycle-sum/model_best_2_0.pt (deflated 54%)\n",
            "  adding: logs/prior5-4cycle-sum/config.yaml (deflated 50%)\n",
            "  adding: logs/prior10-4cycle-sum/ (stored 0%)\n",
            "  adding: logs/prior10-4cycle-sum/model_best_0_0.pt (deflated 53%)\n",
            "  adding: logs/prior10-4cycle-sum/model_best_1_0.pt (deflated 53%)\n",
            "  adding: logs/prior10-4cycle-sum/model_best_2_0.pt (deflated 53%)\n",
            "  adding: logs/prior10-4cycle-sum/config.yaml (deflated 50%)\n",
            "  adding: logs/prior10_skipcircles-mean/ (stored 0%)\n",
            "  adding: logs/prior10_skipcircles-mean/result.yaml (deflated 15%)\n",
            "  adding: logs/prior10_skipcircles-mean/model_best_3_0.pt (deflated 53%)\n",
            "  adding: logs/prior10_skipcircles-mean/model_best_4_0.pt (deflated 53%)\n",
            "  adding: logs/prior10_skipcircles-mean/model_best_0_0.pt (deflated 53%)\n",
            "  adding: logs/prior10_skipcircles-mean/model_best_1_0.pt (deflated 53%)\n",
            "  adding: logs/prior10_skipcircles-mean/model_best_2_0.pt (deflated 54%)\n",
            "  adding: logs/prior10_skipcircles-mean/config.yaml (deflated 51%)\n",
            "  adding: logs/prior5_skipcircles-mean/ (stored 0%)\n",
            "  adding: logs/prior5_skipcircles-mean/result.yaml (deflated 6%)\n",
            "  adding: logs/prior5_skipcircles-mean/model_best_3_0.pt (deflated 54%)\n",
            "  adding: logs/prior5_skipcircles-mean/model_best_4_0.pt (deflated 54%)\n",
            "  adding: logs/prior5_skipcircles-mean/model_best_0_0.pt (deflated 54%)\n",
            "  adding: logs/prior5_skipcircles-mean/model_best_1_0.pt (deflated 54%)\n",
            "  adding: logs/prior5_skipcircles-mean/model_best_2_0.pt (deflated 54%)\n",
            "  adding: logs/prior5_skipcircles-mean/config.yaml (deflated 51%)\n"
          ]
        }
      ],
      "source": [
        "%cd /content/PR-MPNN\n",
        "!zip -r logs.zip logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IdVJ8oGb_RSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045800dd-5ada-4289-b291-d3555d0c4e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted file: pre_transform.pt\n",
            "Deleted file: pre_filter.pt\n",
            "Deleted file: train_sym_skipcircles.pt\n",
            "Deleted file: test_sym_skipcircles.pt\n",
            "Deleted file: val_sym_skipcircles\n"
          ]
        }
      ],
      "source": [
        "#import os\n",
        "#dir_path = \"/content/PR-MPNN/datasets/sym_skipcircles/heu_longest_path_dir_1024_/processed\"\n",
        "#for filename in os.listdir(dir_path):\n",
        "#    file_path = os.path.join(dir_path, filename)\n",
        "\n",
        "#    # Check if it is a file (not a subdirectory)\n",
        "#    if os.path.isfile(file_path):\n",
        "#        os.remove(file_path)  # Remove the file\n",
        "#        print(f\"Deleted file: {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check datasets\n",
        "\n",
        "Here we check and plot the datasets `4cycles` and `skipcircles`."
      ],
      "metadata": {
        "id": "506kL5cUEksw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqMWbPC75t2y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "from torch_geometric.utils import degree\n",
        "from torch_geometric.utils.convert import from_networkx\n",
        "\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "from torch_geometric.data import Data, InMemoryDataset\n",
        "from torch_geometric.utils import degree as pyg_degree\n",
        "\n",
        "\n",
        "# Synthetic datasets\n",
        "\n",
        "class SymmetrySet:\n",
        "    def __init__(self):\n",
        "        self.hidden_units = 0\n",
        "        self.num_classes = 0\n",
        "        self.num_features = 0\n",
        "        self.num_nodes = 0\n",
        "\n",
        "    def addports(self, data):\n",
        "        data.ports = torch.zeros(data.num_edges, 1)\n",
        "        degs = degree(data.edge_index[0], data.num_nodes, dtype=torch.long) # out degree of all nodes\n",
        "        for n in range(data.num_nodes):\n",
        "            deg = degs[n]\n",
        "            ports = np.random.permutation(int(deg))\n",
        "            for i, neighbor in enumerate(data.edge_index[1][data.edge_index[0]==n]):\n",
        "                nb = int(neighbor)\n",
        "                data.ports[torch.logical_and(data.edge_index[0]==n, data.edge_index[1]==nb), 0] = float(ports[i])\n",
        "        return data\n",
        "\n",
        "    def makefeatures(self, data):\n",
        "        data.x = torch.ones((data.num_nodes, 1))\n",
        "        data.id = torch.tensor(np.random.permutation(np.arange(data.num_nodes))).unsqueeze(1)\n",
        "        return data\n",
        "\n",
        "    def makedata(self):\n",
        "        pass\n",
        "class SkipCircles(SymmetrySet):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden_units = 32\n",
        "        self.num_classes = 10 # num skips\n",
        "        self.num_features = 1\n",
        "        self.num_nodes = 41\n",
        "        self.graph_class = True\n",
        "        self.makedata()\n",
        "\n",
        "    def makedata(self):\n",
        "        size=self.num_nodes\n",
        "        skips = [2, 3, 4, 5, 6, 9, 11, 12, 13, 16]\n",
        "        graphs = []\n",
        "        for s, skip in enumerate(skips):\n",
        "            edge_index = torch.tensor([[0, size-1], [size-1, 0]], dtype=torch.long)\n",
        "            for i in range(size - 1):\n",
        "                e = torch.tensor([[i, i+1], [i+1, i]], dtype=torch.long)\n",
        "                edge_index = torch.cat([edge_index, e], dim=-1)\n",
        "            for i in range(size):\n",
        "                e = torch.tensor([[i, i], [(i - skip) % size, (i + skip) % size]], dtype=torch.long)\n",
        "                edge_index = torch.cat([edge_index, e], dim=-1)\n",
        "            data = Data(edge_index=edge_index, num_nodes=self.num_nodes)\n",
        "            data = self.makefeatures(data)\n",
        "            data = self.addports(data)\n",
        "\n",
        "            # data.x = torch.cat([data.x, data.id.float()], dim=1)\n",
        "            # data.x = torch.cat([data.x, torch.randint(0, 100, (data.x.size(0), 1), device=data.x.device) / 100.0], dim=1)\n",
        "            edge_attr = data.ports.squeeze(-1).long()\n",
        "            data.edge_attr = torch.nn.functional.one_hot(edge_attr, 4).to(torch.float)\n",
        "\n",
        "            data.y = torch.tensor(s)\n",
        "            graphs.append(data)\n",
        "\n",
        "        return graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npNF2Ac756qg"
      },
      "outputs": [],
      "source": [
        "dataset = SkipCircles()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8FAIkXY6IqN"
      },
      "outputs": [],
      "source": [
        "train_set = dataset.makedata()\n",
        "val_set = dataset.makedata()\n",
        "test_set = dataset.makedata()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS9r8AE_6KXu",
        "outputId": "2445f486-2fd5-49a2-d8ec-53face596aa0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Data(edge_index=[2, 164], num_nodes=41, x=[41, 1], id=[41, 1], ports=[164, 1], edge_attr=[164, 4], y=0),\n",
              " Data(edge_index=[2, 164], num_nodes=41, x=[41, 1], id=[41, 1], ports=[164, 1], edge_attr=[164, 4], y=1),\n",
              " Data(edge_index=[2, 164], num_nodes=41, x=[41, 1], id=[41, 1], ports=[164, 1], edge_attr=[164, 4], y=2),\n",
              " Data(edge_index=[2, 164], num_nodes=41, x=[41, 1], id=[41, 1], ports=[164, 1], edge_attr=[164, 4], y=3),\n",
              " Data(edge_index=[2, 164], num_nodes=41, x=[41, 1], id=[41, 1], ports=[164, 1], edge_attr=[164, 4], y=4),\n",
              " Data(edge_index=[2, 164], num_nodes=41, x=[41, 1], id=[41, 1], ports=[164, 1], edge_attr=[164, 4], y=5),\n",
              " Data(edge_index=[2, 164], num_nodes=41, x=[41, 1], id=[41, 1], ports=[164, 1], edge_attr=[164, 4], y=6),\n",
              " Data(edge_index=[2, 164], num_nodes=41, x=[41, 1], id=[41, 1], ports=[164, 1], edge_attr=[164, 4], y=7),\n",
              " Data(edge_index=[2, 164], num_nodes=41, x=[41, 1], id=[41, 1], ports=[164, 1], edge_attr=[164, 4], y=8),\n",
              " Data(edge_index=[2, 164], num_nodes=41, x=[41, 1], id=[41, 1], ports=[164, 1], edge_attr=[164, 4], y=9)]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4sDA6kR6iSV",
        "outputId": "4058a28b-a26d-4564-89bc-85bfdaa341e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(edge_index=[2, 164], num_nodes=41, x=[41, 1], id=[41, 1], ports=[164, 1], edge_attr=[164, 4], y=0)\n",
            "Edge Index: tensor([[ 0, 40,  0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,\n",
            "          8,  9,  9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17,\n",
            "         17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 24, 24, 25, 25, 26,\n",
            "         26, 27, 27, 28, 28, 29, 29, 30, 30, 31, 31, 32, 32, 33, 33, 34, 34, 35,\n",
            "         35, 36, 36, 37, 37, 38, 38, 39, 39, 40,  0,  0,  1,  1,  2,  2,  3,  3,\n",
            "          4,  4,  5,  5,  6,  6,  7,  7,  8,  8,  9,  9, 10, 10, 11, 11, 12, 12,\n",
            "         13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21,\n",
            "         22, 22, 23, 23, 24, 24, 25, 25, 26, 26, 27, 27, 28, 28, 29, 29, 30, 30,\n",
            "         31, 31, 32, 32, 33, 33, 34, 34, 35, 35, 36, 36, 37, 37, 38, 38, 39, 39,\n",
            "         40, 40],\n",
            "        [40,  0,  1,  0,  2,  1,  3,  2,  4,  3,  5,  4,  6,  5,  7,  6,  8,  7,\n",
            "          9,  8, 10,  9, 11, 10, 12, 11, 13, 12, 14, 13, 15, 14, 16, 15, 17, 16,\n",
            "         18, 17, 19, 18, 20, 19, 21, 20, 22, 21, 23, 22, 24, 23, 25, 24, 26, 25,\n",
            "         27, 26, 28, 27, 29, 28, 30, 29, 31, 30, 32, 31, 33, 32, 34, 33, 35, 34,\n",
            "         36, 35, 37, 36, 38, 37, 39, 38, 40, 39, 39,  2, 40,  3,  0,  4,  1,  5,\n",
            "          2,  6,  3,  7,  4,  8,  5,  9,  6, 10,  7, 11,  8, 12,  9, 13, 10, 14,\n",
            "         11, 15, 12, 16, 13, 17, 14, 18, 15, 19, 16, 20, 17, 21, 18, 22, 19, 23,\n",
            "         20, 24, 21, 25, 22, 26, 23, 27, 24, 28, 25, 29, 26, 30, 27, 31, 28, 32,\n",
            "         29, 33, 30, 34, 31, 35, 32, 36, 33, 37, 34, 38, 35, 39, 36, 40, 37,  0,\n",
            "         38,  1]])\n",
            "Node Features (x): tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Edge Features tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.]])\n",
            "Edge Features(port) tensor([[1.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [1.]])\n",
            "Graph Label (y): tensor(0)\n"
          ]
        }
      ],
      "source": [
        "graph = train_set[0]\n",
        "print(graph)\n",
        "\n",
        "# Access specific attributes\n",
        "print(\"Edge Index:\", graph.edge_index)\n",
        "print(\"Node Features (x):\", graph.x)\n",
        "print(\"Edge Features\", graph.edge_attr)\n",
        "print(\"Edge Features(port)\", graph.ports)\n",
        "print(\"Graph Label (y):\", graph.y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "34m9ndafGYrS",
        "outputId": "b77c985f-3f9c-4ab0-8e18-8644ba4c76fb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzMAAAKCCAYAAADlSofSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzddVhU29cH8O8MpWAjmIRKWNiKASItIWUr2N157Vauee1uAa9JiwgoKBZiK14BRUKURiSkZs77hy/zE0FyhpmB9Xkenqtzztl7DReHs87ee20WwzAMCCGEEEIIIUTMsIUdACGEEEIIIYRUBSUzhBBCCCGEELFEyQwhhBBCCCFELFEyQwghhBBCCBFLlMwQQgghhBBCxBIlM4QQQgghhBCxRMkMIYQQQgghRCxRMkMIIYQQQggRS5TMEEIIIYQQQsQSJTOEEFJHREdHg8ViYffu3TXa76RJk6Cqqlrhcxs0aCDYgAghhNQalMwQQgifffr0CfPmzYOGhgZkZWUhKyuLzp07Y+7cuXj9+rWwwxO6nJwcbNy4EUFBQcIORWDOnTsHFotV6ldCQoKwwyOEkFpDUtgBEEJIbeLt7Y3Ro0dDUlIS48ePR/fu3cFms/H+/Xu4urri6NGj+PTpE1RUVIQdao05efIkuFwu7+85OTnYtGkTAGDIkCFCiqpmbN68Ge3atSv2WpMmTYQTDCGE1EKUzBBCCJ98/PgRY8aMgYqKCm7fvo1WrVoVO75jxw4cOXIEbHbZg+LZ2dmQk5MTZKg1SkpKStghCI2ZmRn69Okj7DAIIaTWomlmhBDCJzt37kR2djbOnj1bIpEBAElJSSxYsABKSkq814rWiHz8+BHm5uZo2LAhxo8fDwAIDg7GyJEjoaysDBkZGSgpKWHx4sX48eNHsXaL2oiKioKpqSnk5OTQunVrbN68GQzDlBrriRMn0KFDB8jIyKBv374IDQ0t8719+/YNEhISOHDgAO+1lJQUsNlsyMvLF+tn9uzZaNmyZbH4itbMREdHQ0FBAQCwadMm3tSrjRs3FusvPj4eNjY2aNCgARQUFLBs2TJwOJwyY5w4cSKaN2+OgoKCEsdMTEygqalZ5vWCkpmZWW7shBBCqoaSGUII4RNvb2+oqalBW1u7UtcVFhbC1NQUioqK2L17N4YPHw4AuHr1KnJycjB79mwcPHgQpqamOHjwICZMmFCiDQ6Hg6FDh6JFixbYuXMnevfujQ0bNmDDhg0lzr148SJ27dqFmTNnYuvWrYiOjoadnV2pSUCRJk2aoGvXrrh37x7vtfv374PFYiEtLQ3v3r3jvR4cHAxdXd1S21FQUMDRo0cBALa2tnBycoKTkxPs7OyKvRdTU1PIy8tj9+7d0NPTw549e3DixIkyv48ODg5ITU3FrVu3ir2ekJCAO3fuwN7evszrc3JykJKSUu5Xenp6me38Sl9fH40aNYKsrCysrKwQGRlZ4WsJIYRUAEMIIaTaMjIyGACMjY1NiWPp6elMcnIy7ysnJ4d3bOLEiQwAZuXKlSWu+/W8In///TfDYrGYmJiYEm3Mnz+f9xqXy2UsLCwYaWlpJjk5mWEYhvn06RMDgJGXl2fS0tJ453p4eDAAGC8vrzLf49y5c5kWLVrw/r5kyRJm8ODBjKKiInP06FGGYRgmNTWVYbFYzP79+4vFp6Kiwvt7cnIyA4DZsGFDiT6K3svmzZuLvd6zZ0+md+/eZcbH4XCYtm3bMqNHjy72+j///MOwWCwmKiqqzOs3bNjAACj369f38ieXL19mJk2axJw/f55xc3Nj1q5dy8jKyjLNmzdnYmNjy72eEEJIxdCaGUII4YPv378DQKllhYcMGYJXr17x/r5r1y4sW7as2DmzZ88ucV39+vV5f87OzsaPHz8wcOBAMAyDFy9eQFlZudj58+bN4/2ZxWJh3rx5uHHjBgICAjBmzBjesdGjR6Np06a8vxeNokRFRZX5HnV1dXH48GGEh4dDU1MTwcHBMDU1hYKCAoKDgzFr1izcv38fDMP8cWSmombNmlWibycnpzKvYbPZGD9+PA4cOIDMzEw0bNgQAODi4oKBAweWWIj/uwkTJkBHR6fc2H79//Ino0aNwqhRo3h/t7GxgampKQYPHoxt27bh2LFj5bZBCCGkfJTMEEIIHxTdOGdlZZU4dvz4cWRmZiIxMbHUqU6SkpJo27ZtiddjY2Oxfv16eHp6lpjalJGRUezvbDYb7du3L/aahoYGgJ/rVH71exJUlNiUN32qKEEJDg5G27Zt8eLFC2zduhUKCgq8vWuCg4PRqFEjdO/evcy2ylKvXj3euppfY6zI9K4JEyZgx44dcHNzw4QJExAeHo5nz55VKHlo3759ie8hP+no6EBbWxsBAQEC64MQQuoaSmYIIYQPGjdujFatWuHt27cljhWtofk9qSgiIyNTosIZh8OBsbEx0tLSsGLFCnTs2BFycnKIj4/HpEmTipU6riwJCYlSX2f+UCygSOvWrdGuXTvcu3cPqqqqYBgGAwYMgIKCAhYuXIiYmBgEBwdj4MCB5VZsq0p8FdG5c2f07t0bzs7OmDBhApydnSEtLV1slORPsrKySk1GS4vv92SropSUlBAeHl6lawkhhJREBQAIIYRPLCws8OHDBzx58qTabb158wYRERHYs2cPVqxYAWtraxgZGaF169alns/lcktME4uIiAAAXiUxftDV1UVwcDCCg4PRo0cPNGzYEN27d0fjxo3h6+uL58+fY/DgwWW2wWKx+BZPaSZMmIA7d+7g69evuHjxIiwsLIpNq/uT3bt3o1WrVuV+9e3bt8qxRUVFVTkRIoQQUhKNzBBCCJ/89ddfuHjxIqZMmYLbt2+jRYsWxY6XN/Lxq6LRiV+vYRgG+/fv/+M1hw4d4pVOZhgGhw4dgpSUFAwNDSvzNsqkq6uLCxcu4PLlyzAzMwPwc4rbwIED8c8//6CgoKDc9TKysrIAfpZ7FoSxY8di6dKlWLhwIaKiorBr164KXcfPNTPJycklkhYfHx88e/YMCxYsqFA8hBBCykfJDCGE8Im6ujouXryIsWPHQlNTE+PHj0f37t3BMAw+ffqEixcvgs1ml7o+5ncdO3ZEhw4dsGzZMsTHx6NRo0a4fv36H9eN1KtXD76+vpg4cSK0tbVx8+ZN3LhxA6tXr+brSEBRohIeHg5HR0fe64MHD8bNmzd5+9aUpX79+ujcuTMuX74MDQ0NNGvWDF27dkXXrl35EqOCggKGDh2Kq1evokmTJrCwsKjQdfxcMzNw4ED07NkTffr0QePGjfH8+XOcOXMGSkpKWL16NV/6IIQQQtPMCCGEr6ytrfHmzRuMGzcOfn5+WLhwIRYvXgwPDw9YWFjg+fPnxSqL/YmUlBS8vLzQo0cP/P3339i0aRPU1dVx4cKFUs+XkJCAr68vEhISsHz5coSGhmLDhg3YsmULX9+fpqYmFBUVAaDYKEZRktOvXz/IyMiU286pU6fQpk0bLF68GGPHjsW1a9f4GmfRXjyjRo2qUDz8Nnr0aERGRsLR0RHz58+Hr68vpk+fjtDQ0BIjdoQQQqqOxVRm3gMhhBCRM2nSJFy7dq1Ci9frCg8PD9jY2ODevXvVLhNNCCFEdNHIDCGEkFrn5MmTaN++fYXWwBBCCBFftGaGEEJIrXHp0iW8fv0aN27cwP79+wVeOY0QQohwUTJDCCGk1hg7diwaNGiAqVOnYs6cOcIOhxBCiIDRmhlCCCGEEEKIWKI1M4QQQgghhBCxRMkMIYQQQgghRCxRMkMIIYQQQggRS5TMEEIIIYQQQsQSJTOEEEIIIYQQsUTJDCGEEEIIIUQsUTJDCCGEEEIIEUuUzBBCCCGEEELEEiUzhBBCCCGEELFEyQwhhBBCCCFELFEyQwghhBBCCBFLlMwQQgghhBBCxBIlM4QQQgghhBCxRMkMIYQQQgghRCxRMkMIIYQQQggRS5TMEEIIIYQQQsQSJTOEEEIIIYQQsUTJDCGEEEIIIUQsUTJDCCGEEEIIEUuUzBBCCCGEEELEEiUzhBBCCCGEELFEyQwhhBBCCCFELFEyQwghhBBCCBFLlMwQQgghhBBCxBIlM4QQQgghhBCxRMkMIYQQQgghRCxRMkMIIYQQQggRS5TMEEIIIYQQQsQSJTOEEEIIIYQQsUTJDCGEEEIIIUQsUTJDCCGEEEIIEUuUzBBCCCGEEELEEiUzhBBCCCGEELFEyQwhhBBCCCFELFEyQwghhBBCCBFLlMwQQgghhBBCxBIlM4QQQgghhBCxRMkMIYQQQgghRCxRMkMIIYQQQggRS5TMEEIIIYQQQsQSJTOEEEIIIYQQsUTJDCGEEEIIIUQsSQo7AEIIqYjsvEJEp2Yjv5ALaUk2VOXlICdDH2GEEEJIXUZ3AoQQkRWZmAmXkFgEhichNi0HzC/HWACUm8lCX1MR47WVod6iobDCJIQQQoiQsBiGYco/jRBCak5cWg5Wu71B8IcUSLBZ4HD//DFVdFxXrTkcbbWg1Ey2BiMlhBBCiDBRMkMIESmXQmOxwTMMhVymzCTmdxJsFiTZLGyy6oIxfZUFGCEhhBBCRAUlM4QQkXEoMBK7/SKq3c4yEw3M01fnQ0SEEEIIEWVUzYwQIhIuhcbyJZEBgN1+EbgcGsuXtgghhBAiuiiZIYQIXVxaDjZ4hvG1zfWeYYhLy+Frm4QQQggRLZTMEEKEbrXbGxRWYn1MRRRyGax2e8PXNgkhhBAiWiiZIYQIVWRiJoI/pFRqsX9FcLgMgj+k4ENSJl/bJYQQQojooGSGECJULiGxkGCzBNK2BJsF58e0doYQQgiprSiZIYQIVWB4Et9HZYpwuAwCI5IE0jYhhBBChI+SGUKI0GTlFSJWwIv0Y1NzkJ1XKNA+CCGEECIclMwQQoQmJjUbgt7oigEQnZot4F4IIYQQIgyUzBBChCa/kFur+iGEEEJIzaJkhhAiNNKSNfMRVFP9EEIIIaRm0W94QojQqMrLQTB1zP6H9f/9EEIIIaT2oWSGECI0cjKSUG4mK9A+lOVlIScjKdA+CCGEECIclMwQQoRKX1NRoPvM6GsoCqRtQgghhAgfJTOEEKEar60s0H1m7PsrC6RtQgghhAgfJTOEEKFSb9EQumrN+T46I8FmQVetOdQUG/K1XUIIIYSIDkpmCCFC52irBUk+JzOSbBYcbbX42iYhhBBCRAslM4QQoVNqJotNVl342uZmqy5QEnBxAUIIIYQIFyUzhBCRMKavMpaZaPz8C1O9NTScF+7QVqCNMgkhhJDajpIZQojImKevjjHtueAW5oONyiU0LDCQkWRjlaEyZD4EwsjICF+/fhVQpIQQQggRBZTMEEJERkZGBpw2zkLHj1cxUE0BAMotDFB0PC/2NS5P1MJMIy34+/vjx48fMDU1RVpamsDjJoQQQohwsBimmvM5CCGET2bPng1nZ2eEhYVBWVkZkYmZcAmJhdfzKKTkssBi/S+xYeHnhpj6Goqw0GwEw75dMH36dOzZswcA8O7dOwwePBjq6urw9/dHgwYNhPSuCCGEECIolMwQQkTCvXv3oKenh0OHDmHu3LnFjm3btg3/HDiMey/+Q34hF9KSbKjKy0FORpJ3zubNm+Ho6IjIyEgoKSkBAJ4+fQoDAwP069cPN27cgIyMTI2+J0IIIYQIFiUzhBChy83NRffu3dG8eXMEBweDzS4+A3bixImIiIjAo0eP/thGZmYm2rdvD1tbW5w4cYL3elBQEIYOHQpzc3NcuXIFkpKSf2yDEEIIIeKF1swQQoRuy5YtiI6OxqlTp0okMgAQHh4OTU3NMtto2LAhVq9ejTNnziAiIoL3+pAhQ3Dt2jV4eXlh2rRp4HKpyhkhhBBSW1AyQwgRqlevXmHnzp1Yu3YtOnXqVOI4wzAIDw+HhoZGuW3Nnj0brVq1woYNG4q9bmlpiQsXLuDChQtYvHgxaECalCU7rxBhXzLwIjYdYV8ykJ1XKOyQCCGE/AHNtyCECE1hYSGmTZuGjh07YsWKFaWek5qaim/fvlUomalXrx42bNiA6dOnY8WKFejRowfv2NixY5GRkYHZs2ejadOm2LhxI5/eBakNiopNBIYnITYtp1hhcBYA5Way0NdUxHhtZai3aCisMAkhhPyG1swQQoRmz549WL58OR49egRtbe1Sz3nw4AF0dHTw6tUrdOvWrdw2CwsL0blzZ2hoaMDb27vE8e3bt2PVqlXYu3cvFi1aVN23QMRcXFoOVru9QfCHFEiwWeBw//wrsei4rlpzONpqQamZbA1GSgghpDQ0zYwQIhQfP37EunXrsHDhwj8mMgB461/U1dUr1K6kpCS2bNmCGzdu4MGDByWOr1y5En/99RcWL16Ms2fPVi14UitcCo2F0d67eBiVCgBlJjK/Hn8YlQqjvXdxKTRW4DESQggpG43MEEJqHMMwMDY2xocPH/D27dsy94BZtWoVLl68iJiYmAq3z+Vy0bt3bzRs2BB3794ttj9NUf+zZs3CqVOncOXKFQwfPrzK74WIp0OBkdjtF1H+ieVYZqKBefoVS7QJIYTwH43MEEJq3Llz53D79m0cP3683M0sIyIiKrRe5ldsNhvbtm1DcHAwbt26VeI4i8XCkSNHMHLkSIwdOxZ+fn6Vap+It0uhsXxJZABgt18ELtMIDSGECA2NzBBCalRCQgI6d+7MqzBWnq5du2LIkCE4dOhQpfphGAaDBw9GdnY2nj59WmrJ5/z8fNja2iIoKAj+/v4YOHBgpfog4icuLQdGe+8ir5B/JbplJNkIWKxHa2gIIUQIaGSGEFKjFixYAElJSezdu7fcczkcDj58+FDpkRng5+iLo6MjXrx4gevXr5d6jrS0NK5evYrevXvDwsICr169qnQ/RLysdnuDwnLWxlRWIZfBarc3fG2TEEJIxVAyQwipMR4eHrh69SoOHDgAeXn5cs+Pi4tDXl5elZIZANDV1YWZmRnWrVuHwsLS9wqRlZWFl5cX2rdvDxMTE0RGRlapLyL6IhMzEfwhpdyF/pXF4TII/pCCD0mZfG2XEEJI+SiZIYTUiIyMDMyZMwcWFhYYPXp0ha4JDw8HAGhqala5323btiE8PLzMKW2NGzeGr68vmjVrBiMjI8TFxVW5PyK6XEJiIcFmlX9iFUiwWXB+TGtnCCGkplEyQwipEStWrMD3799x9OjREtXF/iQiIgLS0tJQVlaucr89e/bEqFGjsHHjRuTl5f3xPAUFBfj7+4PFYsHY2BjJyclV7pOIpsDwJL6PyhThcBkERiQJpG1CCCF/RskMIUTg7t69i+PHj2P79u1QUlKq8HURERFQU1ODhIREtfrfvHkzvnz5gmPHjpV5Xtu2beHv749v375h6NChyMjIqFa/RHRk5RUiNi1HoH3EpuYgO6/06YyEEEIEg5IZQohA5ebmYvr06Rg0aBBmz55dqWvDw8OrvF7mV5qampg0aRK2bduGrKysMs9VV1eHn58foqKiMGzYMOTkCPYGmNSMmNRsCLp0JwMgOjVbwL0QQgj5laSwAyCE1G6bN29GTEwMPDw8Si2PXJaIiAiMGTOGL3GsX78eTk5O2LdvH9auXVvmud26dYOPjw+MjIwwYsQIuLu7Q1pami9xEMH58eMHUlJSSv2KTC8A5AYJPIZ8PpZ8JoQQUj5KZgghVZKdV4jo1GzkF3IhLcmGqrwc5GSKf6S8evUKO3fuxIYNG9CpU6dKtf/jxw/ExsbyZWQGAJSVlTFnzhzs2rULs2fPLrea2oABA+Du7g5LS0tMmDABLi4u1Z7uRiouPz8fqampf0xOSvsqbRRNRkYGCgoKaNKuKzBI8MmMtCRNeCCEkJpEm2YSQiosMjETLiGxCAxPQmxaTrFpOywAys1koa+piPHaymgnXx/9+/dHXl4enj17VumRjbdv30JLSwvBwcHQ0dHhS/zJyclo37495syZgx07dlToGldXV4wcORLTpk3DsWPHKly8gPwPh8NBWlpapRKT79+/l2hHUlISzZs3r9SXrKwsWCwWsvMK0XXjLYFONWMBeLvRtERSTwghRHDoE5cQUq64tBysdnuD4A8pkGCzSq0IxQCIScuBU0gMzj2KhpJUFl59jMeDWx5VmqLFj7LMv1NQUMDixYuxa9cuLFy4EK1bty73Gjs7O5w+fRqTJ09G48aNsWPHjjqd0DAMg4yMjHKTkeTkZN6f09PT8ftzMxaLBXl5+WKJR7du3cpMTBo1alSl731ERASuXr0KVrYSGLny9zeqKmV5WUpkCCGkhtGnLiGkTJdCY7HBM4y3a3p5pW2Ljsfm1YfSzOOIYrVEvyr0GxERgSZNmqB58+ZVuPrPli5disOHD2Pr1q04cuRIha6ZNGkSMjIysGjRIjRt2hSrVq3ia0zCwjAMsrOzKzVikpKSAg6HU6Ktxo0bF0s81NXVMWDAgD8mJk2bNhXotL3IyEhcvXoVV65cwatXryAnJ4euEzchCc3ABf+TUQkWoK+hyPd2CSGElI2SGULIHx0KjMRuv4gqXctiS4ALYKXrG6Rk5WGevnqlro+IiICGhgbfR0EaN26MlStXYvXq1Vi6dCk6dOhQoesWLlyI9PR0rF69Gk2aNKl0ZbaakJubW+nEpLS9d+Tk5IolHm3btkWPHj3+mJg0a9ZMJAokfPjwgZfAvHz5EnJychg2bBjWr18PMzMzfP5eCON99wTSN4cBcl7fwvchSmjUqJFA+iCEEFISrZkhhJTqUmgsVrq+4Vt7O+y0MLpvxTe/HDhwIDp06AAnJye+xVDkx48fUFNTg4GBQaXaZxgGixcvxoEDB+Ds7Ixx48bxPbYiBQUFJdaZ/Dp1q7Sv7OySZYGlpaWhoKBQ4TUm8vLyqF+/vsDeF799/PiRl8C8ePECsrKyGDZsGEaOHAkzMzPIysoWO9/hdAgeRqXydfNMCRbQJD8Jbw/OgqysLObPn4+FCxeWW2SCEEJI9VEyQwgpIS4tB0Z77yKPj2VmZSTZCFisB6VmsuWfDKB58+ZYtGhRuWWUq+rYsWOYM2cOXr16BS0trQpfx+VyMXXqVDg5OcHNzQ3Dhg2r0DXp6emVGjH59u1biXYkJCRKrDP5/ev3xEVOTq7WrfGJioriJTDPnz+HrKwsLC0tMXLkSJibm5dIYH4lyJ9tidxv2LNnD69QxOzZs7F06VK0bNmSb30RQggpjpIZQkgJAnl6zWZhYHt5OE3VLvfc1NRUNG/eHJcvX8aoUaP4FsOvCgoK0LFjR2hpacHd3b3C1zEMg7S0NIwdOxZBQUHYtGkTWrduXWZikpaWBi635M1zs2bNKlWZq3HjxpXeq6e2+PTpEy+BefbsGerXr18sgZGTk6twW4IedUxJScG+fftw8OBB5OXlYerUqfjrr7+goqLCtz4JIYT8RMkMIaSYyMRMga0rAICAxYOhptiwzHMePXqEgQMH4sWLF+jRo4fAYnF2doaDgwNcXV2hrKxc4VGTwsLCEm01atSoUolJ06ZNISlJyxbLEh0dzUtgnj59ivr168PCwgIjR46EhYVFpRKY31VnPdivJN/54MGJ9WjRokWJY9++fcPhw4exd+9eZGRkwMHBAStXruTb3kmEEEIomSGE/GajZxicQmL4OipTRILNgoO2CjZadSnzvPPnz2PSpEnIysqq1A1rXl5euRst/rruJDk5udQF8PXr16/QOhNZWVnMmTMHnz59wv3799G5c+dKf09IcdHR0bh27RquXLmC0NBQ1KtXDxYWFhg1ahTMzc3RoEEDvvX1a6W+yvy8S7BZkGSzMLe/ArZPt0LTpk0RGBgIRcXSq5llZ2fj+PHj2L17NxITEzFq1CisXr26UtMbCSGElI6SGUJIMXq7AhGTVnIndX5RkZfF3WX6ZZ6zZs0aXLhwAc+ePavUOpPMzMwSbUlJSZWZkMTExOCff/7BkSNHYGFhwUtSKio9PR16enpITU3F/fv30a5du0p/T+q6mJgYXgLz5MkT1KtXD+bm5hg1ahQsLCz4msD8riJ7KBUpOq6r1hyOtlpQaiaL8PBw6OnpQVFREXfu3CmzlHhubi7OnTuH7du3IyYmBtbW1lizZg369u0riLdGCCF1AiUzhBCerLxCaAl4l3QA2NWPg8z0Pycl0dHRKCgoKHEdm80udwH8718NGzYscwE8wzAYOHAgOBwOQkJCqrRYPiEhAbq6uuByubh//z5atWpV6TbqmtjYWF4CExISAhkZmWIJTMOGZU9F5LfIxEy4hMQiMCIJsak5xf4NsPBzQ0x9DUXY91cuMU3y3bt30NfXR6tWrXDnzh00a9aszL4KCgpw8eJFODo6IiIiAiYmJlizZg0GDx7M/zdGCCG1HCUzhBCesC8ZsDh4X+D9fDkzHwVJn9C0adNSExAnJyd07NgRixcvLvZ6kyZNBLIAPjAwEAYGBnB1dYWtrW2V2oiOjoaOjg6aNm2Ku3fvlntDWxfFxcXxEpjHjx9DRkYGZmZmGDVqFCwtLWs8gfmT7LxCRKdmI7+QC2lJNlTl5SAnU/b6prdv30JfXx/KysoICAhA06ZNy+2Hw+Hg+vXr2LZtG16/fg0dHR2sXbsWJiYmta4CHSGECAolM4QQnhex6bA9+lDg/ZwZ0wmDuyiXugCey+VCTk4Of//9NxYtWiTwWIqYmJggPj4er1+/rvLO9P/99x8GDx6MDh06ICAgQKDTo8RFUQJz9epVPHr0CNLS0sUSmNq0weTr16+hr6+P9u3bw9/fH02aNKnQdQzDwNvbG9u2bUNISAh69+6NtWvXwsrKqs5WryOEkIqiT0lCCI+0ZM18JLRQkP9jJa/Pnz8jNze3xis+OTo64t27d3BxcalyG506dYKvry/evXsHGxsb5Obm8jFC8fH582fs27cPgwYNgrKyMlauXAkFBQU4OzsjOTkZ7u7uGDduXK1KZACgW7duCAgIwMePHzF06FB8//69QtexWCwMGzYMjx49gr+/Pxo2bAhbW1t069YN//77LzgcjoAjJ4QQ8UXJDCGER1VeDoKe3ML6/37+JDw8HABqPJnp06cP7OzssGHDBuTn51e5nd69e8Pb2xsPHjzAmDFjSi3jXBvFx8dj//790NHRgZKSElasWAF5eXk4OTkhKSkJHh4eGD9+fK1LYH7Xs2dP+Pv74/379zAzMyu1KMWfsFgsGBkZITAwEPfv34eysjLGjRuHjh074vTp09X6uSSEkNqKkhlCCI+cjCSUm1W8kldVKMvLlrn+ICIiAlJSUlBVVRVoHKXZsmULYmNjcfLkyWq1M3jwYFy7dg03btzAlClTSt0wszb48uULDhw4AF1dXbRt2xbLly9H06ZNceHCBSQlJcHT0xP29vZo3LixsEOtUb1794afnx/evn0Lc3NzZGVlVbqNQYMGwcfHB8+ePUP37t0xbdo0qKmp4dChQ/jx44cAoiaEEPFEyQwhpBh9TUVIsAUzPiPBZkFfo/S9OIpERESgQ4cOQtlQsnPnznBwcMCWLVuQnZ1drbYsLCzg5OQEZ2dnLFq0CLVleeKXL19w8OBBDB48GG3btsWyZcvQuHFjnD9/HklJSfDy8oKDg0OdS2B+169fP9y6dQuvXr2CpaVllX+eevXqhWvXriEsLAx6enpYuHAh2rVrh127dlVq1IcQQmorSmYIIcWM11YWyIaZAMDhMrDvr1zmOeHh4ULdIX3jxo1IS0vDwYMHq93WmDFjcPToURw8eBAbNmzgQ3TC8fXrVxw6dIiXwCxduhQNGzbE2bNnkZSUBG9vb0yYMKHCC97riv79++PmzZt4+vQprKyskJNT9f2bOnfuDCcnJ0RERMDKygpr1qyBiooKNm3ahPT0dD5GTQgh4oWSGUJIMeotGkJXrTnfR2ck2CzoqjUvsUfH7yIiIqCpqcnXvitDVVUVM2fOxI4dO/hykzhz5kxs374dW7ZswT///MOHCGtGQkICDh8+DD09PbRp0waLFy9GgwYNcObMGSQmJuLGjRuYOHEiJTDlKJou9vjxY74UhejQoQNOnDiBjx8/wsHBAdu3b4eKigpWrlyJpKQkPkVNCCHig5IZQkgJjrZaYDNcvk6NkmSz4GirVeY5eXl5iI6OFurIDACsWbMG+fn52L17N1/aW7FiBVauXImlS5fizJkzfGlTEBISEnDkyBEMGTIErVu3xqJFiyArK4vTp08jMTERPj4+mDRpUoX2UCH/M3jwYNy4cQP379+Hra0t8vLyqt2mkpIS9u/fj+joaMyZMweHDx+GqqoqFi5ciM+fP/MhakIIEQ+UzBBCimEYBs7H9yPhxkG+bty32aoLlMopLvDx40cwDCP0ZKZly5ZYuHAh9u3bh4SEBL606ejoiFmzZmH69Om4du0aX9rkh8TERBw9ehT6+vpo06YNFi5ciPr16+PUqVNITEzEzZs3MXnyZNoEtJqGDBkCLy8vBAUFYfjw4XxJaACgRYsW2L59O2JiYrBixQo4OTmhffv2mDFjBj5+/MiXPgghRJRRMkMI4cnNzYWDgwNWr16NJTb9sdRInS/t1o8MwFCN8heEF5VlFuY0syLLly+HtLQ0HB0d+dIei8XCoUOHMHr0aIwbNw63bt3iS7tVkZSUhGPHjsHAwACtW7fG/PnzISMjgxMnTiAhIQE3b97ElClTKIHhM0NDQ3h4eCAgIACjR4/ma6nlZs2aYcOGDYiJicG2bdvg6ekJDQ0NODg44N27d3zrhxBCRA0lM4QQAD8Xeevp6eH69eu4dOkSNm3ahPmGGthupwUZSXal19BIsFmQkWRjXr+mSLh9DpaWluUugI6IiECjRo2gqFh2xbOa0LRpU/z11184duwYoqOj+dKmhIQEzp8/DxMTE9ja2uLBgwd8abcikpOTcfz4cRgaGqJVq1aYN28epKSkcPz4cSQkJMDX1xdTp06FvLx8jcVUF5mYmMDV1RU3b97E2LFjUVBQwNf2GzZsiOXLl+PTp0/Yv38/7t69i65du2L48OF4/vw5X/sihBBRwGJqS71QQkiVPX/+HNbW1uByufDw8ECfPn2KHY9Ly8FqtzcI/pACCTarzGpnRcd11ZrD0VYLSs1k8fjxYxgZGUFHRwceHh6QkZEp9dqpU6fi9evXCA0N5ev7q6rs7Gx06NABZmZmOHv2LN/a/fHjB8zMzPDy5UsEBQWhR48efGv7V8nJyXBzc8OVK1cQGBgIFosFAwMDjBw5Era2tmjevLlA+iXl8/LywvDhw2FrawsXFxeBlSLPz8+Hk5MTtm/fjg8fPsDMzAxr1qzBoEGDBNIfIYTUNEpmCKnjrl27hgkTJqBLly5wd3dHmzZt/nhuZGImXEJiERiRhNjUHPz64cHCzw0x9TUUYd9fuUTVstu3b8Pc3BzDhg3DpUuXSr1509HRgYqKClxcXPj07qrv0KFDWLhwId68eYPOnTvzrd3v37/DwMAAcXFxCA4O5ts6oZSUlGIJDMMwxRIYBQUFvvRDqs/d3R0jR47EyJEj4eTkBAkJCYH1VVhYiKtXr2Lbtm0ICwvDkCFDsGbNGhgaGvJ1bRwhhNQ4hhBSJ3G5XGbTpk0MAGbMmDFMTk5Opa7Pyi1g3sZ/Y57HpDFv478xWbkF5V7j7u7OSEhIMJMmTWI4HE6J4woKCsymTZsqFYeg5eXlMaqqqszw4cOLvV6V9/+75ORkplOnToySkhITExNT5RhTUlKYkydPMsbGxoyEhATDZrMZQ0ND5tixY0xiYmKV2yWCd+3aNUZCQoJxcHBgCgsLBd4fh8Nh3NzcmN69ezMAGG1tbcbT05PhcrkC75sQQgSBRmYIqYNycnIwefJkXLlyBZs3b8batWtr7Omsi4sLHBwcMH/+fOzbt4/Xb3p6Opo1a4Z///0XY8aMqZFYKur8+fOYNGkSXG8/wqushggMT0JsWikjU81koa+piPHaylBvUfZ+OkXi4+Oho6MDaWlpBAcHV3i9UGpqKtzd3XHlyhXcvn0bDMNgyJAhGDVqFGxtbUVi3RGpmCtXrmDs2LGYMGECTp8+DTZb8MtZGYaBn58ftm7divv376Nbt25Ys2YNhg8fLtARIkII4TdKZgipY+Lj42FtbY3//vsPFy5cwPDhw2s8hmPHjmH27NlYt24dNm/eDAAICQlB//798ezZM/Tq1avGYypLdHImDFaeAldRo0prhsrz4cMH6OjooFWrVggMDPzjRpRpaWnFEhgulws9PT1eAtOiRYuqvkUiZBcvXoSDgwOmTJmC48eP10hCU+TevXvYtm0b/Pz8oKmpiVWrVmHcuHGQkpKqsRgIIaSqKJkhpA4JDQ2FtbU1JCQk4OnpiZ49ewotlh07dmDlypXYvXs3li5dCicnJ0yYMAGZmZlo0KCB0OL63aXQWGzwDENBIQdcVHz0SoLNgiSbhU1WXTCmr3K557958wZ6enro0qULbt26BVnZn0lQeno6L4EJCAgAh8PhJTB2dnaUwNQiTk5OmDhxImbMmIGjR4/W+FqW0NBQbNu2DR4eHlBVVcWKFSswadIk1KtXr0bjIISQyqBkhpA64tKlS5g8eTK6d+8Od3d3tGzZUtghYfXq1fj7779x4sQJxMbG4syZM4iPjxd2WDyHAiOx2y+i2u0sM9HAPP3y9+wpqvrWv39/jB49Gm5ubvD39weHw8HgwYN5CYwo/L8jgnH27FlMmTIFc+fOxcGD/N24tqLevHkDR0dHXLlyBS1atMCyZcswc+ZMyMnJ1XgshBBSHkpmCKnluFwuNmzYgK1bt2L8+PE4deqUyDxpZRgG8+fPx5EjR6CtrY169eohMDBQ2GEB+Dkis9L1Dd/a22GnhdFljNCkp6fDw8MDR48exZMnTwAAurq6GD16NOzs7NCqVSu+xUJE28mTJzFjxgwsXLgQe/fuFVq1scjISGzfvh0XLlxAkyZNsGjRIsydO/eP0yAJIUQYaNNMQmqx7OxsjBw5Etu2bcPff/8NJycnkUlkAIDFYuHAgQOwt7fH48ePRSa2uLQcbPAM42ub6z3DEJdWfNPQb9++4fz587C0tESLFi0wZcoUyMjIYNq0aWCxWNDU1MScOXMokaljpk+fjqNHj2L//v1YtmwZhPXMUV1dHadPn8aHDx8wevRobNmyBSoqKli7di1SUlKEEhMhhPyORmYIqaXi4uJgZWWFyMhIuLi4wNraWtgh/VF+fj5kZWXBYrHg7++PIUOGCDUeh9MheBiVWuZC/8qSYLMwsL08Do3oCA8PD1y9ehW3bt1CYWEhBg0ahFGjRmH48OFo3bo1gP9VUFu2bBl27txJe4HUQYcOHcL8+fPx119/Yfv27UL/Gfj69Sv++ecfHD16FAzDYObMmVi2bBnvZ5YQQoSBkhlCaqHHjx/DxsYGMjIy8PLyQrdu3YQdUpk+f/4MJSUldO/eHR8/fsTt27fRr18/ocQSmZgJ4333BNZ+8rn5yEn4VCyB+dNGpQcOHMDChQuxbds2rF69WmAxEdG1b98+LF68GKtXr8bWrVuFntAAP8uC79+/HwcOHMCPHz8wZcoUrFixAqqqqsIOjRBSB9E0M0JqGScnJwwZMgRqamoIDQ0V+UQGAMLDwwEA586dg5aWFoYOHYq3b98KJRaXkFhIsAV0w8jlwHjOFsTFxeH+/ftYsGDBHxMZAFiwYAE2bdqENWvW4PDhw4KJiYi0RYsWYffu3XB0dMSmTZuEHQ4AQF5eHps3b0ZMTAw2btyIa9euQU1NDZMmTeL9WyaEkJpCyQwhtQSXy8XKlSsxYcIEjB07Frdv3xabjRMjIiIgKSmJLl26wMfHByoqKjA2NsaHDx9qPJbA8CS+Ti8rhi2B9Pqt0bZt2wpfsm7dOixevBjz5s2Ds7OzYOIiIm3p0qXYvn07Nm3ahC1btgg7HJ7GjRtj1apViI6Oxu7du+Hv749OnTph9OjRePXqlbDDI4TUEZTMEFILZGZmwtbWFjt37sTu3btx5swZyMjICDusCouIiED79u0hJSWFJk2a4NatW2jcuDGMjIzw+fPnGosjK68Qsb8t0ue32NQcZOcVVvh8FouFPXv2YPLkyZg0aRI8PT0FGB0RVStWrMDWrVuxfv16/P3338IOpxg5OTksWrQIUVFROHbsGEJDQ9GjRw9YWVkhJCRE2OERQmo5SmYIEXPR0dEYNGgQAgMD4eXlhaVLl4rEvPrKCA8Ph4aGBu/vioqK8Pf3B8MwMDY2RnJyco3EEZOaDUEvImQARKdmV+oaFouFEydOwMbGBqNGjRKZ8tWkZq1ZswYbN27E6tWrsWvXLmGHU4KMjAxmzJiBiIgIXLhwAZGRkejfvz+MjY0RFBQktKpshJDajZIZQsTY/fv30a9fP2RlZeHRo0ewsLAQdkhVEhERAU1NzWKvKSkpISAgAOnp6TA1NcW3b98EHkd+IVfgfVS1H0lJSbi4uEBPTw9WVla8vWhI3bJ+/XqsXbsWf/31F/bu3SvscEolKSkJBwcHhIWF4erVq0hJSYG+vj50dHTg4+NDSQ0hhK8omSFETJ07dw4GBgbo1KkTnjx5gi5dugg7pCrJz8/Hp0+fio3MFFFXV4e/vz+io6NhYWGB7OzKjWhUlrRkzXwkVrUfGRkZuLq6olu3bjAzMxNakQQiPCwWC5s3b8bKlSuxZMkSHDx4UNgh/RGbzcaIESPw/PlzeHt7g2EYWFhYoHfv3rh+/Tq43Jp5eEAIqd0omSFEzHA4HCxbtgyTJ0/GxIkT4e/vj+bNmws7rCqLiooCl8stNZkBAC0tLdy8eROvXr2Cra0t8vLyBBaLqrwcBD1Bj/X//VSVnJwcvL290bZtW5iYmCAqKop/wRGxwGKx4OjoiGXLlmHBggU4cuSIsEMqE4vFgoWFBR48eIA7d+6gadOmGDFiBLS0tODs7IzCwoqvISOEkN9RMkOIGPn+/TusrKywd+9e7N+/HydOnIC0tLSww6qWolKuv08z+5W2tja8vLxw7949jB07VmA3P3IyklBuJiuQtosoy8tCTkayWm00bdoUfn5+kJOTg5GREb58+cKn6Ii4YLFY2LlzJxYtWoS5c+fixIkTwg6pXCwWC/r6+rh9+zYePnyIdu3awcHBAZqamjh58qRAH1QQQmovSmYIERNRUVEYMGAAHjx4AB8fHyxYsEDsFvqXJiIiAg0aNEDLli3LPE9fXx9Xr16Fl5cXpk6dKrApKvqaigLbZ0aCzYK+Bn/KZbdo0QIBAQEoKCiAiYkJUlNT+dIuER8sFgv//PMP5s2bh5kzZ+LMmTPCDqnCBgwYAG9vb7x48QK9e/fGzJkz0aFDBxw4cAA5OYKtKEgIqV0omSFEDAQFBaFfv37Iz8/H48ePYWpqKuyQ+CYiIgIaGhoVSsyGDRuGCxcuwMnJCQsXLhTIQuLx2soC22eGw2Vg31+Zb+2pqKjA398fiYmJMDMzQ2ZmJt/aJuKBxWLhwIEDmD17NqZNm4bz588LO6RK6dGjB65cuYKwsDAYGhpiyZIlUFVVxY4dO/D9+3eB9JmdV4iwLxl4EZuOsC8ZlSqVTggRPSyGyooQItJOnjyJOXPmYPDgwbh69SqaNWsm7JD4avDgwWjTpg3+/fffCl9z4sQJzJw5E2vWrMHWrVv5HpPD6RA8jErla1IjwWZhYHt5OE3V5lubRZ4/fw59fX307t0bPj4+qFevHt/7IKKNy+Vi1qxZOHXqFC5cuAB7e3thh1QlUVFR2LlzJ86ePQtZWVksWLAACxcurPbnXmRiJlxCYhEYnoTYtJxiJdhZAJSbyUJfUxHjtZWh3qJhtfoihNQsSmYIEVGFhYVYunQp76nr/v37ISUlJeyw+K5ly5aYNWsWNm7cWKnrdu3ahb/++gs7d+7E8uXL+RpTXFoOjPbeRR4fSzXLSLIRsFgPSgJakxMcHAwTExOYmJjg2rVrtfJnhZSNy+XyRmdcXFwwZswYYYdUZfHx8di9ezeOHz8OCQkJzJ49G0uWLCl3Ourv4tJysNrtDYI/pECCzSrzAUXRcV215nC01RLYv1VCCH/RNDNCRNC3b99gYWGBw4cP4/Dhwzhy5EitvDnNyMhAYmLiHyuZlWX58uVYs2YN/vrrLxw/fpyvcSk1k8UmK/6Wut5s1UWgN0e6urq4fv06fHx8MGXKFCp7Wwex2WycPHkS9vb2sLe3x9WrV4UdUpW1adMGe/fuRUxMDBYsWIDjx4+jXbt2mD9/PmJjYyvUxqXQWBjtvYuHUT/Xk5U30lp0/GFUKoz23sWl0Ir1QwgRLkpmCBExRbtmh4aG4tatW5gzZ46wQxKYiIgIAKhSMgMAW7Zswfz58zF79uxKTVOriDF9lbHMpGpx/W65iSZG9+XfWpk/MTc3h7OzM1xcXLBgwQLanLAOkpCQwJkzZzB69GiMHTsWrq6uwg6pWhQUFLBt2zbExMRgzZo1uHjxIjp06ICpU6ciMjLyj9cdCozEStc3yCvkVnq6KIfLIK+Qi5Wub3Ao8M99EEJEAyUzhIiQ27dvQ1v755qKkJAQGBoaCjkiwapuMsNisbBv3z5MmDABDg4O8PLy4md4mKevju12WpAAFwyXU6lrJdgsyEiyscNOC3P11fgaV1lGjx6N48eP4/Dhw1i/fn2N9UtEh4SEBM6fP48RI0Zg9OjR8PDwEHZI1dakSROsXbsWMTEx2L59O3x8fNCxY0eMGzeuxOaxl0Jjsdsvgi/97vaLwGUaoSFEpFEyQ4iIOHLkCExNTdG3b188fvwY6urqwg5J4CIiItCyZUs0atSoym2w2WycOnUK1tbWGDlyJO7cucPHCAEDlXpIvbAIikw6AJRbtrko6RnYXh4Bi/VqZETmd9OnT8fOnTuxdetW7Nmzp8b7J8InKSkJZ2dn2NjYYOTIkfD29hZ2SHzRoEEDLF26FJ8+fcLBgwfx4MEDaGlpwdbWFk+fPkVcWg42eIbxtc/1nmGIS6Ny0YSIKioAQIiQFRQUYOHChTh69CgWLFiAPXv2QFKyepsqiouxY8fiy5cvuHv3brXbysvLg5WVFR48eFBshKu65syZg3///RcfPnxAWqH0z4pIEUmITS2lIpK8LLIiQsBE3MPzIB++9F8da9asgaOjI06ePIlp06YJOxwiBAUFBRg9ejRu3LgBDw8PDB06VNgh8VVBQQGcnZ3x999/IzIyEp3mHEFuY2Xws7q6ICsREkKqj5IZQoQoLS0No0aNwt27d3H48GHMmDFD2CHVqF69eqFPnz582708OzsbpqamePfuHYKCgtCtW7dqtffu3TtoaWlh165dWLJkSfG+8goRnZqN/EIupCXZUJWXg5yMJK5fv44RI0bg3bt36NSpU7X6ry6GYTBv3jwcPXoUly5dwqhRo4QaDxGO/Px8jBgxAn5+fvDy8oKxsbGwQ+I7DoeDQ07XsTdcTmB9BCweDDVFKttMiKihaWaECMn79++hra2NFy9ewN/fv84lMgzD8DbM5Bc5OTncuHEDqqqqMDExKXOBcEUsX74cqqqqmDt3bsm+ZCTRpXVj9FRuii6tG0NO5udomqWlJZo2bYoLFy5Uq29+YLFYOHjwIMaOHQt7e3v4+voKOyQiBNLS0rh69SoMDQ1hZWXF96mYokBCQgLpzbqUOw20yu2zWXB+TGtnCBFFlMwQIgS3bt1C//79IS0tjSdPnmDIkCHCDqnGff36FdnZ2XxNZgCgcePGuHXrFpo2bQojIyPExcVVqR1/f3/4+Phg586dkJGRqfB1MjIyGDNmDJycnMDhVK5ogCCw2WycO3cOQ4cOhZ2dHe7fvy/skIgQyMjI4Pr169DT04OlpSVfpnaKmsDwJL5udPsrDpdBYESSQNomhFQPJTOE1CCGYbB//36Ym5tj0KBBePToETp06CDssIQiPDwcQNUrmZVFQUEB/v7+YLFYMDIyQmJiYqWu53A4WLp0KXR0dGBnZ1fp/idOnIj4+HiReQIuJSWFy5cvQ1tbGxYWFnjx4oWwQyJ/kJ1XiLAvGXgRm46wLxnIzivkW9v16tWDm5sbBg0aBAsLCwQHB/OtbWHLyitErIAX6cem5vD1/wchhD/qxipjQkRAfn4+5s2bh5MnT2Lp0qXYsWMHJCQkhB2W0EREREBCQgLt27cXSPtt27ZFQEAAdHV1YWpqisDAQDRt2rRC1547dw5v3rxBSEgIWKzKT1vp168fNDQ0cP78eZFZn1C/fn14enrCwMAApqamCA4OhqamZpnX/GldEOGvyMTMn4UlwpMQm1ZKYYlmstDXVMR4bWWot6jemo369evDw8MDlpaWMDc3x61btzBw4MBqtSkKYlKzIegFwAyA6NRsdGndWMA9EUIqgwoAEFIDUlJSMGLECDx8+BDHjx/H5MmThR2S0C1duhSenp7VXtdSnrdv30JPTw+amprw8/NDgwYNyjw/KysL6urqMDAwgIuLS5X7dXR0xNatW5GYmIiGDUVn0XBKSgr09PSQmZmJ+/fvQ1m5eOnomryxruvi0nKw2u0Ngj+kQILNKnOKVNFxXbXmcLTVglIz2Wr1nZ2dDXNzc96aPX5V/xOWF7HpsD36UOD9uM0eiJ7KFXsoQgipGTTNjBABCwsLg7a2NsLCwnDnzh1KZP4fvxf//0nXrl3h6+uLN2/ewNbWFrm5uWWev3PnTqSnp8PR0bFa/drb2yM3NxfXrl2rVjv81rx5c/j5+UFCQgLGxsa8KXhxaTlwOB0C43334BQSg5jfEhng55PpmLQcOIXEwHjfPTicDqH9N6roUmgsjPbexcOoVAAod61H0fGHUakw2nsXl6q5kWNRsYxu3brBxMQEoaGh1WpP2KQla+Z25ltaCugZMCGihZIZQgToxo0bGDBgAGRlZREaGgodHR1hhyQywsPDy53mxC99+/aFl5cX7t+/j7Fjx6KwsPR5758/f8bu3buxePFiqKioVKtPZWVl6Ovr4/z589VqRxDatGmDgIAAfP/+Haampjhz971Qb6zrmkOBkVjp+gZ5hdxKL1jncBnkFXKx0vUNDgVWb1SzQYMGuHnzJrp06QITExM8f/68Wu0Jk1KTegLvg2EYGPbrhhYtWsDU1BQrV67ElStXEBkZCS6XK/D+CSGlo2SGEAFgGAZ79uzBsGHDoK+vj4cPH0JVVVXYYQld0eLm0KhkxGUxUFWrmWQGAIYMGYJr167B29sbU6ZMKfXmY+3atWjQoAFWrVrFlz4nTpyIu3fvIjo6mi/t8VOHDh3g7++PBPke2Oz7Ueg31nXFpdBY7PaL4Etbu/0icLmaiWTDhg1x8+ZNaGhowNjYGK9eveJLbDUhLy8PPj4+mDZtGjqotEVB+heB9te2sQxcr/yL2bNnQ0ZGBi4uLhg9ejQ0NDTQpEkT6OrqYuHChTh37hxevXqFgoICgcZDCPmJ1swQwmd5eXmYPXs2zp49i5UrV2Lbtm1gs+vuc4Oy1mAAgEoNr8G4fPkyxo4di9mzZ+PQoUO8Bf7Pnz9H7969cfToUcyaNYsvfWVlZaFly5ZYsWIF1q1bx5c2+elSaCxWur7hW3s77LQwuq9y+SfWUXFpOTDaexd5hfx7ii8jyUbAYr1qr6H59u0bjIyMEB0djcDAQGhpafEpQv7KysrCzZs34erqihs3biAzMxPq6uqws7PDtw7G8I/OE0h5Zgk2Cw7aKtho1aXY68nJyXjx4kWxr4iIn8mqtLQ0tLS00LNnT95Xt27dICcnuI09CamLKJkhhI+SkpIwfPhwPHnyBKdOnYKDg4OwQxIaYS5uLs/JkycxY8YMrFq1Co6OjmAYBgYGBkhKSsKrV68gKcm/il2TJk3CgwcPEBERUaXKaIIiyjfWtZXD6RA8jErl6822BJuFge3l4TS1+gv409LSYGRkhM+fPyMoKAidO3fmQ4TVl5qaCi8vL7i6usLPzw95eXno0aMH7OzsYGdnh86dO4PFYiEyMRPG++4JLI6AxYOhplj+A5fMzEy8evWKl9w8f/4cYWFhKCwsBJvNhqamJi+56dWrF3r06IFmzZoJLG5CajtKZgjhk9evX8PKygq5ublwc3PDgAEDhB2S0FwKjcUGzzAUcplK3bhJsFmQZLOwyaoLxgj4Cf+ePXuwbNkybN++HR07doSNjQ18fHxgZmbG137u3LkDQ0NDPHjwQKRK4Ir6jXVtIyo32uVJTU2FgYEBEhMTERQUhI4dO/IhusqLj4+Hu7s7XF1dcffuXXC5XAwcOBB2dnawtbVFu3btSr1OVH+u8/LyEBYWxktuXrx4gVevXiEn52cBDRUVlWIJTs+ePdG6dWuRegBCiKiiZIYQPvDw8MD48eOhrq4ODw+PEuVu65JDgZF8WROwzEQD8/TV+RDRn61fvx5btmyBoqIiunfvjlu3bvH95oHL5aJdu3YYOnQojh8/zte2q0pcbqxrk42eYXAKianRKVBVlZycDAMDA6SmpiIoKKhGqg4CQGRkJNzc3ODq6oqQkBBISkrCwMAAdnZ2sLa2RsuWLcttQ5xGHDkcDiIjI3nJTdFXWloagJ+b//6a3PTs2RMdOnSo09OWCSkNJTOEVAPDMNixYwdWr14NGxsbODk51en50OK2BoNhGOjr6+Pu3btwdHTk28L/361duxaHDh3C169fUb9+fYH0URnidGNdW+jtCkSMAMtYq8jL4u4yfb61l5SUBH19fXz79g13796Fmpoa39ouwjAMXr9+DVdXV7i6uuLt27eoX78+hg4dCjs7O1hYWFR4o9tfidvn0K8YhkFcXFyJBOfz588AfhZs6NGjR7F1OJ07d4aUlFSNxEeIKKJkhpAqys3NxfTp0+Hs7Ix169Zh48aNdfqJmTg9ES2Snp6ODh06QEFBAR8/fsT169dhbW3N937Cw8PRsWNHXLp0CaNHj+Z7+5UlbjfW4i4rrxBaG28JdId6FoC3G00hJ8O/9V4JCQkYMmQIsrOzcffuXbRv377abXK5XDx+/JiXwHz69AmNGzfGsGHDYGdnB1NTU8jKVv/fO79GiJebaGKuPv8Tucr6vdDA8+fPeRsO/15ooFevXujWrRtfvo+EiANKZgipgoSEBNja2uLly5c4e/YsxowZI+yQhE5U56qXZdmyZTh27BjCw8OxaNEieHp6wsfHB4aGhnzva8CAAWjWrBlu3LjB97YrQ1xvrEUJwzDgcDjgcDjgcrm8P//+VXTsfWIWZrnHCDyuG/N10KV1Y762+eXLFwwZMgR5eXm4e/dulUrMFxQUICgoCK6urnB3d0dCQgJatGgBGxsb2NraQl9fH9LS0nyNG6j+2r3NVl1EujpfZQsN9OzZs0ojXYSIOkpmCKmkFy9ewNraGoWFhXB3d0e/fv2EHZLQieMajI8fP6Jz585Yt24d1q5di/z8fFhbWyM4OBj+/v58L+Bw7NgxzJs3D58/f67Q3H9BCfuSAYuD9wXez7S2yVCQzKvwDX9ljhUWFvL+W1BQgIKCAt6ffz1W9PXrdb+3WfR3Lpdb4othGN5/f/2qLOlWGmg18R8BfJeLc5s9ED2V+X+zGh8fDz09PXC5XAQFBVVoTWBOTg78/Pzg6uoKLy8vfPv2DaqqqrwF/AMGDICEhATfY/1dZaoqMlwOWGyJGquqKAgVLTTw6zocKjRAxB0lM4RUwvXr1zFhwgR06tQJHh4eaNOmjbBDEgniuAZj5MiRePToESIiInjTMXJycjB06FC8efMGQUFB6N69O9/6S0tLQ6tWreDo6IilS5fyrd3KehGbDtujDwXez9fzS1CQEAkWi1XqF4Ayb6B+TRx+TSSKkgt+kZSUhISEBCQkJMBms3l/l5SULPFnSUlJSElJFftzaV/S0tK8LykpKeTINMNtKcE/9BDEyEyRuLg46Onpgc1mIygoCG3bti1xzrdv33Djxg24urrC19cXOTk56NKlC6+Ecvfu3YV208zb7yoiCbGpxfe7YgFo3Vga7+9cxzJrbayaO1koMQpKRQoN/JrcUKEBIm4omSGkAhiGwdatW7F+/XqMGjUKZ8+epfnIvxC3NRgPHjyAjo4Ozp8/jwkTJhQ7lpGRAQMDA3z+/BnBwcFVruTE4XCQn59f7Gv69OmIiorCtWvXir1eUFBQ4tzSXuPHufmyimCZr+bHt7FMOdfXQTIrodiNfdHNfUVeq+zrVTm3JkYGACA7rxBda8HUvpiYGOjp6UFaWhpBQUFo3bo1EhMT4eHhAVdXV9y5cwcFBQXo168fbwSmpiqhVUZ2XiGiU7ORX8iFtCQbqvJykJORxLBhw/D161c8ffpU2CEKXGULDfTq1QudOnWiQgNEJFEyQ0g5fvz4gSlTpuDSpUvYtGkT1q1bR0Pyv6ipNRiv1hlBisWt9o19bm4u9uzZg8LCQsyYMQOFhYUlzs3MzMTNmzdRUFCAgQMHQlJSstLJBZdb/UIILBYLMjIyfL2BZ0nVwyVO3///rgpGbV8zUxXilvD/SVRUFHR1dVFYWAhVVVWEhoaCxWJBT08PdnZ2sLGxKXXURhx4eXnBysoKT58+Re/evYUdjlCUVWhARkYGXbt2pUIDRORQMkNIGb58+QJra2uEhYXhwoULGDFihLBDEjk1tQbjy5n5KEj6VK02WCwWLzFp1KgR5OTk/pgEMAyD58+fg81mY8iQIWjUqFG1Ewk2m42xY8fC1NQUf/31V7kJiqBGDmrLjbU4EeRUTIbLgVT0I0zt0Qjjxo2DiooKf9tnGPz333+8CmQvXrwAADRo0ABbt27F+PHj0bx5c772KQxFCZqFhYXI7AklCn4tNFA0klNaoYFfp6pRoQFSkyiZIeQPnj59Cmtra7BYLHh6eqJXr17CDkkk1dQajGnK6WjfmF2tUYr8/Hx07NgRvXr1gpubW7l9RkVFQUdHBwoKCggKCuLLL+jFixfj4sWL+Pz5s9CmbIjjGidxJ+giGT3iveB37QJ+/PgBPT092NvbY8SIEWjSpEmV2mMYBk+fPuUlMBEREWjQoAEsLCxgZ2cHNTU1WFpaQl5eHnfu3IGCggJ/35CQbNy4EXv27MGXL1/QsCFt/PonRYUGfp2m9nuhgd/X4dT2QgN/mr5IBI+SGUJKcfnyZUyaNAndunWDu7s7WrVqJeyQRFZNjczwY3Hz9u3bsW7dOoSFhVV4Ln9YWBj09PSgrq4Of39/NGjQoFoxvHz5Ej179oSXlxcsLS2r1VZViWP1udpA0OXLMzMz4ebmBmdnZ9y+fRuSkj/Xgdjb28PMzAwyMjJltlVYWIj79+/D1dUVbm5u+Pz5M+Tl5WFtbQ1bW1sYGRmhXr16vPPfv3+PIUOGoEWLFrhz5w7k5eX59r6EJTY2Fu3atcOxY8cwffp0YYcjVsorNKCoqFgsuenVqxfat28v1oUGeIUlwpMQm1aysIRyM1noaypivLYy1FvQZ6KgUDJDyC+4XC42bdqEzZs3Y/z48Th58qRI7Nguyr5l/UCPbXcE2gc/1mAkJSVBTU0NkydPxv79+yt17dOnT2FgYIC+ffvixo0bxW7oqqJ79+7Q0NDA1atXq9VOdVju9sXb5HyAzb+pbILeF0jc1eTGsl++fMGlS5fg7OyMFy9eoGnTphg1ahTs7e0xaNAg3hPyvLw8BAQEwNXVFZ6enkhJSUGbNm14Fch0dHQgKfnnf3fv3r3DkCFD0KZNG9y+fRvNmjXj23sTFktLSyQkJNSJQgCCxjAMYmNjS6zDiY+PByC+hQYqU/K76Lg4l/wWdZTMEPL/srOzMXHiRFy/fh2Ojo5YuXJlrR4Sr46MjAz4+vrCw8MDPj4+kBuzC1JNWwusP36swZgzZw7+/fdffPjwoUpPkO/duwdTU1OYmJjg2rVr1fplu2fPHqxevRpfv36t8Zu/7OxsrFq1CkcvXEGbGccACf7dNPzpxpr8z6XQWKx0fcO39nbYaZW7sWNYWBhcXFzg4uKC2NhYKCsro0+fPvjx4wfu37+PzMxMqKurY/jw4bC1tUWfPn0q9bT87du30NfXh4qKCgICAqo8tU1UUCEAwfu10EDRSE5phQaKpqqJUqGB6m7GusmqC8aI8Gas4oiSGUIAfP78GVZWVoiIiICzszNsbGyEHZLIiY2NhaenJzw9PREUFISCggL07NkT1tbWSGqrB9+oHJFdg/Hu3TtoaWlh165dWLJkSZXb8fHxgbW1NUaPHo0LFy5UeXpEQkIC2rZti0OHDmHWrFlVjqey7t69iylTpuDr16/4+++/odjfBqvc3/Kt/YrcWBPgUGAkdvtFVLud5SaamKuvVqFzU1JS4OHhgdOnT+PJkyfgcDgAgNatW8PBwQGLFi2q1maur169goGBAdTU1ODn54fGjQWz301NKCoEYGlpiWPHjgk7nDqjqNDAr9PURK3QAL/+7S4z0cA8fXU+REQASmYIQUhICGxsbCAlJQUvLy++bpQozhiGwYsXL+Dh4QFPT0+8fPkSUlJS0NfXh5WVFaysrKCkpARA9NdgWFhY4P3793j37l256wbKc+XKFYwdOxYzZszAkSNHqjx6Z2FhgbS0NDx69Kha8VREdnY2Vq5ciUOHDkFHRwdnzpyBuvrPX6TV/eXMMAxYLBaWGKphgZEmv0Ku9ar6dJcFBtKSEths1aXcxPHz589wd3eHq6sr7t27By6Xi0GDBsHOzg5mZmYICwuDs7Mzbty4AS6XC2NjY9jb28PGxgZycnKVfk8vXryAgYEBOnbsiFu3bqFRo0aVbkNUUCEA0SBKhQaEMapKKoaSGVKnubi4YOrUqejduzdcXV3RokULYYckVHl5eQgKCuKNwHz+/BlNmjSBhYUFrKysMHTo0D/eoAh6cXNV+fv786aGDR8+nC9xnTlzBlOnTsWKFSuwffv2KrVx+fJljBkzBu/fv4empuCSgKCgIEydOpU3GjN//vwSI0rVmTbBBoPkm4cwsndbnDhxgqZmVkJV5t3nx77GjY326KmuVOp5ERERcHNzg6urK548eQJJSUkYGhrCzs4OVlZWpY6+pKWl4erVq3B2dsb9+/chJycHW1tbODg4wMDAoMw1M797+vQpjIyM0LVrV/j6+la7YIawUCEA0cXhcBAREVFiHU56ejoAwRQaqMn1bqTyKJkhdRKXy8XatWvx999/Y8KECThx4kS1n9iLq7S0NNy8eRMeHh7w9fVFZmYmVFVVYW1tDWtra+jo6FRofYgofthzOBz07NkTjRs3xr179/h6o71v3z4sXrwYjo6OWLVqVaWvz83NRcuWLTFv3jxs3bqVb3EVycrKwqpVq3Do0CHo6urizJkzUFP785Sk6ixove15BZMnT8aePXuqNY2vruJVRIpIQmxqKRWR5GWhr6EIy06NYaLdDaNHj8bRo0cB/BwZe/XqFa+EclhYGOrXrw8zMzPY2trC0tKyUmtYoqKicPHiRTg7OyM8PBwtW7bE2LFjYW9vj549e1bo39CTJ09gbGyMHj16/FxTV4VRHlFgaWmJxMREhIaGCjsUUo7fCw0UjeTwq9CAqD6sIz9RMkPqnKysLDg4OMDDwwM7duzAsmXL6tzT5KioKHh6esLDwwPBwcHgcDjo168fb/pY165dq/Q9EbVh+NOnT2PatGkICQlBv379+BZXkU2bNmHjxo04dOgQ5s6dW+nrZ86cCV9fX3z69Imv5UmDgoIwZcoUJCQkYPv27Zg3b16F26/ojbV9f+ViU/9WrVqFHTt2wMPDA8OGDePbe6lryturYu/evVi6dCnOnz+Ply9fws3NDZ8+fUKTJk0wbNgw2NnZwcTEpNqLpRmGwbNnz+Ds7Ix///0XSUlJ6NSpE+zt7TFu3DioqqqWef2jR49gYmKCvn37wtvbW2QWb1dGUSGAZ8+e0T5jYqqo0MCv09QqW2hA1KdRE0pmSB0TExMDKysrREVF4d9//xXaPh81jcvl4unTp7z1L2/fvoWMjAwMDQ1hbW0NS0tLtG7Nn2pkwljcXJqsrCyoq6vDwMAALi4u1Y6nNAzDYOnSpdi7dy8uXLgABweHSl3/8OFDDBo0CLdv34aBgUG148nKysLKlStx+PDhCo3GlKcym8BxuVyMGDECfn5+ePDgAa0947P8/HwEBQXh6tWrOHv2LDgcDlq0aAFbW1vY2tpiyJAhkJaWFkjfhYWFCAgIgLOzM9zc3JCTkwNdXV3Y29tj5MiRf1yEff/+fQwdOhQDBgyAp6en2JW5p0IAtVNFCg38ug7HL7Uxrr5MFNkCN4SSGVKHPHjwALa2tpCTk4OXlxe6du0q7JAE6sePH7hz5w48PT3h5eWFr1+/Ql5eHpaWlrCysoKJiYnA5rNXt3RlRRY3l2f9+vXYuXMnwsPDoaKiUq22ysIwDKZPn45z587h6tWrsLW1rdS1GhoaGDRoEM6dO1etOAIDAzF16lQkJiZi+/btmDt3bo1vRpednQ1dXV2kpKTgyZMn1aqORYCcnBzcunULrq6u8PLyQkZGBtq1awctLS14enrixo0bMDc3r9GYsrKy4O7uDicnJwQEBEBSUhKWlpawt7eHubl5iem6d+/ehZmZGQYPHgx3d/dq79FU0zZs2IB//vmHCgHUcnl5eXj79m2xdThFhQZazzwh8lsP1HWUzJA64fz585gxYwa0tbVx/fp1KCgoCDskgUhJSYG3tzc8PT1x69Yt5OTkQE1Njbf+ZcCAAZVazFsdlVmDwQYDLlh821Ts8+fP0NDQwKJFi+Do6FittiqCw+Fg3LhxcHd3h7e3N4yNjSt87ZYtW7Bjxw4kJCRUKbnMysrCihUrcOTIEQwePBhnzpxBhw4dKt0Ov8THx6Nv375QUlJCUFCQ2D2NF7Zv377B29sbrq6u8PX1xY8fP9C1a1fY2dnB1taWN+Klp6eH9PR0vHz5EhIS/Nv4tDK+fv3K25jz+fPnaNKkSbGNOYuS6Tt37sDCwgIGBgZwdXUVq/WJVAig7uJwOHgZ9h7D//2En5NsBYMfm0LXdZTMkFqNw+Fg1apV2LVrF6ZOnYojR44IbCqGsERERPDWvzx8+BAMw2DAgAG89S8dO3YU6pqg8tZgsHJS0Tw/ERc3zeLbvOGJEyfC19cXkZGRNVYeNj8/HzY2Nrh79y78/PwwaNCgCl0XHR2Ndu3a4fz585gwYUKl+hSF0ZjSPH36FIMHD4aVlRX+/fffOrcmrbISEhLg4eEBV1dX3LlzB4WFhdDW1uYlMEVltH/19OlT9O3bFydOnBCJm+x3797BxcUFzs7OiI2NhYqKCsaPHw97e3t06tQJ/v7+GDZsGK+yoDh9DlMhgLor7EsGLA7eF3g/N+broEtr8d2bSdgomSG11vfv3zF+/Hj4+Phgz549WLhwYa24qeJwOHj8+DEvgQkPD0f9+vVhbGwMa2trWFhYiGyJ6dLWYBzcuxtbtmxBUlISX6oePX/+HL1798bRo0drdENK4Oe0IDMzM7x69QqBgYHo2bNnha4bMmQIJCQkcPv27QqdL2qjMaW5fv06RowYgQ0bNmDjxo3CDkfkfPr0iVdC+eHDh2Cz2dDT04OtrS1sbGzQtm3bcttwcHCAn58fPnz4IDJToLhcLh48eABnZ2dcuXIF3759Q69evWBvb48WLVpg8uTJMDc3x5UrVypcSUrYPD09YW1tTYUA6qAXsemwPfpQ4P24zR6Inso1uwFobULJDKmVoqKiYGVlhbi4OFy6dAlmZmbCDqlasrOzERAQAA8PD3h7eyM5ORmKiooYNmwYrKysYGRkJJbVggAgMjISGhoafNkHhmEYGBgYICkpCa9evaqxKXW/+v79OwwNDRETE4N79+6hY8eO5V5z9uxZTJ06FdHR0VBWLnutUGBgIKZMmYKkpCTs2LEDc+bMEYnRmNI4OjpizZo1uHjxIsaOHSvscISKYRi8e/eOV0L55cuXkJGRgYmJCWxtbTFs2DA0b968Um3GxcVBQ0MDS5cuFUh57+rKzc2Fj48PnJ2d4e3tDQ6Hgx49euDVq1ewtLTE1atXxSKhoUIAdU/RXjbe91/icJTgR/dpZKZ6KJkhtc69e/dgZ2eHJk2awMvLC506dRJ2SFWSkJAAb29veHh4ICAgALm5uejUqROsra1hZWWFfv36CW2uPL/16NEDnTp1wr///lutdjw8PGBjYwMfHx+hJrCpqakYPHgwvn//jvv375dbgCAzMxMtWrTA2rVrsXr16lLPycrKwl9//YWjR49CT08Pp0+fFrnRmN8xDIOJEyfiypUrCAoKQv/+/YUdUo0qqiLo6uoKNzc3REREoEGDBrC0tIStrS3MzMyqPaKydu1a7NmzB+Hh4eUmwsKUlpaGa9euwdnZGcHBwQB+7t5++PBhmJqaCuXBQ2UUFQL4+vWr2G4ESkrH5XIRERGBZ8+e4enTp3j27BlevHiBrKwssKTqQXnJVUCAszpozUz1UTJDapVTp05h9uzZ0NXVxdWrVyEvLy/skCqMYRj8999/vPLJISEhYLFY0NHR4a1/KW3ufG2wdetW7NixA0lJSVVeMJ6fn4+uXbtCVVUVt27dEvqUwi9fvkBXVxdsNhvBwcHlVvayt7dHaGgo3r9/XyL2O3fuYOrUqUhKSsLOnTsxe/ZskR2N+V1eXh4MDQ0RGRmJJ0+eCLSynCgoLCxEcHAwL4GJj49H8+bNYW1tDVtbWxgaGvK1oldmZibU1dVhZGQEZ2dnvrUrSNHR0bwROwBQUFDAuHHjYG9vj969ewv9325pqBBA7cDlchEZGVkiccnMzAQAtG/fHr1790afPn3Qu3dv9OrVCzanXiImLUdgMVE1s+qjZIbUCoWFhVi+fDn27duHWbNm4cCBA2IzfeHBgwe89S8fP36EnJwcTE1NYW1tDXNz80pPPRFH79+/R6dOneDu7g5ra+sqtXHgwAEsXrwYL1++hJaWFp8jrJpPnz5BR0cHzZo1w927d9GsWbM/nuvv7w8TExM8fvwY2to/d4TOzMzEihUreKMxZ86cQfv27WsqfL5JTk6GtrY25OTk8PDhQ5FZ38Evubm5CAgIgKurKzw9PZGamoq2bdvyFvDr6OgIdOTh5MmTmDFjBp48eYK+ffsKrB9+u3LlCsaNGwcNDQ2kpqYiKSkJHTt25G3M2a5dO2GHWAwVAhAvXC4XHz58KJa4PH/+nJe4tGvXrkTiUtpn9EbPMDiFxNA+MyKMkhki9jIyMjBmzBj4+/tj//79mDNnjkg+2SuSmZmJW7du8faJSEtLQ6tWrWBlZQVra2vo6+uL3V4M/FC0E7OTk1Olr01PT4eamhrs7Oxw8uRJAURXde/evcPgwYPRoUMHBAQE/PFGnsPhQEVFBVZWVjhy5Aju3LmDKVOmIDk5WexGY0oTFhaGgQMHQldXFx4eHmI/RTIzMxM+Pj5wdXWFj48PsrKyoKGhgeHDh8PW1hZ9+vSpsc8hDoeDnj17onHjxrh3755If/797tKlSxg/fjwcHBwwevRoXLx4Ea6ursjJyYGOjg5vY86yHgTUFCoEILq4XC4+fvxYInH5/v07AEBVVbVE4lLRmRuRiZkw3ndPYLEHLB7Mt0qedRUlM0SsffjwAcOGDUNCQgKuXLlSqf09alJ8fDy8vLzg4eGBO3fuID8/H1paWrz1L7179xbrG1V+2LhxI/bu3YukpKRK70OxbNkyHDt2DB8+fBDJjRqfPXsGAwMD9O7dGz4+Pn9MVletWoVjx45h5MiROHnyJIYMGYLTp0+L5WhMaW7dugVzc3MsXLgQ//zzj7DDqbSUlBR4enrC1dUV/v7+yM/PR69evXgjMJ06dRJaIlE0snf9+nXY2dkJJYaqcnFxgYODA6ZOnYrjx48jJycHHh4ecHZ2hp+fHyQkJGBhYQF7e3tYWFgI7WEPFQIQDQzDlJq4ZGRkAPi5FuvXxKV3797VnnLucDoED6NS+To6I8FmYWB7eThN1eZbm3UVJTNEbN25cwcjRoyAgoICvLy8oKGhIeyQeBiGwZs3b3jrX54+fQoJCQno6enx1r+I2hQKYXv79i20tLTg7e0NCwuLCl/38eNHdOrUCevXr8fatWsFGGH1BAcHw9TUFEZGRrh+/Xqp0yDPnDmDqVOnQkZGBnv27BH70ZjSHDp0CPPnz8fx48cxY8YMYYdTrri4OLi7u8PV1RX37t0DwzDQ0dGBra0tbG1toaqqKuwQeczNzREREYF3796J1T4uwM+NjSdPnoyZM2fiyJEjvKQwISGBtzHns2fP0KRJE4wcORL29vbQ0dGp8X8fVAigZjEMg6ioqBKJy7dv3wAAysrKJRIXQUzNjkvLgdHeu8gr5PKtTRlJNgIW61V7k2hCyQwRU8eOHcP8+fOhr6+Py5cvo2lT4ddnLygowL1793gJTExMDBo1agQzMzNYWVnBzMxMJOIUVQzDoFOnThgwYADOnj1b4etGjhyJR48eISIiQuTLU/v6+sLKygojRoyAk5MTb6pVZmYmli9fjuPHj6Nhw4bo168fAgIChByt4MybNw/Hjx+Hr68vDA0NhR1OCREREbwF/E+ePIGUlBQMDQ1hZ2cHKysrkd3H6d27d+jWrRt27dqFxYsXCzucSitK5ufNm4cDBw6UGOX677//eBtzxsTEQFlZmbcxZ+fOnWskRioEIDgMw+DTp08lEpf09HQAgJKSUonERUFBocbiuxQai5Wub/jW3g47LYzuK7oVCMUJJTNErBQWFmLRokU4fPgw5s+fj3/++UeoJT0zMjJw8+ZNeHp6wsfHBxkZGVBSUuKtf9HT0xO7J6TCtHbtWhw+fBiJiYkV+r49ePAAOjo6uHDhAhwcHGogwuq7fv06Ro0ahWnTpuHYsWO4ffs2pk6ditTUVOzcuRNcLheLFy9GfHw8FBUVhR2uQBQWFsLCwgJPnjzB48ePoampKdR4GIbBy5cveQlMWFgYZGVlYWZmBltbW1hYWKBJkyZCjbGiZs+ejUuXLuHDhw9iVc2xyIkTJzBz5kwsWrQI//zzT6nT9rhcLh4+fMjbmDM9PR09e/aEvb09xo4di1atWgk0RioEUH0MwyA6OrpY4vLs2TNe4tK2bdsSiYsofB4eCozEbr+Iarez3EQTc/XV+BARASiZIWIkPT0do0aNQlBQEA4dOoSZM2cKJY6YmBje+pegoCAUFhaiV69evOljPXr0EKsFuKLk5cuX6NmzJ3x9fWFqalrmuVwuFwMGDACHw8GTJ0/EajrWuXPnMHnyZHTv3h2vXr2Cvr4+Tp8+jXbt2iE1NRWtWrXCzp07sWjRImGHKjAZGRkYMGAACgoK8Pjx4xq/8eZwOHj06BEvgYmOjkaTJk1gZWUFW1tbmJiYiPxIX2mSkpKgpqaGKVOmYN++fcIOp0qOHj2KOXPmYNmyZdi5c2eZn6d5eXm4efMmnJ2d4eXlhcLCQhgaGsLBwQG2trYCmQpGhQAqh2EYxMTElEhc0tLSAABt2rQpkbiI6ugn8HOEZoNnGAq5TKXW0EiwWZBks7DZqguNyPAZJTNELISHh2PYsGFITU3FtWvXoK9fczXZGYbB8+fPeeWTX716BSkpKejr68Pa2hrDhg2DkpJSjcVTmzEMA3V1dejr65dblezff//FuHHjEBQUBD09vRqKkD8CAgIwatQopKenY9iwYXB3dy+WjNnZ2SE6OhrPnz8XYpSCFxUVhX79+kFLSwu3bt0S+Chmfn4+AgMD4erqCg8PDyQmJqJly5a89S9DhgwRi5Lu5dm+fTvWrVuHsLAwkVpLWBkHDx7EggULsHLlSjg6OlboAVF6ejpvY8579+5BVlYWNjY2sLe3h7GxMd9G8akQwJ8xDIPY2NgSiUtqaioAoHXr1iUSF1Es2lKeuLQcrHZ7g+APKZBgs8pMaoqO66o1h6OtFq2REQBKZojI8/Pzw6hRo9C6dWt4enpCTU3wQ7N5eXkICgrirX+Jj49HkyZNYGFhASsrKwwdOhSNGjUSeBx10cqVK3Hq1CkkJCT88ebjx48f6NixI3r16gU3N7cajrDqvn//juXLl+PEiRPQ19dHjx49sHfvXuzfvx8LFizgnefh4QEbGxu8fv1aZPbMEZT79+/D0NAQ9vb2OHXqFN9HNbOzs3Hr1i24urrC29sbGRkZaN++Pa8CWf/+/cVqVK8icnNz0bFjR/To0QPu7u7CDqfK9u7diyVLlmDdunXYvHlzpa6NiYnBxYsX4eTkhP/++w+KiooYM2YM7O3t+VI2mwoB/Exc4uLieAlLUfKSkpICAGjZsiX69OlTLHER9BTAmhaZmAmXkFgERiQhJjUbwP9+rlgAlOVloa+hCPv+ylR+WZAYQkQUl8tlDhw4wEhISDBmZmbMt2/fBNpfamoq4+TkxIwYMYJp0KABA4BRVVVlFi5cyNy5c4fJz88XaP/kp9DQUAYAExAQ8Mdz/v77b0ZSUpIJDw+vwciqx8/Pj1FWVmbk5OSYI0eOMBwOh+FyuczSpUsZAMzZs2d55+bl5THy8vLM0qVLhRdwDTp//jwDgNm1axdf2ktLS2MuXLjA2NjYMPXr12cAMFpaWsyGDRuYly9fMlwuly/9iLKLFy8yAJjAwEBhh1Itu3btYgAwmzZtqtL1XC6Xef78ObNkyRKmZcuWDABGQ0OD2bx5M/Px48cqxxUTE8Ow2Wzm5MmTVW5DnHC5XCY2NpZxc3Nj1q5dywwdOpRRUFBgADAAmBYtWjAWFhbM+vXrGU9PTyY+Pl7YIde4oyfPMFKK7ZhL/o8ZKcV2zJ17D4QdUp1ByQwRSfn5+czMmTMZAMzixYuZwsJCgfTz8eNH5p9//mGGDBnCSEhIMACYfv36MVu3bmVev35dJ256RA2Xy2VUVFSYWbNmlXo8MTGRadiwIbNgwYIajqxqMjIymBkzZjAAGAMDA+bTp0/FjnO5XGb69OkMm81mrl27xnt9/vz5TMuWLZmCgoIajlg4Vq1axbBYLMbDw6NK13/9+pU5evQoY2JiwkhKSjIAmP79+zM7d+5kIiMj+Ryt6ONyuYy2tjbTq1cvhsPhCDucavn7778ZAMzWrVur1U5BQQFz69YtxsHBgZGTk2MAMIMGDWKOHj3KpKSkVLo9CwsLpk+fPtWKSRRxuVwmLi6OcXd3Z9atW8eYmZkxioqKvMRFUVGRMTc3Z9atW8d4eHgwnz9/pt+VDMPs37+fqV+/PpOWlsYAYC5fvizskOoMSmaIQGTlFjBv478xz2PSmLfx35is3IrfkKWkpDBDhgxhpKSkmFOnTvE1Lg6Hwzx+/JhZvXo106VLFwYAIyMjw5ibmzPHjx+vk0+TRNHSpUsZRUXFUpPYWbNmMU2aNKnSzUdNKxqNadCgAXP06NE/3lQWFhYyo0ePZqSkpBhfX1+GYRjm6dOnDADGx8enJkMWGg6Hw9jZ2TFycnLMixcvKnTNx48fmd27dzODBg1iWCwWIyEhwRgaGjKHDh1iPn/+LNiAxcCDBw8YAMz58+eFHUq1bdmyhQHAbN++nS/tZWVlMS4uLoyZmRkjISHBSElJMdbW1szVq1eZHz9+VKgNDw8PBgDz7NkzvsQkLPHx8YyHhwezfv16xtzcnGnRogUvcVFQUGDMzMyYtWvXMu7u7kxcXBwlLn+wbds2RkFBgeFyuUyDBg2Y3bt3CzukOoOSGcI3EQnfmQ0eb5nBO+8wqiu9GZVfvlRXejODd95hNni8ZSISvv+xjbCwMKZ9+/ZM8+bNmbt37/IlrpycHMbLy4uZPn06b5qBvLw8M3HiROb69etMZmYmX/oh/PPo0SMGQImfgbCwMIbNZjN79uwRUmQVk5GRwUyfPv2PozGlyc/PZywsLJj69eszwcHBDJfLZbp06cKMHj1a8AGLiKysLKZXr15M27ZtmS9fvpQ4zuVymTdv3jCbNm1ievTowXsYYWVlxZw9e1YsEtyaNnLkSKZNmzZMdna2sEOptg0bNjAA+H6TmJCQwOzfv5/p27cvA4Bp3LgxM3XqVCYwMLDMUa2CggKmTZs2fxxFFkXx8fGMp6cns2HDBsbCwoL3O7EocRk6dCizZs0axs3NjYmNjaXEpRJWrVrFtGvXjmEYhunUqROzcOFC4QZUh1ABAFJt/KrqcfPmTYwZMwZKSkrw8vJCu3btqhxTcnIybty4AQ8PD/j5+SEnJwdqamqwtraGtbU1BgwYINT9aUjZuFwuVFRUYGtriwMHDvBet7CwQHh4OMLCwiAjIyPECP/Mz88P06ZNQ3p6Onbv3o0ZM2ZUeLHxjx8/YG5ujufPnyMwMBABAQFYv349EhISxGafk+qKj49Hv3790KZNG9y9excyMjIIDQ3llVCOjIxEw4YNYWlpCVtbW5iZmdXZBdgVERUVhU6dOmHt2rVYt26dsMOpFoZhsG7dOmzbtg379u3DwoUL+d5HeHg4nJ2d4ezsjOjoaCgpKfE25uzSpUuJ8zds2IC9e/fiy5cvIvdz+PXr1xKL879+/QoAaN68OW9RftECfSUlJdpWoBrmz5+Pe/fu4dWrVzA1NYWcnBxcXV2FHVadQMkMqZbq1lvfZNUFo/soYd++fVi2bBnMzc3h4uJSpUph4eHh8PT0hKenJx4+fAiGYTBgwADeBpaampr0QS1GFi1ahKtXryIuLg5sNhv+/v4wMTHBtWvXMHz4cGGHV8L379+xdOlSnDp1CoaGhjh16hRUVVUr3U5mZiYMDQ3x6dMnXLt2DQYGBnVut/EnT55g8ODBaNu2LXJzcxEfH4/mzZvDxsYGtra2MDQ0FNlkVhQtX74cR48eRWRkpNhXk2IYBqtWrcKOHTtw6NAhzJ07t9xrsvMKEZ2ajfxCLqQl2VCVl4OcTNkPsxiG4W3MefnyZaSnp6NHjx68jTlbt24NAIiNjYWqqipOnDiBadOm8eU9VkVCQkKJxOXLly8AAHl5+RKJi7KyMv0+5LPJkycjIiICDx48wLRp0/Dq1SvaWLWGUDJDqoxfO+G2zwpD4KEV+Ouvv+Do6AgJCYkKXcfhcPD48WNe+eTw8HDUr18fxsbGsLa2hoWFhUhvvEXKdv/+fejq6uLBgwfQ1tZGz5490bhxY9y7d0/kfgnfunUL06dPr9JoTGlSU1MxZMgQpKeno0OHDuBwOLh//z4fIxY9ubm58Pf3h6urKzw9PXkb6vXr1w87d+7EoEGDaDS1ir59+wY1NTXY2Njg1KlTwg6n2hiGwfLly7Fnzx4cPXoUs2bNKnEOr2RueBJi03Lw640OC4ByM1noaypivLYy1FuUXTI3Pz+/2Mac+fn5vHLidnZ2GDNmDJKSkmrsxjUxMbFE4hIfHw8AaNasWYnERUVFReQ+M2ujkSNH4vv377h16xY2b96MI0eOICEhQdhh1QmUzJAquRQai5Wub/jWnlXLTBxYOKbc87Kzs+Hv7w9PT094e3sjOTkZioqKGDZsGKytrWFoaCiWu3aTkrhcLtq2bYsxY8agS5cumDZtGkJCQtCvXz9hh8aTkZGBZcuW4dSpUzAyMsKpU6egoqLCl7a/fv0KXV1dZGZmIikpCZGRkTWyx1JN+v79O3x8fODm5gYfHx9kZWVBU1MTdnZ2sLOzg7+/P1avXg1nZ2eMHz9e2OGKtUOHDmHBggV48eIFunfvLuxwqo1hGCxevBj79+/HyZMneaMigt7M8Nu3b7h+/TqcnJxw9+5d1K9fH3369EFwcLBAPp+SkpJKJC6fP38GADRt2rRE4qKqqkqJi5CYmZlBVlYW169fx9mzZzFlyhTk5ubSKHINoGSGVFpcWg6M9t5FXiGXb23KSLIRsFiv1F8mCQkJ8PLygqenJwICApCbm4tOnTrB2toaVlZW0NbWrnWb3pGf5s2bBw8PDxQUFMDQ0BAuLi7CDonn1q1bmDZtGr59+4Y9e/Zg+vTpfL+JiI6OxsCBA5GYmIglS5Zg165dfG1fGJKTk+Hp6Qk3Nzf4+/sjPz8fvXv3hq2tLezs7NCpUyfeuQzDYPLkyfj3338RGBiIgQMHCjFy8VZQUAAtLS20bdsW/v7+teKGl2EYzJ8/H0eOHMHp06dRv6thtac9j+mrXOHrYmNjcfHiRVy4cAH//fcf6tWrh+nTp8Pe3h59+/at9Pc4OTm5ROISFxcHAGjSpEmJxKVdu3a14v9jbaGrq4v27dvj/PnzCAgIgLGxMT58+IAOHToIO7Raj5IZUmkOp0PwMCq1Ur8syiPBZmFge3k4TdUGwzB49+4dPD094eHhgZCQELDZbOjo6MDKygpWVlZQV1fnW99EdAUFBUFfXx9SUlKIjIzk26hHdWRkZGDp0qU4ffo030djSvP+/Xv06NEDDMMgKSkJjRs3LvP8qqwPELS4uDi4ubnB1dUVwcHBYBgGurq6sLW1ha2tbZnfv7y8PBgbG+P9+/d48uRJldYhkZ+8vLxgZWUFb29vWFhYCDscvmAYBnPmzMG/r9PRZLBDtdtbZqKBefqV+/3CMAxmzpyJ8+fPo2nTpkhMTIS6ujrs7e0xfvz4Um9mU1JSSiQusbGxAIDGjRuXSFzat29PiYuI69mzJwYOHIjDhw8jPDwcHTt2RGBgIIYMGSLs0Go9SmZIpUQmZsJ43z2BtW+U/whBHpfw8eNHyMnJYejQobCysoK5uTmaN28usH6JaIqJiYGqqiq0tbXx+PFjYYcDX19fTJ8+HRkZGdizZw+mTZtWIzcYp06dwvTp09G9e3c8evQI9evXL3acn+sD+CU8PJxXgSw0NBRSUlIwMjKCra0trKysKrWeLSUlBdra2pCVlcWDBw+qVCCE/LzpNjIywpcvX/D69WtISUkJOyS++DckBqvc3/KtvR12WhhdiREa4H+FAI4dO4b27dvDyckJrq6uyMrKQp8+fdC/f380btwY//33H549e4aYmBgAPxOXXr16FUtcOnToQImLGFJTU8OIESOwfft25OTkQE5ODhcuXICDQ/WTbFI2SmZIpWz0DINTSAxfR2WKMFwOuOFBsGiZA2tra+jr66NevXp874eIj4kTJ+LKlSto0aIFPn36JLRf8BkZGViyZAnOnDkDY2NjnDp1CsrKlbvZqY6i9UNJSUkYOnQoXF1dIS0tLfD1AZXBMAxevHjBS2DevXsHWVlZmJubw9bWFhYWFuWOKpXlv//+w4ABAzBw4EB4enpSMYAqevnyJXr16oVDhw5hzpw5wg6n2mp62nNZLCws8PXrV+zYsQPPnj3D48eP8fDhQyQnJ/POkZeXx4ABAzBy5EgMGDAAHTp0oGnStUTLli0xb948rF27FsDP8tdLlizB6tWrhRxZ7UfJDKkUvV2BiEnLEVj7Ks1kcXe5vsDaJ+Lj2bNn6NOnDxYsWIADBw7g2bNn6NWrV43HcfPmTcyYMaPGR2N+t3HjRuzatQsFBQWws7OD1eLt2OT9X42tDygNh8PBw4cPeQlMTEwMmjZtimHDhsHOzg4mJiYlRpGqw8/PD+bm5pg3bx727dvHt3brmilTpsDLywsfPnyoVoIpCgQ97bks6enpxaaKBQcHIzExEQDQsGHDYiMuqqqqePLkCS5evIgnT56gUaNGGDFiBOzt7aGnp0cJTS3QoEEDbN26FYsWLQLwc9qZtrY2jh07JtzA6gBKZkiFZeUVQmvjLQjyB4YF4O1GU6HP8SfCxTAM9PX1kZycjKdPn0JJSQkzZsyAo6NjjcXw7ds3LF26VGijMb+LiopChw4dsHDhQpwLTRDa+oD8/HzcuXMHrq6u8PDwQFJSElq1agUbGxvY2dlBT09PoNOXjhw5grlz5/6xJC8p35cvX6Curo558+Zhx44dwg6nygQ97Tlg8WCoKf6clpmeno7nz58XW+MSFRUF4OdNbK9evdCzZ0+cP38eQ4cOhYuLyx8TlIiICLi4uMDZ2RlRUVFo27Ytxo0bB3t7e2hpaQns/RDB4XK5kJCQKFZZz8rKChwOBzdu3BBydLUf3TGSCotJzRZoIgMADIDo1Gx0aS3eTwtJ9Xh6euLu3bvw8fFB/fr1YWNjg6tXr2Lbtm01Mipy8+ZNTJ8+Hd+/f+dthifsOezt27eHjo4OHiWx+ZLIAMBuvwgoNJApd31AdnY2fH194ebmBm9vb2RkZKB9+/aYOHEibG1ta7Si4Jw5c/D+/XvMmzcPampqMDIyqpF+a5PWrVvz9vWaNWsW2rVrJ+yQqsQlJLbc6ZVVxWYBi4+4Qfa/G3j27Bk+fvwIAJCTk0OvXr1gZWXFW+OioaHB+/lv1KgR9u3bh5ycHDRo0KDUtjU0NLBp0yZs3LgRjx49grOzM06dOoWdO3eiW7dusLe3x7hx49CmTRu+vy8iGDk5P2es/Pr/XFlZGffuCS7ZJv9DIzOkwl7EpsP26EOB9+M2eyB6KjcVeD9ENOXn56Nr165o164dfH19wWKx4OvrCzMzM7x69QrdunUTWN/fvn3DkiVLcPbsWZiYmODkyZNCHY353a6jZ3HwY2Owpfi3b8Gf1gekp6fDy8sLbm5u8PX1RW5uLrp168YroaylpSW0BK+wsBCWlpZ4/PgxHj9+jI4dOwolDnGWnZ0NDQ0N6Ojo4PLly8IOp0oEPe258NtXtHt7vtjifA0NjTI3di4qBFD0EKSi8vPz4evrC2dnZ3h6eiI/Px8GBga8jTmp6IVoS0hIQKtWreDl5QVLS0sAwI4dO/D333/j27dvwg2uDqBJmqTCpCVr5selpvohounYsWP4+PEjdu/ezbtZNjAwQJMmTXDt2jWB9Xvz5k107doV165dw8mTJ+Hr6ytSiQwAPGNrgCXB3wH1Qi6D1W4/N8D9+vUrjh49ChMTEygqKmLixIlITEzE5s2bERkZiVevXmHjxo3o1q2bUEeqJCUlcfnyZbRp0waWlpZITU0VWiziSk5ODtu2bcOVK1fw6NEjYYdTaVl5hYgVYCIDAFJNWsE3IBD79u2Dvb09OnXqVGYiA/x8Gm9mZoYTJ05Uqi9paWlYWVnhypUrSExMxKlTp8DlcjFlyhS0aNECY8aMgbe3NwoKCqrzlqokO68QYV8y8CI2HWFfMpCdV1jjMYi6zMxMAD/XShVRUlJCRkYG7xgRHBqZIRWWnVeIrrRmhghQeno61NTUYGdnh5MnTxY7NmnSJDx58gTv3r3ja5+/jsaYmprixIkTIpfEAIJfH6AYehxP73iDzWZDX18ftra2sLGxQevWrQXWZ3V9+vQJ/fr1Q+fOneHv7w9paWlhhyRWuFwu+vTpAxkZGTx8+FDoUykrI+xLBiwO3hd4Pzfm61R62rOHhwdsbGzw/Plz9OzZs1r9x8XF4eLFi3ByckJYWBiaN2+O0aNHw97eHtra2gL7fyaK5d5F2YsXL9CrVy88ffoUvXv3BgAEBwdj8ODBCAsLQ+fOnYUcYe1Gj8BJhcnJSEKZz+Vcf6csL0uJTB22detW5OXlYcuWLSWOjRgxAv/99x9fkxkfHx907doV169fx6lTp3Dz5k2RTGSA/60PEASGywG3wyCcPXsWSUlJ8Pf3x5w5c0Q6kQGAdu3awd3dHY8fP8asWbNAz+Yqh81mY8+ePXj8+DGuXLki7HAqJZ+PpZj53Y+FhQVat25d6dGZ0igpKWHFihV48+YNXr58iUmTJsHNzQ0DBgzgrb358OFDtfspEpeWA4fTITDedw9OITGI+S2RAX6ubY1Jy4FTSAyM992Dw+kQxAl4lEzUZWVlASi5Zgb4mZASwaJkhlSKvqaiwG6oJNgs6GsoCqRtIvo+fvyIgwcPYuXKlWjZsmWJ48bGxmjYsCFfppp9+/YNkydPhoWFBbp27Yq3b99i6tSpIv1kOjA8SSALnQGAxZaAnHo/TJw4Ec2aNRNIH4IyaNAgnDp1CmfPnsXu3buFHY7Y0dfXh5WVFVasWIHc3Fxhh1NhojztWVJSElOnToWLiwvvJre6WCwWunfvjl27diE2NhYBAQHQ0dHB7t27oa6ujgEDBuDw4cPF9rSprEuhsTDaexcPo35O2yzv86bo+MOoVBjtvYtLobFV7lvclZbMtG7dGiwWC7Gxdff7UlMomSGVMl5bWWA3VBwuA/v+ovlUnAjeypUroaioiCVLlpR6XEZGBlZWVtVOZnx8fNClSxe4urryRmOUlJSq1aag1cT6gNjUHLGdC+/g4IA1a9ZgxYoVcHd3F3Y4Ymfnzp2Ij4/HgQMHhB1KhUnnZQACrq/JAqAqL1ela6dNm4asrCxcunSJv0EBkJCQgKGhIc6ePYvExERcunQJ8vLyWLhwIVq3bo1hw4bh8uXL+PHjR4XbPBQYiZWub5BXyK3073gOl0FeIRcrXd/gUGBkZd9OrVBaMiMlJYVWrVrRyEwNoGSGVIp6i4bQVWvO99EZCTYLumrNeTX9Sd1y//59XLt2DX///TdkZf88lXHEiBF48+YNwsPDK91Heno6Jk2aBAsLC3Tr1k0sRmOK1GRZdHG1efNmDB8+HOPHj8eLFy+EHY5Y0dTUxOzZs7Ft27ZqPdkXtPfv32P79u3Q1taGRnsVFKYnCLS/6kx7rmohgMqSlZXF6NGj4e3tja9fv2Lv3r1ITk7GmDFj0KJFC0yZMgV37twBh8P5YxuXQmOx2y+CL/Hs9ovA5To4QlOUzMjJFU9+lZSUKJmpAZTMkEpztNWCJJ+TGUk2C462tFlYXcTlcrF06VL07t0b48ePL/NcU1NTyMnJ4fr165Xq48aNG+jatSvc3Nxw+vRp+Pj4iPxozK9EeX2AqGCz2Th//jw6deqEYcOG4cuXL8IOSaxs2LABbDYbGzduFHYoPFwuF48fP8bKlSvRsWNHdOrUCVu2bIGSkhKcnJxgb9hTpKc9z5gxA6GhoTWWXCsoKGDevHl4/PgxIiIisGTJEty7dw+GhoZQUVHBX3/9hdevXxe7Ji4tBxs8w/gax3rPsDq3hiYrKwv169eHpGTx5FdZWZmSmRpAyQypNKVmsthk1YWvbW626lJinwtSN1y+fBlPnjzBnj17yt14sX79+rC0tKzwVLOi0RhLS0veaMyUKVPEYjTmV6K8PkCUyMrKwtPTEwBgbW3N28iOlE9eXh5r167F8ePH+V4xsDLy8/Nx69YtzJ49G23btsWAAQNw+vRpDBo0CF5eXkhJScG1a9dgb2+PqYM1RHraMz8LAVSWuro6Nm7ciMjISDx69Ag2NjY4c+YMunfvjm7dumHnzp34/PkzVru9QSGfv4e/lnuvC7LzCvHpWwEatetWonS1kpISrZmpAVSamVTZocBIvgxNLzfRxFx9NT5ERMTNjx8/0LFjR/Tq1Qtubm4VuubatWsYOXIkPnz4gA4dOvzxvBs3bmDGjBnIysrCvn37MGnSJLFLYopQWfTKefHiBXR0dGBubo7Lly+XmySTn/Ly8tC5c2d07NgRN27cqLF+v3//jps3b8Ld3R0+Pj74/v072rVrxysPPnDgwD/u7+JwOgQPo1L5mtRIsFkY2F4eTlO1q93W+vXrsW/fPnz58qXYegphKEoUizbm5DRQROtpRwTWX8DiwbV26nhFS1dzIu5i97qlyMnJEdvfP+KAPuFJlc3TV8d2Oy3ISLIrPdQvwWZBRpKNHXZalMjUYfv378eXL1+wY8eOCl9jZmaG+vXr/3GqWXp6OiZOnMgbjQkLC8PkyZPF+hcJlUWvnJ49e8LFxQXXr1/Hhg0bhB2O2JCRkcGOHTvg4+MDf39/gfaVkJCAEydOwNzcHAoKChgzZgzCw8OxdOlSvH79Gh8/fsSePXugq6tb5kaVoj7teerUqcjKysLly5f50l51SEtL84oDJCQkwGbZboARzNRSCTYLzo9r34hEZUtXO6WqoJH1aryOihdGuHUGjcyQaotLy8FqtzcI/pACCTar7CdkDBdgsaHTQR5/23WjqWV1WFJSEtTU1DBlyhTs27evUtcOHz4ccXFxePLkSbHXvb29MWPGDOTk5GDv3r1iPRrzK4ZhMO3oLdyOLQBY/H8GJcFmwUFbBRv5PH1U2Hbu3IkVK1b8XF9hby/scMQCwzDQ1dXF9+/f8eLFi3J3vK+MiIgIuLu78/YGYrPZGDx4MGxsbGBtbQ0VFZUqtXspNBYrXfk3rWmHnRZG9+VfZU0LCwskJyeX+LwSNr1dgYgR4NoWFXlZ3F2mL7D2a9ql0Fhs8AxDIZep1EggwymEjJQkNttoYQwff67I/9DIDKk2pWaycJqqDf9Fg+GgrQIVeVn8fvvIws8PtqEd5BB/chbGtEyhRKaO27BhAyQkJLB+/fpKXztixAiEhoYiJiYGwP9GY4YNG4YePXrg7du3Yj8aA/y8sfT19YWOjg6c1s8QSCID1N6y6MuXL8fkyZMxdepUPHjwQNjhiAUWi4V//vkHb968wblz56rVFpfLRWhoKNasWYMuXbpAU1MTGzduRIsWLXhlhe/cuYMFCxZUOZEBgDF9lbHMRKNasRZZbqLJ10QGqPlCABVB5d4rpzqlq1kSksjnMnW6dLWg0cgMEYjsvEJMW7wK/4VHwsXpPFTl5SAnIwmGYdC3b1+0aNGiRudkE9ESFhaGbt26YdeuXX/cV6Ys379/h6KiIhwdHaGuro6ZM2ciJycH+/btw8SJE2tFEuPl5YUtW7bg6dOn6N+/P9atW4eLX+XxSITXB4ii/Px8GBsb47///kNISAjatWsn7JDEwvjx43Hnzh1ERkZWaq1HQUEBgoKC4O7uDg8PD8THx0NeXh7Dhg2DjY0NjI2Nyyy/Xh1VfXIuwWZBks3CZqsufE9kAKCwsBAqKiqwsrLC0aNH+d5+VYR9yYDFwfsC7+fGfB10ad1Y4P0IkqiP/BFKZogATZkyBe/fv8fDhw+LvX7mzBlMmzYNHz58QPv27YUUHREmc3NzREREICwsDDIyMlVu4+nTp0hOToa5uTlOnDiBNm3a8DnSmsXlcuHq6oqtW7fi1atXGDx4MNatWwdDQ0OwWCzEpeXAaO9d5PGxhLKMJBsBi/Vq9UhpSkoK+vfvDxkZGTx8+BCNG4v3zVVNiI2NhaamJpYvX47NmzeXeW5WVhZ8fX3h7u4Ob29vZGRkQEVFBTY2NrCxsYGOjk6JkrWCUpVpz7pqzeFoqyXQfwOiVAgAAF7EpsP26MPyT6wmU9ZrdG/bGG3atEHbtm3Rpk0bKCoqik1RDvrMFQ+UzBCBmTBhAqKjo3Hv3r1ir+fk5KBNmzaYMWNGpRZ+k9rB398fJiYmuHbtGoYPH16lNry8vODg4ICMjAz8888/WLRokViPxnA4HFy+fBnbtm3Du3fvYGRkhHXr1mHw4MElzqWnhFXz/v179O/fHwMGDICXl1eN3VyLs9WrV2Pfvn0IDw8vsS9TUlISPD094e7ujoCAAOTl5aF79+68BKZ79+5C/TfJqzYVkYTY1JLVphpJ5OPLU3/cPLgafdTbCjyemJgYtGvXDidPnsTUqVMF3l95ampkhvFxxJd3T1BY+L/pZpKSkmjVqlWxBOfXr7Zt26J169aoX7++wOMrj6hXyyM/UTJDBGbs2LFISkrC7du3SxxbvHgxnJyc8PnzZ9SrV08I0RFh4HA46NmzJxo3box79+5V+mYnLS0NCxcuhLOzM4yNjREUFITdu3djwYIFAopYsAoKCuDi4gJHR0dERkbC3Nwca9euxYABA8q8jsqiV42/vz/MzMwwd+5c7N+/X9jhiLzv379DXV0dpqamuHDhAj5+/MhbwP/gwQOwWCzo6OjwFvCL6kh7dl4holOzkV/IhbQkG6rycsj6loo2bdrgwIEDmDNnTo3EIUqFAGqy3Ht9KTaSk5Px+fNnxMfH875+//v379+LXd+sWbMyE542bdqgWbNmAkuaIxMzYbzvXvknVlFtLl1d0yiZIQIzcuRIfP/+Hbdu3SpxLDw8HB07dsSFCxfg4OAghOiIMJw6dQrTp09HSEgI+vXrV6lrPT09MXPmTPz48QP79+/HhAkTYGlpiczMzBKjf6IuPz8f586dw/bt2/Hp0yfY2Nhg7dq16N27d4XbqOr6AHA5AJeDjVZdMEmXP4umxcmxY8cwe/ZsHD58uMZuYsUVwzBYu3YtHB0d0aFDB3z8+BH16tWDiYkJbGxsYGlpCQUFBWGHWWVWVlZITExESEhIjfTn4eEBGxsbPH/+HD179qyRPssiatXMMjMziyU3pSU8CQkJ+PW2tV69emjdunWZSU+rVq0gJSVV6fg3eobBKSRGIBuz1tYKksJCyQwRGBsbGxQWFsLb27vU48bGxsjOzi6xpobUTllZWVBXV4eBgQFcXFwqfN2vozEWFhY4fvw4b23M2bNnMXXqVMTHx6NVq1aCCp1vcnNzcfr0aWzfvh3x8fEYOXIk1qxZg27dulWpvV/XBxTN//+TovUDvdvIIcBxCuxM9XD27NmqvhWxtnDhQhw+fBg3b96EsbGxsMMRKQUFBQgODuaNwMTFxUFCQgLy8vI4cuQIhg4dCjk5OWGHyRfXr1/HiBEj8O7dO3Tq1Eng/RUVArC2tsaRI4LbrLKiVl9/iX+ffgZTov5o9QnqZr2goAAJCQllJjzx8fH48eMH7xoWiwVFRcVyR3kaNWpUrC9RS/bIn1EyQwTG0tISkpKScHd3L/W4m5sb7OzsROYpFRGs9evXY+fOnQgPD69wGdbSRmN+nVKQlpaGFi1aYP/+/SL9lD0nJwfHjx/Hrl27kJiYiLFjx2LNmjV8u4GKTMzEugt+CP6QBqlmrYsdY+Hnhpj6Goqw768MNcWGOHfuHCZPnoyrV69ixIgRfIlBnBQWFsLKygoPHz7Eo0ePauRGVpRlZ2fDz88Pbm5u8Pb2Rnp6OpSUlHjrX3JycjBs2DC4ubnBxsZG2OHyTV5eHlq1aoUZM2Zg+/btNdKnKBQCyM/Px4kTJ7D14GnUs9sqsH6ENY2KYRikp6eXm/CkpKQUu65Bgwa8BKelkioetLIFBJDoFSmahldbNisWJkpmiMCYmpqiUaNGuHr1aqnHCwsLoaqqyqtERWqvz58/Q0NDA4sWLYKjo2O556elpWHBggVwcXEpMRrzO1NTUxQUFODOnTv8DrvaMjMzceTIEezZswfp6elwcHDAqlWroK6uzve+srOz0bJlSyxatgJjps8vtj7g91+WDMNg5MiRuHPnDt68eSP2VeCq4vv37xg4cCB+/PiBkJAQNG/eXNgh1ajk5GR4e3vD3d0dfn5+yM3NRdeuXXkJTK9evYo9OBg6dCg+fvyIsLAwSEtLCzFy/po7dy7c3d0RGxvL1w1C/0SYhQC4XC4uX76MtWvX4tOnT5g4cSIy+0zCiy/ZdXKBe25u7v+xd95xNf/v/7+fiuyVTUVlRrJHSCSbIluismd2tsjee5OZmZEVsiWZpUIkEYWQ1T7P3x9+9X37WI0zKud+u7l9bp/O6/W8rlfvzjnP63ld1+Pi1atXvwx4nn1M4G1tB7n7kB2kqzMDqmBGhdxo0aIFJUqUYM+ePb+9xtnZmQULFhAeHk6hQoUU55wKmfKrBtv/bqBtbW05ffo0wcHBP6Xy/5ejR48yaNAg4uLiWLFiBTY2Nn9s8Ny0aRODBw/m9evXFC9eXGbPlBE+fvzIqlWrWL58OZ8/f8bOzo5JkyZRrlw5udrt378/V65cITg4+K9NsVFRURgZGVG1alXOnDmTZaRSZUloaCj16tWjcuXKnD17Nt0y4VmFZ8+ecfToUdzd3bl69SpCCExMTFIa+A0Mfi8G8eDBA2rUqMHSpUsZNWqUAr2WL76+vtSrV48zZ85gYWGhEJtt27YlKipKYb068F38YuLEidy9e5cOHTowd+5cqlWrppIe/g2Kkq52H9KImjqF5W4nu/PvfXupUBgJCQl/lT91cHAgPj6eHTt2KMgrFbIiOPIzM48FYLroAtVmnqHdqqtYrbtOu1VXqTbzDKaLLjDzWABHLviwY8cOnJ2d/xjIREVF0bt3bywtLalbty4BAQE/lZX9iuSyl9+VMyqSqKgopk2bhq6uLnPnzqV3796EhISwfv16uQcy8F0O/enTp6madq+lpcX27ds5d+7cP6vsVa5cOY4cOYKPjw+DBg0iu53tCSG4d+8eM2fOxNjYGD09PSZOnEj+/PnZsGEDr1+/5sqVK4wdO/aPgQxAtWrVcHBwYNasWbx//15BTyB/6tSpQ5UqVdi+fbvCbA4aNIibN29y7949udu6desW5ubmWFhYkDt3bq5cucKxY8eoVq0aANpF8jBLxn0t45ppZ+lARgjBuzcRCrGVU0O1DZcFqsyMCrnRqFEjqlSpwpYtW/54Xbdu3fD39ycwMDBLzwr5V0jLULrk19XePOb8fHvKF/t1MHPkyBEGDx5MXFwcK1eupE+fPmn6W2jRogXq6up4enqm+XlkwZs3b1iyZAlr165FKpUyZMgQxo0bR8mSJRXqh1QqRU9PDwsLi1SXbjo6OrJ27Vpu3bpF9erV5exh5mT37t306dOH+fPnM3HiRGW7kyESExO5du1aSgN/aGgoBQsWpH379lhaWtKqVSvy509fH0NkZCQGBgYMGDCApUuXythz5bFgwQJmzpxJRESEQgaqJiYmoqOjg6WlpdyEAIKDg5k6dSr79++natWqzJs3jw4dOvz2c1VWcu+Jd9zJ9/wq58+fzxLlqzExMTx48ID79++n/PPz8+PTtzi0xxyQ655E1TMjO1TBjAq5UbduXWrVqsWGDRv+eN3FixcxMzPDy8sLMzOVskdmJr1ywGpADg01ZnU0pMd/BjRGRUUxcuRI9uzZQ/v27dmwYQOlS5f+/UK/Yd26dYwYMYLIyEi0tLTSfH96efXqFYsWLWLDhg1oaGgwfPhwHB0dlSpXO23aNFauXElERESqhs7FxsZSt25dJBIJN2/e/GfnPk2bNg0XFxcOHTqElZWVst1JE9++fePs2bMcOXKE48ePExUVRenSpVP6X0xNTWXW5zJ37lxmzpxJQECAXHq/lEF4eDg6Ojps2LABBwf590nA97+3FStWyFwI4PXr1zg7O7N582ZKliyJs7Mzffv2TVU/UHo/39XVJGioSXDuaEjtwvE0b96cHDly4OXllWqxF3kjhODVq1c/BC3379/n8ePHSKVS1NTUqFixIjVq1Ej55+KXg/DoeLn5pFIzkx2qYEaF3KhZsyYmJiasXr36j9cJITA0NMTQ0PC3YgEqlI+sTu7GWVRkuFmFDGdj/ktERASlS5dm8+bN2NnZZdjHvxEWFsaCBQvYsmULuXLlYvTo0YwcOZIiRYrI3fbfCA4OpmLFiuzdu5cePXqk6h4/Pz/q1q3LsGHDstWJe1qQSqX06NGDEydOcOXKFWrVqqVsl/5IVFQUJ06cwN3dnTNnzhATE0OVKlWwtLTEysqK2rVry6UPKiYmhkqVKlGnTh0OHz4s8/WVRevWrfn69StXrlxRiD1ZCwFER0ezaNEili1bhqamJpMnT2bYsGGpOtD4L+nJvDcxKMpcq+oppWWhoaE0b96cpKQkvLy80NfXz9CzpZX4+HgCAwN/ClyioqIAKFCgwA9BS40aNTA0NCRPnh9L42YeC2DnjVCS5LBLVs2ZkS2qYEaF3KhWrRrm5uYsX778r9euWrUKR0dHwsLC0nUyr0K+uPmGMemwv8zW04+6gdemOXTo0IENGzbIZEaMqakpefPm5eTJkzLw8NeEhIQwf/58tm/fToECBXB0dGT48OEKKU1JC40aNaJQoUJp+l0sXbqUsWPHcvbsWczNzeXoXebl27dvNGvWjPDwcHx9fTPdZ1FYWFhK+djly5dJSkqiYcOGKQ38lSpVUogfyWV5ly5domnTpgqxKW/27t1Lr169CA4O/mv/kKyQhRBAXFwca9euxcXFhW/fvjFq1CgmTpyYYUGd4MjP7PYJ48LjN4RFfeO/G8Vfyb3/Ly9fvqRFixZ8/fqV8+fPy+1v882bNz8FLUFBQSQmJgKgp6f3U+BSrly5vx6cvXv3Dqd5Kzibs4Fc/AblSVdnR1TBjAq5UblyZTp06MCiRYv+em10dDSlS5dmwoQJzJgxQwHeqUgtsla7EUJAUgJjq8Qwon9PmdUkr1q1irFjxxIZGUnhwrJVh3n8+DFz585l165daGlpMW7cOIYMGaK0ORF/Y8OGDQwdOpSXL1+mOlCUSqVYWFgQFBSEn5+fQsv1MhOvX7+mXr16lChRgsuXL/90WqtIhBA8ePAgJYC5c+cOOXLkoEWLFlhaWtKxY0elDIuVSqU0aNAAqVTKzZs3s4USXkxMDCVLlmTUqFE4OzsrxObRo0extLTk7t27GBsbp+nepKQkdu/ezbRp0wgPD8fe3p4ZM2bIJQD/m1rl74iIiKBFixZERUVx/vx5DA3Tn4VITEzk0aNHPwUuERHfG/Xz5MlD9erVfwhaqlev/lf1zP/l8+fPLF26lCVLlgBQbcR6IkVBmWZnsop0dVZCFcyokBv6+vp069aNefPmper6QYMG4eHhQWhoKDly5JCzdypSi80WH66HRMl2DoEEGukXlemHeXh4OGXLlsXV1ZW+ffvKZM2AgABcXFzYt28fJUuWZMKECQwYMECpG9zU8OHDB0qVKsWcOXMYN25cqu97+fIlRkZGNG/enAMH5Nv8mpm5d+8ejRs3pnXr1uzfv1+hm/WkpCS8vb1xd3fnyJEjhISEkD9/ftq1a4elpSVt2rRJ8wZNHly9epUmTZqwc+dO+vTpo2x3ZMKAAQM4e/YsISEhCvlvnh4hACEEJ0+eZNKkSTx48IAuXbrg4uKisKxcWnn79i0tW7YkPDycs2fPpipo+/DhA35+fty7dy8laAkICCAuLg4AbW3tn7It+vr6GZoTFBsby/r163FxceHz588MHz6cSZMmEaOWRyVdnQVQBTMq5Iauri62trapPuW6d+8eNWvW5NChQ3Tu3FnO3qlIDcGRn2m5/LLc1pd1mt3ExISiRYty9OjRDK1z79495syZw6FDh9DR0WHSpEn0798/SzXHd+vWLSXLkpag5ODBg3Tt2pXt27dja2srRw8zN0ePHsXKygonJydcXFzkais2NpZz585x5MgRjh07xtu3bylZsiSdOnXC0tISMzOzTDkDx9raGh8fHx49epTpA/zUkBygXbhwgWbNminEZrIQwOvXr8mbN+8fr/X29mbixIlcuXKFZs2aMX/+fOrXz/yn++/fv6dVq1Y8ffqUM2fOULduXeB7hu/p06c/ZVvCwsIA0NTUxNDQ8IegxcjISKa9iYmJiezcuZOZM2fy8uVL7OzsmD59Otra2inXyLrMekHn6nT/jxCOioyT9XPDKjItqZkz81+MjY1p1KiR3KQqVaSd3T5hqKvJ53ReXU3CrhthMl3T2tqaM2fO8OnTp3Td7+vrS8eOHalZsyZ3795l8+bNBAcHM2TIkCwVyMD3QaUPHjxI8ywLa2trbG1tGT58OCEhIfJxLgvQqVMnFixYwNy5c9m5c6fM1//w4QO7d+/G2tqaokWL0qFDB65cuYKdnR3e3t6Eh4ezfv16WrdunSkDGfguaRwZGcmyZcuU7YpMMDExQV9fX6EzZxwcHPjy5Qtubm6/vSYoKAgrKysaNWrEp0+fOHXqFF5eXlkikAEoUqQI7u7ulClThqZNm2JpaUnDhg0pUKAAFStWpGvXrmzevJn4+Hh69OjB7t27efDgAZ8/f+b27dts3bqVUaNG0axZM5kFMkIIDh8+jJGREXZ2dtSvX5/AwEA2bdr0QyAD0KOuDuMsKsrErnmxb6pARg6oMjMq5Ebx4sVxdHTEyckp1ffs2rULGxsbgoKCqFy5shy9U5EaTBdd4Pn7b3JbX9bSlGFhYejq6rJnzx569uyZ6vuuXbvG7NmzOXPmDJUqVWLKlCn07NkzTcF4ZiMxMZEyZcrQs2fPVIlw/JdPnz5hbGxMqVKluHTpUpb+PWQEIQQODg7s2rWL8+fP07hx4wyt9/LlS44ePcqRI0e4ePEiiYmJ1KtXL0VCuUqVKjLyXHGMHTuWDRs28OTJE4XPVZIHs2fPZsGCBURERCisJ+53QgAvX75k5syZbNu2DR0dHebMmUPPnj0zdY+SEILnz5//lG15+vRpyjVqamo0b96cVq1apWRcihcvrjAfz58/z+TJk7l58yYWFhbMnTuX2rVr//W+jEpXR3ttYkTb2kyZMiUj7qv4BZn3HaEiy5PWzAyQckq5fv16OXmlIrV8iUskTI6BDEBY1De+xiXKbD0dHR3q1avHwYMH/3qtEIILFy7QvHlzGjduTHh4OG5ubgQEBGBjY5PlN/AaGhr07t2bPXv2kJCQkKZ7CxQowM6dO7lx4wbz58+Xk4eZH4lEwrp162jYsCFWVlZpzlQJIQgMDGTu3LnUrVsXbW1tRo8ejZqaGitXruTly5f4+Pjg5OSUJQMZgKlTp6Kpqcn06dOV7YpMsLGx4evXrwqVnR44cCA3b95MyaK+f/+eCRMmUKFCBY4ePcqyZct4+PAhvXv3zlSBTExMDL6+vmzevJkRI0bQtGlTChcuTPny5bG0tGTFihV8+PCBDh06sG3bNu7cucP79+9p0aIF165do0aNGrRs2VJhgYyvry/m5uaYm5sjkUjw8vLizJkzqQpk4HuG5pyjKY30vouj/K1qIfn1RnpanHM0pdS3Z7x48SJjD6Hil6gyMyrkRv78+Zk9ezajR49O032TJk1i/fr1hIeH/7WGWIX8CHgVTbtVV+Vu58SIxhiWlp208aJFi5g+fTpv37795cmqEAJPT09mz57NtWvXqFmzJtOmTaNTp06ZaqMgC+7fv4+xsTHHjh2jQ4cOab5/2rRpzJs3j+vXr1OvXj05eJg1iIqKokGDBuTMmZPr16//UYpbKpVy48aNFAWy4OBg8uXLR5s2bbC0tKRt27YZls3NbKxcuRJHR0fu3r2LkZGRst3JMGZmZqipqXH+/HmF2EsWAmjfvj36+vrMnz+fhIQExo4dy9ixY5Uu+PC3gZMSieSngZM1atSgTJkyv+zXi42NpUuXLpw/f55Dhw7Rrl07ufofFBTE1KlTOXz4MFWrVmXu3Ll07NgxQwIn6ZGu7tixI0lJSZw4cSJjD6TiJ1TBjAq5kStXLhYvXszw4cPTdN+zZ8/Q19eX2TAxFenjbtgHrNZdl7sd9yGNqKkjOynlkJAQ9PX12b9/P127dk35uRACDw8P5syZw82bN6lfvz7Tpk2jbdu22Vq1y9jYGAMDg1Rlq/6XhIQETExM+PDhA3fv3s20UtSK4OHDhzRs2JD69evj4eHxQ+YuLi4OLy8v3N3dOXbsGJGRkRQvXjylgb958+ZZrucqLSQkJFCtWjV0dXU5c+ZMln8/ubq60q9fP0JDQxUywT4xMRFLS0tOnjyJuro6gwcPZurUqZQoUULutv+X+Ph4goKCfgpc3r17B3zP2hoZGf0QtFSrVi3NAhDJ/TEeHh7s27cPKysrmT9LWFgYM2fOxNXVFW1tbZydnendu3eGVM9+RWqlq4cPH87ly5fx8/OTqX0VqjIzFXIkMTExXRLL5cuXp23btqxZswZVrK08cmoo5uNB1nb09PSoVatWyuZdKpVy6NAhatWqRceOHdHU1MTT0xNvb2/atWuX5Tdef8PW1pbjx4/z/v37NN+bI0cOdu3axatXrxgzZowcvMs6VK5cmYMHD3Lu3DkcHR2Jjo5m7969dO/enaJFi9K2bVu8vLywsbHh6tWrvHr1io0bN9K2bdtsHcjA97+ThQsXcvbsWU6fPq1sdzJMly5dyJs3r1yEH/6LEAJ3d3eqV6/OiRMnEELg7OzMqlWrFBLIvH37lnPnzrFkyRL69u1LjRo1yJcvH8bGxtja2nLkyBEKFCjA8OHDcXd3JyQkhI8fP3LlyhVWr17NgAEDqFevXrqU7HLmzMm+ffvo3LkzXbt2Zd++fTJ9LkdHRypUqICHhwfLly/n0aNH9O3bV+aBDEBeTQ0MSxekpk5hDEsX/O0MHm1t7RSlNhWyRZWZUSEXhBCoqamxefPmdGVXTpw4Qfv27blx40aWUWzJbnyNS6TazDPI8wNCAjyY2SpVA9jSwrx583BxcWHNmjUsWrSIgIAAmjdvzvTp0zE1NZWprcxOZGQkZcqUYdWqVQwZMiRda2zcuJFBgwZx5MgROnXqJGMPsw6vXr1i/Pjx7NmzB3V1dZKSkqhduzZWVlZYWlpStWrVbB8c/w4hBM2bNycyMhI/P78s33Nma2uLt7c3jx49kst/00uXLjFp0iRu3LiBhYUF8+bNY+rUqb8UAsgoiYmJPH78+Kdsy+vXrwHZDZxMD0lJSdjZ2bFr1y62bt2aITn4T58+pQy8VFNTY/z48YwePTrTZJT37NlD7969+fTpE/nzy24kgQrI2p82KjItiYnfm7rTO/yydevWlCtXjrVr16qCGSWRV1MDnSJ55KpmpqOVR+aBTGJiIurq6nz9+pV+/frRunVrNm7cSKNGjWRqJ6tQokQJWrdujaura7qDmQEDBnDixAkcHByoX79+tlCtSi0PHz5M6X/x8fFBXV0dbW1tXr58ma0GRmYUiUTCkiVLqFOnDps3b2bw4MHKdilD2NrasmPHDry9vWX62eHn54eTkxMnT56kTp06nDt3jhYtWgDfhQCsrKy4d+9eqoZL/oqPHz/+FLQEBAQQGxsLQNmyZalRowZ2dnYyGziZEdTV1dm2bRs5c+akf//+xMXFMXDgwDStERsby7p163BxceHr168pAy+1tLTk5HX6SJZ8fvHiBVWrVlWyN9kLVWZGhVz49u0befPmZffu3fTq1StdayxYsIAZM2YQHh6e6T6U/hVmHgtgp8/zNMlQphZ1NQk29XWZ2dFQJuvFx8ezY8cO5s2bR0hICAUKFKBhw4bZouwloxw4cIBu3brx8OHDdE8Kf/v2LdWrV6dmzZqcPHky22YgpFIpvr6+HDlyBHd395SBkK1bt8bKyoq2bdtSsGBBOnbsyNWrV/H29lZtTP5Dv379OHnyJE+ePFF643pGkEqllC9fnlatWrFx48YMrxcaGsr06dPZtWsX+vr6zJ07F2tr6x/eR8lCAJaWln+dt/a3gZM5c+b8aeBkjRo1ZDpwUpZIpVJGjRrF6tWrWblyJSNGjPjrPYmJibi6ujJr1ixevXqVMvCybNmyCvA47Tx//pxy5cpx6tQpWrdurWx3shWqnhkVciGjmRkAOzs7hBBs27ZNVm6pSCO96+vIJZABSJIK+jTI+PCw2NhY1q5dS4UKFRg4cCC1atXi3r17jBs3juvXr6ecSP7LdOjQgUKFCuHq6pruNYoVK8bWrVs5ffp0thtsGx8fz5kzZxgyZAhly5alQYMGbN68GRMTE44dO8a7d+84dOgQffr0oUiRIqirq7N3794UBaq3b98q+xEyDS4uLnz58oV58+Yp25UMoaamRt++fdm3bx8xMTHpXuft27eMHj2aSpUqcfbsWdauXUtgYCBdu3b96UBAQ0MDe3t7du3axdevX1N+/uXLF65fv866desYPHjwTwMnN23aRFxcHN27d2fXrl34+/vz5csX7ty5w7Zt2xg9ejRmZmaZNpABUuTKx44dy8iRI1m8ePFvrxVCcOjQIapVq4aDgwMNGzYkMDCQjRs3ZtpABqB06dJIJBKVPLMcUGVmVMiFqKgoihYtyuHDhzOkUmJjY8P169cJDg7OdrK5WQWbLT5cD4mSaVCjriahkZ4WO+3TX0L47ds3Nm7cyKJFi4iIiKBHjx5MmTIl5ZQ8KCiIqlWrpluWOLsxePBgTpw4wfPnzzP0Xho+fDhbtmzhzp07WXY2Cnyvrz99+jRHjhzhxIkTfPr0KWU+RvK09b+V3oSGhlK/fn0qVqzIuXPn0NTUVJD3mZsZM2awYMECHj58SLly5ZTtTroJDg6mYsWK7N27lx49eqTp3i9fvrBs2TIWLVqERCJhwoQJjB49+o/jBoQQXL9+ncaNG9OpUyfU1dV/GDipoaFB5cqVf8q2KEP1TF4IIZg+fTpz5sxh9uzZTJ069YfXz507h5OTE7du3aJ169a4uLhQq1YtJXmbdsqUKYO9vT3Ozs7KdiVboQpmVMiFiIgISpUqxfHjx2nfvn2617l+/TomJiaqtKwSefH+G+bLLhGXKJXZmpoaapxzNEW7SNpVcL58+cLatWtZsmQJUVFR2NjY4OTkRMWKFX+61tDQkDp16mQoI5FdSK79/2+Nfnr49u0btWvXJnfu3Ny4cYOcOXPK0Ev5EhERwbFjxzhy5Ajnz58nPj6emjVrYmlpiaWlJdWrV09z+Zy3tzdmZmZ0796d7du3Z9vyu7Tw5csXKlasiKmpKXv37lW2OxnCxMSEAgUKcOrUqVRdn5CQwKZNm3B2dubDhw8MHz4cJycnihYt+sN1MTExBAQE/FAi5ufnx8ePH4HvvSRNmzb9IWipWrXqPxMwz5kzh2nTpjFlyhRmz56Nr68vTk5OeHl50bBhQ+bNm5clxVwaNGhAlSpVVBUnMkYVzKiQCy9fvkRbWzvDQYgQgpo1a6Kjo8OxY8dk6KGKtODmG8akw/4yW29B5+p0r5u2ErPo6GhWrVrFsmXL+Pz5M/3792fSpEmUL1/+t/fMmDGDFStW8ObNmyy16ZYHQggqVapEgwYN2LFjR4bWunPnDg0aNGDMmDHMnz9fRh7Kh+Dg4JQGfm9vbyQSCU2bNk0JYGQxRyRZpWjevHlMmjRJBl5nfbZu3Yq9vT3e3t40aNAg5eepncmRWdi0aRODBw/mxYsXlC5d+rfXSaVSDhw4wJQpUwgJCaFv377MmjULHR0dXr9+/VNvy6NHj347cPL169cMGDCAu3fvplsIIDuwePFixo8fj4GBAU+ePKFatWq4uLjQoUOHLHto0K1bN6KiohQ2kPVfQRXMqJALz549Q09PL8OnwPBdFnbIkCGEhIQoZICZil+z+kIwiz0fZ3id8RaVGGZmkOrr379/z4oVK1ixYgWxsbE4ODgwceLEFGWYP+Hv74+RkREnT56kTZs2GXE7WzBnzhzmzZtHZGRkhuVKFyxYkHJS2qxZM9k4KAOEENy6dSslgAkMDCR37ty0atUKS0tL2rVr99MpuSyYMWMGzs7OHDp0iM6dO8t8/axGsmx1njx52H74NHtuvuDCozeEvf/FtPQieTCrVJze9XWoUCJzSdZGR0dTsmRJZs2axYQJE355zblz55g4cSJ37tyhadOmWFhY8P79+58GTubPnx8jIyOMjY3/OHAyISEBXV1drKysWLNmjdyfMTPy/PnzlIGXQghatmzJiRMnMtSHmxkYO3Ysx48f5/HjjH+Xqvg/VMGMCrmQXGt88eLFDKeCv3z5QpkyZRg+fDguLi4y8lBFenDzDWP60QfEJSQgUUv9aaq6mgQNNQnOHQ1TnZF5+/YtS5cuZfXq1SQlJTF48GDGjx9PqVKlUm1XCEHlypVp3LgxW7ZsSfV92ZVkNZ3t27dnaJ4DfN+sNm/enGfPnuHn50ehQoVk42Q6SEhI4NKlSykBTHh4OEWKFKFDhw5YWlpiYWGRrsF+aUEqldKzZ0+OHz/OlStXqF27tlztZQXcPM4yatcNcpevhbqa5I99d8mvNzEoylyr6ukqQZUXPXv2xM/PjwcPHqRkBN6+fcuBAwdYvnw5wcHB5MmTh7i4OJKSkoDvw5//t7elXLlyqe5XmzZtGitXruTVq1d/7LPJbrx58wYXFxfWr19P4cKFmTZtGhKJhOHDh+Pg4MD69euzdP/s8uXLcXJy4tu3b1k2u5QZUQUzKuRCYGAghoaGXLt2TSYa/SNHjmTfvn2EhYX9MzXDmZXOfQdwW1IRSemqf92gCGkSEjX1NG1QXr9+zeLFi1O+tIYNG8aYMWMoXrx4uvydMmUK69evJyIiIsuf6smC5s2bA+Dl5ZXhtcLCwjAyMqJdu3bs3r07w+ulhS9fvnDmzBnc3d05ceIEHz9+RFdXN6V8rHHjxgof3BgTE0OzZs14+fIlN2/epEyZMgq1n5lw8w1jxrEA4hISQZL6zWfywcesjob0SGMpqjxITExky5YtDB48mL59+/LmzRtu376domAnkUgwMDDA1NQ0JeNiZGSUYVnq0NBQ9PT02Lx5M3Z2drJ4lExNdHQ0S5YsYenSpairqzNhwgRGjRqVkkF2dXXFzs6OPn36sHXrVqXNxckohw4dwtramrdv38olQ/yvogpmVMgFPz8/atSogY+PD/Xq1cvwesnB0Z49e+jZs6cMPFSRHu7fv4+xsTGbNm3CtEN3dvuEceHxG8KiflE6opWHlz5nMMoTjbvrur+u/eLFCxYuXMimTZvIlSsXI0eOZNSoURmeMXT37l1q1aqFp6cnLVu2zNBa2YHt27fTv39/QkNDZdovkpGZUqnlzZs3HD9+nCNHjnD27Fni4uIwMjJKUSCrUaOG0k87IyIiqFevHsWKFePy5cv/1Kl6MrIqSR1nUZHhZhVk4FHq+NvAyTx58lCsWDFevHhB4cKFGTNmDGPHjpXbAVubNm14//49Pj4+clk/MxATE8PatWuZN28eX79+ZeTIkUycOPGXMtJubm706dOHrl27smPHjix5OOXr60u9evW4fft2llJhy+yoghkVcuH27dvUqVOHO3fuULNmTZmsaWZmRmJiIleuXJHJeirSTocOHXj06BEBAQE/fJH8rql34cKFTJ8+ndevX1O4cOFfrvns2TPmz5/Ptm3byJ8/P46OjgwfPlxmZUtCCAwMDDA3N2fDhg0yWTMr8/nzZ0qWLMnkyZOZMmUKkPGm7F69enHy5Enu378v8762p0+fcvToUdzd3bl27RoSiYTGjRtjaWlJp06d0NPTk6k9WXD//n1MTExo1aoVBw4cyNJlMWklM4iF/A2pVEpISMhPgcvz58+BnwdOGhgY4OzsjK+vL4UKFWLy5MkMHz6c3Llzy9Sv/+XIkSNYWVllSyGAxMREtm/fzsyZM4mIiMDBwYFp06b9NZvp7u5O9+7dad++PW5ubllO2CVZ6fXIkSN06tRJ2e5kG1TBjAq54OPjQ4MGDfDz86N69eoyWTN5irks11SRepJlstMycyEiIoKyZcuyYsUKhg0b9sNrwcHBzJ07l507d1KkSBHGjRvHkCFDyJ9f9g3AEydOZNu2bbx69UrhpUeZkb59++Id8IxeM9Zx8dHbDDdlf/z4ESMjI/T09Dh//nyGSkCEENy9ezel/8Xf3x9NTU0sLCywtLSkQ4cOFCtWLN3rK4pjx45haWnJpEmTmDt3rrLdUQiZTcYdvpcj+vv7/xC0JA+VBChRosRPvS2VKlUiR44cxMXFsW7dOlxcXPj06RPx8fG4urrSt29fmT3fn8iOQgBSqZRDhw4xdepUHj9+TI8ePXB2dqZChdRn4E6cOEGXLl0wNzfn4MGD5MqVS44eyxapVEquXLlYunQpw4cPV7Y72QZVMKNCLly9epUmTZoQFBRE5cqVZbJmQkICOjo6WFlZZbsJ5JkdIQRmZmZ8+PCBu3fvpumk2dLSkrCwMO7cuQN8Lxl0cXHBzc2NEiVKMGHCBAYOHCjXBu3k1L6XlxdmZmZys5MVePH+G4O2XCTwvUBNAn+ahZqWpuxLly5hZmbGvHnzmDhxYpp8Ss64JgcwYWFhFCpU6IcG/oyqrymDJUuWMG7cOJkILmQFlDlgVwhBWFjYT9mWp0+fIoRAXV39lwMnS5Ys+dNaSUlJ7Nmzh2nTpvHixQvs7OyYOXMmlpaWlC5dmqNHj8rs+f5GdhECEEJw9uxZnJycuHPnDm3atMHFxSXdlRuenp5YWlpiYmLC0aNH5S7wIUv09PTo2rUrCxYsULYr2QZVMKNCLly8eBEzMzOCg4MxMEi9DO/fmDFjBkuXLiU8PDzDDZYqUs/Zs2exsLBI1xDU48eP07FjR9zc3Dh06BAHDx6kbNmyTJo0CTs7O4WcqgkhKFeuHO3bt882J5zpIbkpO1Eq0rThTG1T9qRJk1i6dCk3btz4az34169f8fT05MiRI3h4ePD+/XvKli2b0sDftGnTLFkT/1+EEAwYMIAdO3Zw/vx5mjRpomyX5EZw5GdaLr8st/XPOTbFoPj3DOHfBk4WLlz4p6ClatWqf/2sEUJw6tQpJk2ahL+/P1ZWVri4uFClShUA1qxZw+jRowkPD0+3IElayQ5CADdu3MDJyYmLFy/SqFEj5s2bR9OmTTO87sWLF2nfvj21a9fGw8NDLll9edCsWTNKlSqV5QfKZiZUwYwKuZC8+ZVVk3EyL1++pFy5cqxcuZKhQ4fKbF0Vv0cIQb169ciRI0dKz0Ja8PHxwdTUlLi4OMqXL4+TkxO2trYKr3UeO3Yse/bs4eXLl1lWCScjKKIpOz4+nvr16xMbG8vt27d/Oi199+4dHh4eHDlyBE9PT2JiYqhWrVpKAFOrVi2lN/DLmvj4eFq1aoW/vz8+Pj7o6+sr2yW5MPNYADt9nss0K5OMGlA15zvyPz7908DJChUq/BS4lC1bNs1/Rzdu3GDixIlcvnyZpk2bsmDBgh+GfQJERUVRqlQpFi5cyOjRo2X3gH+hTZs2fPjwgRs3bijMpiwICAhgypQpHD16lOrVqzN37lzatWsn0/f49evXadOmDYaGhpw6dYqCBQvKbG15YWNjw7Nnz7h69aqyXck2/DtdiSoUSmJiIoDM+xPKli1Lx44dWbt2Lao4XDG4u7tz69Yt5s2bl6YvIW9vb9q2bUuDBg3ImzcvefLk4d69ewwYMEApTZtdunQhIiKC69evK9y2snHzDZNJIAOw2PMx+3zDfvlazpw52bNnD6GhoSkDBkNDQ1m+fDnNmjWjRIkS2NnZ8e7dO5ydnXn8+DH+/v7Mnj2b2rVrZ7tABr7/Tg4dOpQy9yY5e5DduPDojVwCGQApcP9NAi9evMDMzIx169Zx48YNPn/+zKNHj9i/fz9Tpkyhffv2aGtrp+nv6OHDh3Tu3JmGDRvy4cMHTpw4wcWLF38KZAC0tLTo0KEDrq6uMny6vzNo0CB8fHy4f/++Qu2ml9DQUGxtbalevTr+/v7s2rWLe/fu0b59e5m/xxs1asS5c+cICgrC3Nyc9+/fy3R9eaCtrc2LFy+U7Ua2QhXMqJALCQkJAHIpExk6dCgBAQEqVTMFkJSUxNSpU7GwsEj18NNLly5hbm5Oo0aNeP78OXv27OHatWt8+/YNDw8POXv8exo0aEDp0qU5ePCg0nxQBi/ef2PGsQCZrjn9WAAv3n/75WuVK1dm9OjRrFmzBn19fcqXL8/EiRPJly8fGzZs4PXr11y9epVx48alqek3K1OkSBE8PDx4/fo13bt3TznsyS58iUsk7Dd/D7JCo1Apzpy/yJo1axg4cCD169fPUA9JeHg4AwYMwNDQkDt37rBjxw7u3r1L27Zt/7jhtrW15d69e/j5+aXbdlpp164dpUqVYuPGjQqzmR4iIyMZOXIkFStWxNPTk9WrVxMUFETv3r3lquhXt25dLly4wLNnz2jevHnKDKDMira2NuHh4SkDVlVkHFUwo0IuyCszA9+H/lWsWJF16/4+u0RFxti9ezdBQUG4uLj88brk5s6mTZvSrFkzoqKiOHjwIP7+/vTs2ZPKlSvTtGlTtmzZoiDPf0ZNTY0uXbpw6NAhpFLZqS1ldia7+5Mo4xPzRKlgsvv/ye8mJiZy6dIlHB0d0dPTY/78+WhoaPDq1Ss2bdqUUl7m4OBAiRIlZOpLVqFixYocPHgQLy8vhZYoKYLnUV+Rd55cAKFRXzO8zocPH5g4cSIGBga4u7uzdOlSHj16hI2NTarKT9u0aUOxYsUUmp3JkSMHdnZ27Nq1i69fM/47kDXR0dFMmzYNfX19duzYwaxZs3jy5AlDhw5VWBbe2NiYS5cuERERQbNmzYiIiFCI3fSgo6NDUlISr1+/VrYr2QZVMKNCLsgzM6OmpsaQIUM4dOhQpv7AyurEx8czY8YMOnfuTJ06dX55jRCCEydO0LBhQywsLIiNjeXYsWPcuXOHLl26/HAaZ29vj5eXFyEhIYp6hJ+wtrYmPDw8Ww+h+y/BkZ+58uSdzMt/kqSCK0/esdHtGHZ2dpQqVYpmzZqxf/9+2rZti6enJ0+fPiVfvnwcP348SyqRyYMWLVqwdu1a1qxZw+rVq5XtjsyIl6EUs7zsxMTEsGjRIvT09Fi9ejXjx48nJCSEUaNGpWnoZY4cOejduze7du1K+Z5TBA4ODnz+/Jl9+/YpzObf+O/vdMmSJQwfPpyQkBCcnJyUorxmaGjI5cuXiY6OxtTUlJcvXyrch9Sgra0NoCo1kyGqYEaFXJBnZga+p/o1NDSUetKf3dm8eTNhYWHMnj37p9ekUinu7u7UqVOH9u3bo6GhwenTp/Hx8aFDhw6/LNOwtramQIECbN26VRHu/xITExNKlCjxz5Sa7fYJQ11NPn0oQprEpC0nuHHjBgMGDMDHx4cXL16wZs0aWrZsiY6ODps3b+bYsWNs3rxZLj5kRQYMGICjoyOjRo3i9OnTynYnw0RHR3P7pmIOB3JqpH3LkpiYyNatW6lYsSKTJ0+mV69ePH36FGdn53QrYtra2vLmzRvOnDmTrvvTQ7ly5WjVqlWmKDVLSEhg48aNVKhQgcmTJ9OtWzeePHnC/PnzKVKkiFJ9q1ixIpcvXyYuLo6mTZsSGhqqVH9+hSqYkT2qYEaFXJBnZga+S2/27NmTDRs2ZLv688zAt2/fmD17NjY2NlStWjXl50lJSezbt48aNWrQuXNnChYsiJeXF1euXKFVq1Z/rDXPkycPPXv2ZPv27UqrFVZXV6dz584cPHjwnxCQkGdTtkRNncpmXQgMDGTu3LnUq1fvp7r4Tp06MWDAAEaPHs3jx7IRIMgOLFq0iDZt2tC9e3cCAmTbzyRPhBApfXDDhg2jRo0aFC5cmIG9rOT+fpIA5bRSf9ovhODIkSMYGRlhb2+PiYkJQUFBrFmz5pezZdKCsbExRkZGChcCGDhwoFKFAKRSKfv27cPQ0JBBgwZhampKUFAQ69ato3Tp0krx6Vfo6elx6dIl1NTUMDU15cmTJ8p26QcKFSpE3rx5CQv7tZCKirSjCmZUyIXkAEOeErhDhw7lxYsXnDhxQm42/lVWrVpFVFQUM2bMAL7/99y5cyfVqlWjR48elClThqtXr6YMoUytQo29vT3h4eEKPdH8X6ytrQkLC+PWrVtK80ERKKIp+/XnBL7G/fkwYdmyZZQpU4Y+ffootCwnM6Ours7evXvR1dWlffv2mbZhOTExkTt37rBq1Sq6d++OtrY25cqVo3fv3pw7d47atWuzefNmgvzvoZuGQCM96GjlIa9m6jL9V65cwcTEBCsrK8qUKcOtW7dwc3OT6cwzW1tbjh07plD1rPbt2ytFCEAIwenTp6lTpw49evSgYsWK3Lt3j927d8v0dypLdHV1uXTpErlz58bU1JSHDx8q26UUJBIJOjo6qsyMDFEFMyrkQkJCAhoaGnKVWq1duzb16tVTCQHImI8fP7JgwQIGDhxImTJl2LJlC5UrV6Zv375UqFABHx8fTp8+jYmJSZrXrlOnDtWrV1dqeWDTpk0pWrRoti81yyxN2Xnz5mXXrl3cuXMHZ2dnOXuUdcifPz/Hjx/n27dvWFlZERcXp2yX+Pz5M2fPnmXGjBmYm5tTuHBhateuzbhx43jx4gW9evXiyJEjvHnzhkePHrF161bs7OyoVKkSzSsVl1tJo7qaBLOKfx9S6e/vT/v27WnatCnx8fGcPXuWs2fPUrt2bZn71Lt3b5KSknBzc5P52r/jV0IAX+MSCXgVzd2wDwS8iv7r4UJauX79Os2aNaNNmzbkzZuXK1eu4OHhQY0aNWRqRx6UKVOGS5cuUaRIEUxNTXnw4IGyXUpBJc8sW+TT0KDinycxMVFu/TL/ZejQofTr148nT55k2hOirMaSJUuIiYmhbNmyVKhQgbCwMLp06cLBgwcxNjbO0NoSiQQHBwfGjh3LmzdvFDZF+79oaGhgZWXFwYMHmT9/fracbQKZqym7Xr16zJgxg5kzZ9K6det0BcLZEV1dXY4ePUqzZs0YMGAArq6uCv17fPHiBdeuXePq1atcu3YNPz8/pFIpRYoUwcTEhKlTp2JiYkKdOnXIlSvXH9fqXV+H7d6hcvEzSSro00Dnt68/f/6c6dOns3PnTvT09HBzc6Nr165ylQMuUaIEbdq0wdXVVaEDnB0cHFi4YSf9V50kUr0oYe+//XBoIQF0iuTBrFJxetfXoUKJ/Omy8+DBA6ZMmcKxY8cwMjLixIkTtGnTJst9XpYoUYILFy7QsmVLmjVrhqenJ7Vq1VK2W2hra2eZuUFZAVVmRoVcSEhIkFu/zH/p1q0bRYoUYf369XK39S/w/PlzFi5ciIaGBpMnT6ZRo0b4+/vLJJBJJnnmwM6dO2WyXnqwtrYmJCSEe/fuKc0HeZOeZml52nFycqJBgwbY2Njw6dMnOXuVdWjQoAHbt29n586dzJs3T252kpKSuHfvHqtXr6Znz57o6Oigo6NDz549OXPmDMbGxmzYsIHAwEDevn3LsWPHmDhxIo0bN/5rIANQoUR+mhgUlXl2Rl1NQhODohgU/3lT/u7dO8aMGUPFihU5c+ZMylyT7t27yzWQScbW1pabN28qrITpxftvTDsfSWmHtdz8oMnz/wlk4Hu29Pn7b+z0eU7L5Zex2eLz25lQvyIkJAQbGxuMjIwICAhgz549qZq/k5kpWrQoXl5e6Onp0aJFi0yhZqmjo6PqmZEhqmBGhVxQVGYmd+7c9O/fn61btxITEyN3e9mVL1++sHjxYqpWrUp8fDzt2rUjKCiIvXv3Uq1aNZna0tLSwsrKii1btiitCd/MzIzChQtn61Kzclp5kffWIy1N2RoaGuzcuZN3794xcuRI+TqWxejRowczZsxgypQpMvub/PLlC+fOnWPWrFlYWFhQuHBhatasyZgxYwgNDaVbt24cPnyYiIgIgoOD2bZtGw4ODlSpUiXdgcBcq+poyDiY0VCTMNeq+g8/+/r1Ky4uLujr67N582amTZuWMtdEEYdoyXTo0IHChQsrRAjAzTcM82WXuB4S9f0Han/uR00W/rgeEoX5sku4+f554xwREcHw4cOpXLky58+fZ+3atQQFBdGzZ0+FBIbypnDhwpw7dw5DQ0NatmzJ1atXleqPtrY2b968yRTlpdmBrP8XqiJToqjMDMDgwYP58OFDptLfzyp8+vSJuXPnUq5cOSZPnkxsbCxjxozBzc2NSpUqyc2uvb09QUFB3LhxQ242/kSOHDmwtLTkwIED2VbVLK+mBjpF8sjVRlqasuG7ytCqVatwdXXlwIEDcvQs6zFjxgx69OhB37590yVOER4ezr59+xg5ciS1a9emUKFCtGzZkhUrVpAzZ06cnJy4dOkS0dHReHt7s3jxYqysrGQ6xFS7SB5mdTSU2XoAzh0N0f7/f8cJCQmsX78eAwMDnJ2dsbOz4+nTp0ydOlUps4w0NTXp2bMnO3bskKtC4+oLwUw67E9cojTN6oRJUkFcopRJh/1ZfSH4p9c/fvzIlClT0NfXZ/fu3cyePZsnT54wePBghQaGiqBAgQKcPn2a2rVr06pVK7y8vJTmS7I8c2adhZPVkIjs+k2uQqnMmjWLjRs3Eh4erhB7rVu35sOHD5kifZwV+PDhAytWrGDFihV8+/YNBwcHoqKiUoZayntjIJVK0dPTw9zcXGkzSE6ePEm7du3w8/OjevXqf78hCzJ403lOP/0GEtmfW6mrSbCpr8vMNG5ehRB069aN8+fP4+/vT5kyZWTuW1YlJiYGMzMzwsLCuHnzJmXLlv3ldUlJSQQEBKT0uly7do3nz58DoK+vj4mJScq/jGRa0svqC8Es9sy4FPd4i0oMMzNACMGBAweYOnUqT548oU+fPjg7O1OuXLmMO5tBbt68Sf369Tlz5gwWFhYyX9/NN4xJh/1ltt6CztXpXleHb9++sXr1aubPn09sbCyjR49m/PjxFC5cWGa2MivJohuXL1/myJEjtGrVSuE+PHr0iMqVK3PhwgWaNWumcPvZDVVmRoVcUGRmBr4LAdy8eTPby+1mlHfv3jF58mR0dXVZuHAh/fr149mzZ4wcOZKDBw8yZcoUhZxwqqmp0b9/f/bt28eXL1/kbu9XtGjRgoIFC2a7UjOpVIqHhwempqZsdeovl0AG/t6U/TskEgnr168nd+7c2NraIpUqRqggK5A7d26OHDmChoYGHTt2/D/Fqq9f8fLyYvbs2bRu3ZoiRYpQo0YNRo0axZMnT1JmJ71+/ZonT57g6urKwIEDMTQ0VEqJ0HCzCszvXB1NDbU099Coq0nQ1FBjQefqDDMz4Pz589SrV4/u3btToUIF7t27x44dOzJFIANQt25dKleuLJdSsxfvvzHjmGznEE0/FsCC1ZsxMDBgypQp9OzZk6dPnzJ37tx/IpCB7zPPjh49irm5OR07duT48eMK9yE5M6Pqm5ENqmBGhVxQVM9MMu3atUNHR0cl0/wbIiIiGDduHLq6uqxcuZIhQ4bw7Nkzli1bRunSpZkxYwalS5dm0KBBCvOpf//+fP36lf379yvM5n/R1NSkY8eO2SaYiYuLY9u2bVSvXp0OHTqQkJDAvk0raKyvpdCm7NSgpaWFq6sr58+fZ8WKFTL1LatTsmRJtm7dSlBQEEZGRtStW5eCBQvSokULlixZgpqaGhMmTODixYtER0fj4+PD0qVL6dKlS4aHQcqSHnV1OOdoSiM9LYC//g0mv95IT4tzjqZU1IiiVatWmJubo6GhwaVLlzhx4gRGRkZy9z0tSCQSbG1tcXd3Jzo6WqZrT3b3J1HGQ2/j4hNYejWC5s2b8/DhQ9asWUOpUqVkaiMrkCtXLg4dOkT79u3p3Lkzhw4dUqj9PHnyoKWlpZJnlhGqYEaFXFB0ZkZdXZ2BAweyZ88ePnz4oDC7mZ2XL18ycuRIypcvz6ZNmxgzZgzPnz9nwYIFKbXy9+7dY9++fcyYMSNVqkWyQkdHh5YtWyp15oy1tTWBgYEEBgYqzYeMEh0dzcKFC9HT08POzg4DAwOuXr3K9evXsbKyYl5nI4U0ZacVc3NzHB0dmTRpEn5+fjLyLOshlUrx9/dn/fr12NjYoKenR8uWLYmNjSUkJISYmBhWr16Nv78/79+/5+TJk0yZMgVTU1Py5JFvT1RG0S6Sh5329Tk7uik29XXR1crzsyiFEOhq5cGmvi7nHJsys1lRJgyzp1atWoSFhXH48GGuX79O06ZNlfEIqcLGxoa4uDiZ9oEFR37mypN3ae6R+Stq6uQuX5OZS9ehr68v27WzGDlz5mTfvn107dqV7t27s2fPHoXaV82akR2qYEaFXFB0Zga+N5UnJSWxfft2hdrNjDx//pwhQ4agr6/Prl27cHJy4vnz58yePRstLa0frp0yZQoVK1bE1tZW4X7a29tz/fp1goKCFG4bwMLCgnz58in8VE4WvHz5kvHjx6Otrc20adNo06YNgYGBHD169Ic5LvJuys4Ic+fOpVKlSvTu3ZvY2FgZeJb5+fbtGxcvXsTFxYW2bduipaWFkZERw4cP5+HDh3Ts2JH9+/cTHh7OkiVLCAgIIFeuXFSrVi3LqkpVKJGfmR0NuTTOjAczW3FiRGPchzTCMOwYenfWcGmcGUPqF2W5sxOVK1fmypUrbNq0CX9/f6ysrDK9JHCZMmUwNzeXaanZbp8wuQ4h3XVDVd4E/6eyaGNjQ58+fdi2bZvCbKuCGdmhGpqpQi4oOjMD38szunTpwrp16xg1alSW/eLPCE+ePGHevHns2LGDQoUKMWvWLIYOHUqBAgV+ef3Vq1c5efIk+/btU3jwCdCpUye0tLTYunUrixYtUrj9XLly0aFDBw4ePMi0adMUbj89PHjwgMWLF7Nnzx7y5MnDsGHDGDly5B9LRXrU1eHdlziZNWV3r5v2XplfkStXLnbv3k2dOnWYPHkyS5culcm6mYmIiIiUJv1r165x584dEhMTKVCgAA0bNmTMmDE0btyYevXqkTfvjzLXjo6OPHz4kIEDB6Knp5epsxOpJa+mBoalCwJQXbswbtfOMGPGDJYsWUKOHDlwcXFhxIgR5M6dW8mepg1bW1t69+7N06dPZZLxuPDojeyzMv+fJKngwuM3zES2hxxZFXV1dbZs2YKmpiZ2dnbEx8crpORaR0eHS5cuyd3Ov4AqmFEhF5SRmYHvQgBNmzbFy8sLc3NzhdtXFg8fPsTFxYU9e/ZQvHhxFixYwKBBg37aHP0XIQSTJ0/G2NgYa2trBXr7f2hqatKnTx927NiBi4sLOXPmVLgP1tbW7N27l+DgYCpUqKBw+6lBCMHly5dZuHAhJ0+epGzZssyfP58BAwaQP3/q+laGm1WgaD5NZhwLIFEq0rRRUleToKEmwbmjocwCmWSqV6/O/PnzGTNmDG3atKFly5YyXV+RSKVSgoKCUgKXq1evEhISAoCuri4mJib069cPExMTDA0NUVf/86wQiUTCmjVrePr0KVZWVvj4+GBgYKCIR5E7cXFxhIaGppS9jho1ikmTJmXZJnRLS0vy58/Pjh07mDVrVobW+hKXSFgaBl2mh7Cob3yNS0yTtHp2Rk1NjXXr1pErVy4GDx5MbGwso0aNkqtNVWZGdqikmVXIhf79+/P48WOuXbumULtCCIyMjKhQoQKHDx9WqG1l4O/vz5w5czhw4ABlypRh4sSJ2Nvbp+pU88yZM7Ru3ZoTJ07Qtm1bBXj7a/z9/TEyMuLQoUN07txZ4fa/fftGsWLFmDZtGpMmTVK4/T+RlJSEu7s7CxcuxNfXl2rVqjFhwgS6d++e7sDvxftvTHb358qTd6irSf4Y1CS/3sSgKHOtqsuktOxXSKVSWrVqRWBgIH5+fj+VQmZWYmJi8PX1TQlerl+/zocPH1BXV6dGjRo0btw4RSI5IxLU79+/p0GDBqirq+Pt7U2hQoVk9xAKRiqVsmfPHqZNm8bz588RQnDlyhUaN26sbNcyzIABAzh79iwhISEZqgwIeBVNu1XyH+p4YkTjlCyZiu8IIZg0aRILFy5kwYIFTJgwQW629uzZQ+/evYmOjv5t9YSK1PHv1eGoUAjKysxIJBKGDBnC0aNHs/Uwqjt37mBlZYWRkRE3b95k/fr1PHnyhOHDh6cqkEnOypiYmNCmTRsFePx7qlevTt26dZUmBJAnTx7atWuXqVTNYmJiWLduHZUqVaJr167ky5ePU6dO4efnh42NTYYyWKlpyhZCkE98S2nK3mlfX26BDHw/Fd2+fTsxMTEMGjQo0w4yffPmDe7u7owbN44GDRpQsGBBTE1NmTdvHgkJCYwePZpz587x8eNHbt++zYoVK+jWrVuGZ+kUKVIEDw8PIiMj6datGwkJCTJ6IsUhhODUqVPUqlULGxsbjI2NOXPmDPD9QCE7YGtry/Pnz7l8+XKG1olPVIxcuaLsZCUkEgnz589n+vTpTJw4EWdnZ7l9HiXLM6uyMxlHlV9UIReU0TOTTJ8+fZg4cSIbN27E2dlZKT7Iixs3bjB79mxOnjxJhQoV2LZtG717907z7/rQoUPcuXOHy5cvZ4rmWgcHB4YMGUJ4eLhShihaW1vTvXt3nj17Rvny5RVuP5moqCjWrFnD6tWriYqKwtraGjc3N+rUqSNzW8lN2TMx5GtcIqFRX4lPlJJTQ415U8bgf+cWM+ffl7nd31GmTBk2btxI165dcXV1pV+/fgqz/SuEEDx8+DClXOzatWs8efIE+L4Jady4MTY2NpiYmFC9evW/loxllIoVK3Lo0CEsLCwYOXIka9euzRTv3dTg4+PDpEmTuHjxIk2aNOH69es0bNgw5dAruRQvq2NiYoK+vj6urq4ZGoSYU0Mx58yKspPVkEgkzJo1C01NTaZMmUJsbCwuLi4yf7/p6Hwv2X3x4gWGhqr+pYyg+ktWIReUlZkBKFCgADY2NmzatClLnmD+isuXL9OyZUsaNmzIs2fP2L17N4GBgfTr1y/NgUxiYiLTpk2jdevWNGnSRE4ep40ePXqQK1cupSnRtW3bNmXugDJ49uwZI0aMQEdHh/nz59OtWzeCg4PZt2+fXAKZ/yW5KbumTmEMSxekbcsW+Pn58fr1a7nb/i/W1tb069ePESNGKHyDGxsby9WrV5k/fz4dOnSgaNGiVK1alUGDBuHn50fr1q3Zu3cvYWFhhIWFsWfPHoYNG4axsbHcA5lkzMzMWLduHevXr2fVqlUKsZkRHj16RJcuXWjQoAFRUVF4eHhw6dIlGjZsCHxXktLV1eXp06dK9lQ2SCQS+vbty8GDB1MGnqaHclp5f5awljGS/29Hxe+ZPHkyS5YsYd68eYwdO1bmGZrSpUsjkUhUmRkZoApmVMgFZWZmAIYMGUJERARHjhxRmg8ZRQjB+fPnadasGaamprx584YDBw7w4MEDevXqle5gcdeuXTx8+JA5c+bI2OP0U6BAAbp27crWrVuVMhE+X758tGnTRuGlZrdv36ZHjx4YGBiwd+9eJkyYQFhYGKtXr0ZPT0+hvvyXZPGMc+fOKdz2ihUrKFasGH369CExMVFudt6+fcvRo0cZP348jRo1omDBgjRp0gQXFxdiY2MZMWIEnp6efPz4kTt37rBq1Sp69OiRUhqiLBwcHBg7diyOjo6cOnVKqb78jvDwcAYOHIihoSG3bt3C1dWVu3fv0q5du59Ot/X09LJNZgagb9++fPnyJUM9m3k1NdCRY1kngI5WHlXzfyoYM2YMq1evZtmyZQwbNkym3085cuSgVKlSqmBGBqiCGRVyQZmZGfjeh9GkSRPWrl2rNB/SS3JtuYmJCebm5nz58oUjR45w9+5drK2tM9RYGhcXx8yZM7G2tqZ27doy9Drj2NvbExISojSpSmtra3x8fAgLk+/8BSEEZ86coUWLFtSpUwdfX19WrVpFWFgYM2bMoGjRonK1nxqKFy9OzZo1OXv2rMJtFyhQgF27duHj48O8efNksqYQgkePHrF161bs7OyoVKkSxYsXx9LSEjc3N3R0dFi8eDG3b9/mw4cPnD17lpkzZ9KyZctUq8UpkgULFtCuXTu6d+/OgwcPlO1OCh8/fsTJySlFgGXRokU8evSIvn37/jZ7pa+vn62CmXLlytGsWbMMZ5nNKhVHTmNmUFeTYFaxuHwWz4YMGzaMTZs2sX79egYOHEhSUpLM1tbW1pb7d86/gCqYUSEXlJ2Zge/ZmYsXL2aZ6e5SqZSjR49Sr169FHWxkydP4uvrS6dOnWQyN2fTpk28ePEiU/YSNW7cmIoVKypNCKB9+/bkzJlTbip4CQkJ7Nq1C2NjY1q3bs2nT5/Yv38/jx8/ZujQoZlumruFhQWenp5KacZv1KgRU6ZMYdasWdy8eTPN98fFxXH9+nUWLlxIp06dKF68OJUrV8bBwYE7d+5gbm7O7t27ef78OS9evMDNzY0RI0ZQq1YtpR7CpBZ1dXX27NlD+fLlad++PW/evFGqP7GxsSxevBg9PT1WrlzJ2LFjefr0KY6OjuTKleuP9+rp6fH06dNMK/qQHmxtbblw4UK6N6k3btzgyvZ5yGnMDElSQZ8GspVYz+44ODjg6urKtm3b6Nevn8yyxjo6OqrMjAxQBTMq5IKyMzMAnTt3pnjx4qxbt06pfvwNqVTKgQMHqFmzJpaWluTLl4/z589z7do12rRpI7Omw69fvzJnzhz69u1LlSpVZLKmLJFIJNjZ2XHo0CE+fvyocPsFChSgVatWMi81+/z5M8uWLUNfXx8bGxvKlCnDhQsXuHnzJl27dlVYv0VasbCwIDIyEn9/f6XYnzZtGrVr16Z37958+fLlj9e+e/eOY8eOMXHiRBo3bkzBggUxMTHB2dmZL1++MGTIEM6cOcPHjx+5d+8ea9asoVevXikNuFmRfPnycfz4cWJjY7GysiI2Nva3136NSyTgVTR3wz4Q8Cqar3Gy2YglJSWxbds2KlSogJOTEz169ODJkyfMnj2bggVTJ/mrr6/P58+fiYqKkolPmYEuXbqQO3dudu7cmab7/tsb+frhHQzyJqAu4/SMupqEJgZFMSie+TKOmR0bGxv27t3L3r176dWrl0x6clWzZmSDKphRIRcyQ2ZGU1MTBwcHduzY8dfNkDJITExk9+7dVKtWjW7dulGiRAkuX77MhQsXaN68ucyVU1auXMn79++ZOXOmTNeVJba2tiQkJLBnzx6l2Le2tubatWuEh4dneK2IiAgmT56Mjo4OEyZMwMzMDD8/P06ePEmzZs0yvRKViYkJuXPnxtPTUyn2c+TIwa5du3j16hVjxoxJ+bkQguDgYLZt24aDgwNVqlShWLFidOrUiV27dlG6dGkWLFjArVu3+PjxI+fPn8fZ2RkLC4tsN8tBR0eHo0ePcufOHRwcHH7IbgRHfmbmsQBMF12g2swztFt1Fat112m36irVZp7BdNEFZh4LIDjyc5rtCiE4evQoRkZG2NnZ0ahRIwIDA1m7di2lSpVK01rJvWHZRQQAIH/+/FhbW+Pq6vrXjNP/9kZGRkayf/9+/P392TbUAg0ZBzMaahLmWlWX6Zr/Et26dePgwYMcOXKErl27EhcXl6H1koOZ7JSZVAaqYEaFXEhISFB6ZgZg4MCBfPnyRWmb41+RkJDAtm3bqFKlCn369EFPT48bN27g6ekpN3WxDx8+sHDhQgYPHoyurq5cbMiCkiVL0q5dO6WVmnXo0IEcOXLg7u6e7jUePXrEgAED0NXVZdWqVSm9QK6urlSvnnU2EZqampiamiotmAGoUKECixcvZtOmTfTv3x8rKytKlChBxYoVsbe35+bNmzRr1oxdu3bx7NkzXr58yf79+xk1ahS1a9fOFJ9B8qZ+/fps376d3bt34+Liwov337DZ4kPL5ZfZ6fOc5++/8b/bJAE8f/+NnT7Pabn8MjZbfHiRyonzV69epXHjxlhaWlKqVCl8fX3Zt28fFSpUSJf/ycFMduqbge8HM8HBwXh7e//y9f/tjfz8+TPu7u7cu3cvJWOrXSQPszrKVrLXuaOhXGdG/QtYWlpy5MgRTp8+jZWVFTExMeleS1tbm9jYWN69eydDD/89VMGMCrmQmJio9MwMgK6uLu3bt2fNmjVKP/mIi4tjw4YNVKxYETs7O6pXr87t27fx8PCgfv36crW9ePFi4uPjmTx5slztyAJ7e3vu3LnDvXv3FG67cOHCmJubp6vU7Pr161haWlKlShU8PDxwdnbmxYsXLF68WOkKWOnFwsKCy5cvZ+jLOq28f/8eDw8PnJycaNq0aUpWZvv27bx584ZBgwZx6tQp3r9/j5+fH+vWraN3796UK1cu02e75EX37t2ZNWsWCw5cwWyxF9dDvpdsJf2l6SL59eshUZgvu4Sb7+97PB48eEDHjh1p0qQJsbGxeHp6cu7cuQxLhxcoUICiRYtmq8wMQLNmzdDR0cHV1fWHnydntZJ7I4UQnDhxglu3bmFpaflTb2SPujqMs6iYfHeGfBpvUYnudbNuaWVmom3btnh4eHDx4kU6dOiQbinu/86aUZF+VMGMCrmQWTIz8F0IwM/P77cnZPImJiaGVatWYWBgwJAhQ6hfvz5+fn4cPnyYWrVqyd1+ZGQky5cvZ9SoUZQsWVLu9jJK27ZtKVmypNKyM9bW1ly+fJnIyMi/Xpss2mBiYoKJiQmPHj1i8+bNhIaGMnHiRAoVKiR/h+WIhYUFcXFxXLlyRS7rCyF48uQJrq6uKVK+WlpadOjQge3bt1OiRAnmzZvHmTNnKF68OAUKFMDZ2ZnWrVtn+d+trCncuCdabUeSIP17EPO/JEkFcYlSJh32Z/WF4B9eCwsLo1+/fhgZGREYGMjevXvx9fWlZcuWMvM9u8kzA6ipqWFjY8O+ffuIiYlJ6Y00NjbG0tKSPHnycPbsWa5fv07btm3/GIgPN6vAtFb6iMQEEGmTBlZXk6CpocaCztUZZmaQ0cdS8R/Mzc05ffo0Pj4+tGnThs+f016ymXzQpQpmMoYqmFEhFzJLZga+b8j09PR+kGmWV0Psf/n69StLlixBT08PR0dHzMzMCAwMxM3NTaHlRi4uLuTMmZPx48crzGZG0NDQwNbWlt27d/+xqVleJCvH/anULDY2ls2bN1O1atWU09Rjx44REBCAnZ0dmpqaCvRYflStWpXSpUvLrNQsPj4eHx8fli5dSpcuXShVqhQVKlSgX79+eHt706RJE3bs2MHTp0959eoVBw4cYPTo0VhYWLB9+3ZOnz7NmjVrZOJLdsLNN4wlZx8DZDg7tdjzMft8w4iKimLs2LFUrFiRU6dOsWrVKgIDA+nRo4dMlBX/i76+frbLzMD3UrPo6GgmTJiQ0htZvHhxLl26xKVLlzA3N0/1f68bu5fwae946ul8F1b4mzBA8uuN9LQ452iqysjIiaZNm3L27Fn8/PywsLBIs3hN8eLFyZEjhyqYySCZ4+hcRbYjM2Vm1NTUGDJkCDOWrGPivlvcCPtM2P/UkUsAnSJ5MKtUnN71dahQIv1KL58+fWLNmjUsXbqUjx8/Ymtri5OTE/r6+hl+lrTy/Plz1q9fz6xZsyhcuLDC7acXOzs7FixYgLu7Oz179lSobS0tLZo3b87BgwcZPHjwD699+PCB9evXs2LFCt68eYOlpSXbtm1LmWie3ZBIJFhYWKR73syHDx/w9vbm2rVrXLt2jZs3bxITE0OuXLmoX78+9vb2mJiY0LBhw7/+fbZp04bhw4czfvx4mjdvTtWqVdPlU3bjxftvzDgWINM1Jx++z+CdjiRGRzJ58mTGjBlDvnz5ZGrjv+jp6ckt+6csEhISuHr1Krly5WL16tW0bduWLVu2pOuz4tKlS2zevJl169YxeHBTgiM/s9snjAuP3xAW9YvvMq08mFUsTp8GOirVMgXQoEEDzp8/T8uWLWnRogWenp5oaWml6l41NTXKli2rmjWTQSRC2Y0EKrIlenp69OjRg7lz5yrbFV68/8a4/XfweR6NBIHg9yda6moSkqSCJgZFmWtVPU2Nkh8+fGDlypWsWLGCr1+/Ym9vz8SJE5XacG9nZ8eJEycICQkhb968SvMjPTRt2pQcOXJw/vx5hdveuHEjQ4cO5fXr1xQrVoywsDCWL1/Opk2bSEhIwNbWNuXUOruTLEP66tWrPypVCSF49uxZSuBy9epVAgK+b7JLlCiRUopnYmJCzZo1yZkzZ5p9+fbtG3Xq1EFTUxMfH590rZHdsNniw/WQqDSXlv0JIU2iuPjAiXFtKF5c/sMVt23bhr29Pd++ffvrXJrMTlxcHNu3b2f+/PmEhoZibGyMn58fL168oHTp0mleLzY2lho1aqRkdP43K/Y1LpHQqK/EJ0rJqaFGOa285NXMHAeJ/xr379+nZcuWlCxZknPnzqX6vdOsWTNKlSrF3r175exh9kVVZqZCLmSWzIybbxjmyy5x68UngD8GMpC2hthk3r17x9SpUylXrhzz58/HxsaGkJAQ1q5dq9RA5uHDh7i6ujJ16tQsF8jAdyEALy8vpdTSW1paIoRgzZo12NjYoK+vz/bt2xk1ahTPnz9PEXL4FzA3Nwfg3LlzP/w8ISEBX19fli9fTteuXSldujT6+vr07duXK1eu0KhRI7Zv305wcDCvX7/m0KFDjBkzhvr166c7CMmTJw+7d+8mICCAadOmZfjZsjrBkZ+58uSdTAMZAImaOm/Vi/KJ3DJd93fo6ekhhCA0NFQh9uRBTEwMq1evTumNrFevHvfv3+fChQvkyJGD3bt3p2tdFxcXQkND2bhx4y/L+/JqamBYuiA1dQpjWLqgKpBRIjVq1ODixYu8ffsWU1NTXr16lar7VLNmMo4qmFEhFzJDz8zqC8FMOuxPXKJUpg2xyURGRjJhwgTKlSvH8uXLGThwIM+ePWPFihWUKVNGFo+QIaZPn07ZsmUZOHCgsl1JF9bW1hQoUIBt27Yp1K4QggcPHlCoUCFmzZrF5cuXWbx4MWFhYcyZM4cSJUoo1B9lU6xYMWrVqoWHhwenT59m2rRpmJmZUahQIerVq8ekSZOIjIykX79+HD9+nKioKAICAti4cSO2trYYGBjIVGWsZs2azJkzh0WLFnHx4kWZrZsV2e0TJvOhismoq0nYdUMxpS/JJbhZsW/m69evLF26FD09PUaNGoWpqSkBAQHs27cPIyMjChUqhJWVVapmzvwvDx48YP78+UyePDlTDjpW8TNVq1bl8uXLfPnyBVNT01QFKdra2rx4/UbufbzZGVWZmQq5ULRoUcaOHYuTk5NS7Lv5hjHpsOwmly/oXD2lgTI8PJxFixaxceNGNDQ0GDFiBI6OjhQtWlRm9jLKnTt3qF27Nlu3bqV///7KdifdDB48mBMnThAaGoq6urpcbSUmJnLo0CEWLlzInTt3KFOmDBEREYSHh/9zAYwQgufPn6eUix06dIi3b98C34Ob/5aM1apVS+GCB0lJSbRo0YKQkBD8/Pz+WWUz00UXeJ7K+TDpQVcrD5fGmclt/WSkUil58uRh0aJFjBgxQu72ZMHnz59Zs2YNS5Ys4ePHj9jY2ODk5PTLeTunT5+mTZs2+Pr6plrKOikpicaNGxMdHc3du3ezjajIv8KzZ89o3rw5AF5eXpQvX/6na5J7n474BvMhQeOHQx9Z9vH+C6gyMyrkgjIzM/JoiJ1+LIAb/sEMHToUPT09XF1dmThxIs+fP8fFxSVTBTIAU6ZMoXLlytjY2CjblQxhb2/Py5cv5Tq48evXr6xevZqKFSvSo0cPihQpgqenJzdv3kQqlXLq1Cm52c4sJCYmcvv2bVasWEG3bt0oW7Ys5cuXp0+fPly8eDFlA3b06FEiIyNxd3dn3LhxNGzYUCmbLHV1dXbs2MGnT58YOnSowu1nBr7EJRImx0AGICzqm0JOiNXU1ChfvnyWkGf++PEjzs7O6OrqMn36dLp06UJwcDBbt2797eDQli1bUqpUqZ9mzvyJdevWcePGDTZu3KgKZLIg5cuX59KlS2hoaNC0aVOCg/+vwuN/B9t+TMzxU/Y6I4Nt/0VUwYwKuaDMnpnJ7v4kyriGPD4hEcu5+zhw4AAzZ87k+fPnzJgxI1MqhF2+fJnTp08ze/bsTNG3lBHq1KlD9erV5TJz5u3bt8yYMQMdHR1Gjx5NgwYNuHPnDmfPnqVly5aULl2axo0bpwzQVISct6L49OkTnp6eTJ8+nRYtWlCoUCHq1KnDhAkTePXqFX369OHYsWO8ffuWoKAg3N3dyZ07N48ePco0gyl1dHRYt24de/fuZc+ePcp2R+E8j/qawRGKf0cAoVHpGwaYVvT09DJ1mVlUVBRTp05FV1eXefPm0adPH0JCQli/fj3lypX7473q6ur06dOHvXv3Eh8f/1dbL168wMnJicGDB9O4cWMZPYEKRaOjo8OlS5fIly8fTZs2/T6a4f/38cpjsO2/jKrMTIVc0NTUZOnSpQwbNkyhdoMjP9Ny+WW5rX9scF2MdOWv7pNehBA0bdqUb9++4evrK/N5EMpgxYoVjB8/nvDwcIoVK5bh9Z48ecLSpUvZtm0bampqODg44Ojo+MsNyYwl61jj6Ufl5taEf4yVm5y3vAkLC0spGbt27Rr+/v5IpVK0tLRSysUaN25M7dq1f3sK3LZtWxISEtIt0ywvevfujYeHB35+fkoV3FA0d8M+YLXuutztuA9pRE0d+R/ajBw5Ei8vLx48eCB3W2khMjKSJUuWsHbtWoQQDBkyhHHjxqV5AHFAQADVqlXj8OHDWFlZ/fY6IQSWlpb4+voSFBREwYIFM/oIKpTMmzdvMDc3512JOuSs0yXD642zqMhws19nAf9Vsv5OR0WmRFmZGXk3xB6+/1Yua8uK06dPc/XqVVxcXLJFIAPQp08fJBIJO3fuzNA6N2/epGvXrlSqVImDBw8yZcoUwsLCWLFixU+BTHIZgOs7HfIat+Hl/wQykHnLABITE7l79y6rVq2iR48eaGtro6urS69evTh37hy1atVi06ZNPHz4kLdv33L06FEmTJhAo0aN/ljOYmFhwZUrV4iJiVHg0/ydNWvWUKhQIfr27UtSUpKy3VEYOTUU8/5WlB09PT1CQkLS3CQvL169esXo0aMpX74869evZ+TIkYSGhrJ48eI0BzIAhoaG1KlTh+3bt//xukOHDnHs2DFWr16tCmSyCcWLF2fkin0yCWTg/wbbqvg/ssduR0WmQiqVIoRQSs/MhUdvZC5TmkySVHDh8Ru5rC0LpFIpU6ZMoUmTJrRq1UrZ7sgMLS0tLC0t2bJlS5o3OkIITp48SbNmzahfvz73799n7dq1PH/+nKlTp/5ysNn/lgFI1P4sPKDsMoDPnz9z9uxZZs6cScuWLSlcuDC1atVi3LhxhIWF0bNnT44cOcKbN2949OgRW7duxc7OjkqVKqWpZMzCwoK4uLhMN9ywUKFC7NixgytXrrB48WJlu6Mwymnl/YvQfMaR/H87ikBfX5+YmBgiIiIUYu93hIWFMWzYMMqXL4+rqysTJkwgNDSUuXPnZjgzbGtry8mTJ1PENP6Xjx8/MmLECCwtLencuXOGbKnIPLx4/41FXqEyXXP6sYBMcXiWWVAFMypkTkJCAoDCMzPZqSE2PRw8eJC7d+8yd+7cTNPXICvs7e0JDAzEx8cnVdfHx8fj6upK9erVadeuHbGxsRw6dIigoCAGDRpE7ty/np8hbzlvWfDy5Uvc3NwYMWIEtWrVolChQlhYWLBq1Spy587N1KlTuXLlCtHR0Vy/fp2FCxfSqVOnDG/EqlSpQpkyZeQqxpBeTE1NmTBhAtOmTePOnTvKdkch5NXUQCcNQ33Tg45WHoXNLdHT0wOUJ88cEhLCgAEDMDAwYN++fcyYMYPQ0FBmzpxJkSJFZGKjZ8+eSCSS3/Z4TZw4MUWQREX2QR59vIlSwWR32Sm2ZnWydnewikxJYuL3zb6iMzOKbIg1LJ250v+JiYlMmzaNtm3bZsuGUXNzc3R0dNiyZQsNGjT47XWfPn1i48aNLF++nPDwcDp06MC6deto3LjxXwM8N98wFns+lom/iz0fUyyfZoqcd3pJSkrC39+fa9eupfwLC/ue+alQoQImJiYMHToUExMTKlWqJNfSQolEgoWFRaYMZgCcnZ3x9PSkd+/e3L59mzx55LvRVyZJSUnfZ/o8uI8oWfOv2cP0oK4mwayi4voDk6VrQ0JCFPoZ9ujRI+bOncvu3bspWrQoc+fOZfDgweTLl0/mtrS0tGjfvj2urq6MGjXqh9cuX77Mxo0bWbNmTaaYU6ZCNiQPtpU1SVLBlSfvePLmMwbFM2+/pqJQZWZUyBxlZWbiE6XZyk5a2LFjB48fP2bOnDnKdkUuqKmp0b9/f9zc3Pjy5ctPr7969YqJEyeira3N5MmTsbCwICAggGPHjtGkSZO/BjLykvNOaxnAly9fOH/+PM7OzrRq1YrChQtTs2ZNHB0defbsGV27duXw4cNERETw+PFjtm3bhoODA1WqVFFIj1TLli3x9/fn9evXcreVVnLmzMnu3bsJDQ1lwoQJynZHLnz+/JmVK1dSsWJFrKysyPv6jlwCGfi+WerTIGPBeFrIkycPpUqVUlhm5sGDB/Ts2ZMqVapw7tw5li5dyrNnzxg3bpxcAplk+vXrx927d/Hz80v5WWxsLAMHDqRRo0YMHjxYbrZVKJ7sMtg2s6MKZlTIHGVlZrJbQ2xqiYuLY+bMmXTr1o2aNWsq2x250b9/f75+/cqBAwdSfhYYGIidnR3lypVj/fr1DB48mNDQULZu3UrVqlVTvbayygDCw8PZv38/o0aNok6dOhQqVAhzc3OWL19Ojhw5cHJy4tKlS0RHR+Pt7c3ixYuxsrJS2hBPc3NzgEynaJZMlSpVWLJkCWvWrOHkyZPKdkdmPH/+nHHjxlG2bFnGjh1L/fr1uXnzJj6eR2liUFTmmyV1NQlNDIoq/MQ3WQRAnty9e5cuXbpQvXp1rl+/zpo1a3j69CkjR478bfmpLGnTpg3FihX7YebMvHnzCAkJYePGjdlGuEXFd/7lPl5FoiozUyFzlJWZSW6IlWepmSIbYlPLhg0bCA8Px9nZWdmuyBVdXV3Mzc3ZvHkzBgYGLFy4EA8PD8qUKcPcuXMZOHAgBQoUSPO6iioDSEpKIiAgIKVc7OrVqzx//hz43vxsYmLCwIEDMTExUVimJa0UK1aMWrVq4enpSd++fZXtzi8ZMmQIHh4e9O/fH39/f4oXz7xS6n/D29ubZcuWcfjwYfLnz8+QIUMYPnw4ZcuWTblmrlV1zJddkumGSUNNwlyr6jJbL7Xo6+v/MFxQlty8eZPZs2fj4eGBnp4emzdvxsbGhpw5c8rF3u/IkSMHvXr1Yvfu3SxYsIBHjx4xb948Jk2ahKGhoUJ9USFfFNnHq6jetsxK5vu2VJHlUVZmJrs1xKaGL1++MGfOHPr160elSpWU7Y5cSUpKSjlNbdq0Kc+ePWP79u2EhIQwbty4dAUyIN8yADUJjFx1kDZt2lCkSBFq1KjByJEjCQ4OpnPnzhw8eJDXr1/z5MkTXF1dGThwIIaGhpkykEnGwsKCs2fPIpVmvnJL+N7bs3XrVqRSKQ4ODplG6je1JCYmsm/fPho0aECjRo24d+8eK1eu5OXLl8yfP/+HQAZAu0geZnWU7SZ4VkdDtOX8Wfor5JGZuXr1Kq1ataJ+/foEBwezY8cOHj16hL29vcIDmWRsbW2JjIzk1KlTDBw4ED09PSZPnqwUX1TIj+w22DYzk3m/MVVkWZSVmQEwq1RcrvWpimyITQ0rVqwgOjqaGTNmKNsVuRETE8P69eupXLkyS5cuRUNDg86dO+Pv74+trW2GNyTyLAOQCvB/J0UikTBhwgQuXrxIdHQ0Pj4+LF26lC5duqRrZoUysbCw4M2bN/j7Z14lnZIlS7JlyxaOHz/Opk2blO1Oqvj48SOLFi1CT0+PHj16kCdPHo4fP87Dhw8ZOnQoefP+PiPco64O4ywqysSPD5dceXB0o0zWSiv6+vpERkby9WvGNmdCCC5cuICZmRlNmjTh1atXuLm5ERAQgI2NjVK+m/6LsbExRkZGzJgxg+vXr7Np0yZy5cqlVJ9UyJ5/uY9X0aiCGRUyR1mZGYDe9XXkWp+qyIbYv/H+/XsWLVrEkCFD0NHJPH7Jivfv3zNnzhx0dXUZNmwYNWvW5ObNmwwbNoyrV6+m/J1lBEWUAWgUKskB92NMmTIFU1PTLK+y1ahRI/LkyZNpVc2S6dixIwMHDsTR0ZHHj2WjUicPkvs1ypYty9SpU2nevDl3797Fy8uL9u3bpzpLN9ysAvM7V0dTQy3NBzrqahI0NdRY0Lk607vUx8XFBRcXl/Q8ToZIlmdOb3ZGCMGZM2do0qQJzZs3Jzo6msOHD3P//n26d++Ourp8xBLSikQiwdLSkrt379K3b1+aNGmibJdUyIF/tY9XGah+AypkjjIzMxVK5M9WDbF/YtGiRSQmJuLk5KRsV2RKaGgoo0aNQltbGxcXF6ytrXn06BH79++nbt262Nvb8+bNGzw8PDJsS1UGkHY0NTVp1qxZpg9mAJYuXUqZMmXo06dPyudSZkAIwaVLl7C0tKRChQrs3buXMWPG8Pz5c7Zv346xsXG61u1RV4dzjqY00vs+DPZvn4PJrzfS0+Kcoynd6+owduxYnJ2dmTp1KsuXL0+XH+lFX18fSHswI4Tg+PHj1K9fn9atW5OYmIiHhwe3b9/GysoqU5Zt3rx5EwAjIyMle6JCXmS3wbaZmcz3DleR5VFmZgbAqWV5pIkJMq2VV1ZD7O+IiIhgxYoVjB49WmnKVrLm7t279OrVCwMDA3bv3p0ywX7t2rUYGBikXFe9enXq1q3Lli1bMmxTVQaQPiwsLLhy5QrfvmXuCdR58+Zl9+7d3LlzJ1MIZMTHx7Nz505q165Ns2bNCA4OZuPGjYSFheHs7CyTkkPtInnYaV+fs6ObYlNfF12tPD9tqCSArlYebOrrcs6xKTvt6//QIzN16lQmTZqEo6MjGzZsyLBPqaV48eLkzZs31fLMUqmUQ4cOUatWLTp27EiuXLnw9PTE29ubdu3aZdrhwYcPH+b06dPUqlXrB3VGFdmLf7GPV1mofgMqZI4yMzMhISF069iRL/n0yWs2UGbrzuxQVSkNsb9jzpw5aGpqMm7cOGW7kiGEEJw9e5ZFixZx7tw5ypcvz/Lly+nfv/8fewTs7e0ZOnQo4eHhGRowpyoDSB8tW7Zk9OjRXLlyhVatWinbnT9St25dZs6cyYwZM2jdujUmJiYK9+Hdu3ds2LCBNWvW8Pr1a1q3bs2ZM2do2bKl3DbcFUrkZ2ZHQ2ZiyNe4REKjvhKfKCWnhhrltPL+cQMkkUiYO3cu3759Y8iQIeTJkwcbGxu5+Pm/dlMjApCUlMT+/ftxcXEhICCA5s2bc/HiRUxNTeXuY0aJjo5m+PDhdOzYkT59+tCtWzcePnxI5cqVle2aCjnQWL8wYe+/IuSQo8mMfbzKInt9w6rIFCgrM3Px4kXq1atHXFwcl7fNl1lD7MdLO/BcN5OkpCSZrJdRnj17xsaNG5k4cSKFChVStjvpIiEhgd27d1OzZk1atWrFhw8fcHNz4/HjxwwfPvyPgQxAjx490NTU/GFWQ2qQSqUEBQWxdetWBg4cSPe2zeWudpUdywCqVKlCmTJlskSpGYCTkxMNGzakT58+fPr0SWF2g4KCGDRoENra2syZM4f27dsTEBDAqVOnsLCwUFjmIK+mBoalC1JTpzCGpQum6iRXIpGwbNky7O3t6devn8IyCHp6er/NzCQmJuLq6krVqlXp1asX2traXLt2jfPnz2eJQAZg0qRJfP78mdWrV9OxY0cKFy6c5s8xFZmfpKQktm/fzvapDnIJZCDz9fEqE1Uwo0LmKCMzs3HjRlq2bImxsTE+Pj5UqVJFZg2xa4Z1ZMeOHfTt21cmTecZZdasWWhpaTFixAhlu5Jmvnz5wooVKzAwMKBPnz6UKlWK8+fP4+vrS/fu3VP9N1OwYEG6du2aIsH7O6Kjo/H09GTWrFm0bt0aLS0tqlatioODAzdu3KBRvdoUySnfIDU7lgFIJBIsLCyyTDCjrq7Ozp07iYqKYuTIkXK1lZxtbNu2LVWrVuXYse/iDy9evGDjxo1pGuaqbNTU1Fi/fj09evSgV69eMulT+xu/yszEx8ezadMmKlasSL9+/ahcuTI3b97k1KlTNGrUSO4+yYqrV6+yfv165s2bh7a2NpqamvTo0YOdO3dmmsMyFRnn7Nmz1KpVi/79+9PEyIDaZfL+E328ykQVzKiQOYrMzCQmJjJixAgGDRrEoEGDOHXqFEWKFEl5XRYNsb1792bv3r3s27eP3r17K7WRODAwkJ07dzJ16tS/Zi8yE5GRkUydOhUdHR3GjRuHqakp9+/f59SpUzRv3jxdJ9T29vY8ffqUy5cvA9+zLoGBgWzZsgUHBwcMDQ0pXLgwrVq1YuXKlairqzNmzBjOnj3Lx48f8fPzY+PGjXSqY/BPyXnLCgsLCx48eMCrV6+U7UqqKF++PKtWrcLV1VUuWYbY2Fi2bNmCkZERFhYWvH79GldXV0JDQ5k6dSpFixaVuU1FoK6ujqurKx07dqRLly6cO3dOrvb09fV59uwZSUlJxMbGpvTMDRo0iNq1a3Pv3j2OHj1K3bp15eqHrImLi2PAgAE0aNCAIUOGpPzc1taW8PBwvLy8lOidClnw4MED2rRpg4WFBfnz58fb2xs3NzeW96qHGiJb9/EqG4nIahPFVGR6PD09adWqFaGhoejq6srNzvv37+nWrRuXLl1i1apVDB48+I/XB0d+ZrdPGBcevyEs6tsPKlYSvp+gm1UsTp8GOr887XB3d6d79+60b98eNzc3pQxcs7a25vbt2zx69EhpA9/SwuPHj1myZAmurq7kyJGDAQMGMHr0aJlISX/48IFq1aqhpaVFqVKl8PHxITo6GjU1NapVq0bDhg1p0KABDRs2pGLFir8NmIIjP9Ny+eUM+/M7zjk2zZanZ+/evaN48eJs376dvn37KtudVCGEoHv37pw7dw4/P7+fBlCmh8jISNauXcu6det49+4dHTp0wNHREVNT00zbgJ4e4uPjsbKy4uLFi5w+fVpucsKnTp2ibdu2TJ8+nU2bNhEZGUmPHj2YMmVKlspq/S+zZs1izpw53L17l2rVqqX8XAhB1apVqV27Nrt27VKihyrSy+vXr5k+fTpbt25FT0+PBQsWYGVllfL+v3nzJu1Gusi0j3dB5+p0r6sqMUtGFcyokDknT56kXbt2hIeHU7p0abnYePjwIR06dOD9+/ccPHgQMzOzNN2f1obYZI4fP461tTUWFhYcPHgQTU3N9D5Cmrl16xZ169Zl+/bt2NraKsxuevD29mbRokUcOXKE4sWLM2rUKAYPHkzhwoXTtV5y1uXGjRt4e3vj7e1NUFBQyusWFhY0bdqUBg0aUK9ePfLnT1vwYLPFh+shUTKdUaSuJqGRnhY77evLbM3MRp06dahcuXKW2oS9f/8eIyMjKleujKenZ7ple/38/Fi2bBl79uxBQ0OD/v37M2rUKCpUqCBjjzMPMTExtG/fHl9fX86dO0e9evVkuv7nz59xdnZm8eLFqKmp0bdvX5ycnKhYUTb9j8oiKCgIY2Njxo8fz5w5c356ff78+Tg7OxMREUGBAgWU4KGK9PD161cWL17MokWL0NTUZMaMGQwePPiHg0ZfX19atmyJoaEhnadtZNWl0AzbHW9RiWFmBn+/8B9CFcyokDlHjx7F0tKSyMhIiheXfYnN6dOn6dGjB2XKlOHYsWMpswkUxenTp7G0tMTMzIzDhw+TO3duhdht1aoVL168wN/fP9MMf/svUqkUDw8PFi1axNWrV6lUqRLjxo2jT58+aZ5u/eHDB27cuJESvPj4+PDp0yfU1NSoXr06DRs2pGHDhujp6WFqasrq1at/KN1IKy/ef8N82SXiZCihrKmhxjlH00ylgidrJk+ezJYtW3j9+nWmnOXxO86fP4+5uTlLlixhzJgxqb5PKpVy8uRJli1bhpeXF2XLlmXEiBEMGDAg3YF6VuPLly+0atWKwMBALl68SI0aNTK8ZnR0NKtWrWLZsmV8+vSJxMRE5s2bx6RJk2TgsXKRSqU0bdqUt2/fcv/+/V9+Fr58+RIdHR02bdqEvb29ErxUkRaSm/unTZtGVFQUo0aNYvLkyT8J8iQHMlWrVuX06dMUKFAAN98wZhwLIFEq0nR4pq4mQUNNgnNHQ1VG5hdknW8fFVkGefXMCCFYtmwZ7dq1o3Hjxnh7eys8kAFo3bo1Hh4eXLp0iQ4dOihk1sbFixfx9PRkzpw5mS6QiYuLY8uWLRgaGtKpUyeEEBw5coTAwEAcHBz+GsgkJSXh7+/Pxo0bsbOzo0qVKhQpUoS2bduydu1aNDU1mThxIl5eXnz8+JF79+6xbt06+vbtS+PGjWnXrl2GZ85oF8nDrI6GGVrjf3HuaJitAxn4LtH85s0b/Pz8lO1KmmjRogVjxozByckpVb5//fqVdevWUaVKFTp06MCXL1/Yu3cvISEhTJgw4Z8JZADy5cvHyZMn0dfXp2XLlj9kSNPK+/fvmT59Orq6usyZM4eePXsSEhKCjo6OQlXn5MnGjRu5du0aGzdu/O1nYdmyZTE3N1epmmUBzpw5g7GxMQ4ODjRr1oxHjx6xcOHCnwKZW7du/RTIgGz6eFX8jCozo0LmuLm50bNnTz5//ky+fPlksmZcXBxDhw5l69atjB8/nnnz5il9U3/p0iXatWtHnTp18PDwkNmz/i9CCBo3bkxcXBy+vr6Zpg7/48ePrF+/npUrVxIREUGnTp0YP378X9WF3r9/n5JxuXHjBj4+Pnz+/Bl1dXWMjIxS+lwaNmyIvr7+X583ORN47969DJ8Sr74QzGLPxxlaA/6dMoC4uDiKFCnCjBkzmDBhgrLdSROxsbHUq1cPIQS+vr6/3GiGh4ezevVqNmzYQHR0NJ07d8bR0ZGGDRtmmvehsoiKiqJZs2ZERUVx5cqVNB0svXnzhqVLl7JmzRqkUimDBw9m3LhxlCpVCgAzMzNKlCiBm5ubvNxXCK9evaJKlSp069aNTZs2/fHaPXv20Lt3b548eaKUQzoVf8bPz4/x48fj6elJkyZNWLx48W/LLG/fvo25uTmVK1fmzJkzvy0dzGgfr4r/QxXMqJA5u3btwsbGhpiYmDSXF/2KN2/e0KVLF27evMmmTZsyVbPxtWvXaNOmDUZGRpw8eVIu9c4nTpygffv2nDlzBgsLC5mvn1ZevnzJ8uXL2bBhA/Hx8dja2jJ27FgqVar007VJSUkEBASk9Ll4e3vz+PH3YKFYsWI/NOnXqVMnXQFhQkICOjo6dO3alZUrV2b4+dJdBiABDXW1f64MoF27dsTFxcld5Uoe+Pv7U7duXYYMGcKyZctSfn7r1i2WLVvG/v37yZ07NwMGDGDEiBGUK1dOec5mQiIjI2natOn32V6XL/9V2OP169csWrSI9evXo66uzvDhwxkzZgzFihX74ToHBwf8/Py4efOmPN2XO126dOHatWsEBQX9NXv37ds3SpYsiaOjI7NmzVKQhyr+Rnh4ONOnT2fbtm1UqFCBhQsX0rFjx98eZiQHMpUqVeLMmTMULFgwVXbS28er4v8jVKiQMVu3bhWASExMzPBa9+/fFzo6OqJEiRLi+vXrMvBO9nh7e4uCBQuK+vXriw8fPsh07aSkJFGjRg1hamoqpFKpTNdOK35+fqJv375CQ0NDFCpUSEyePFm8fv36h2vevn0rPDw8xJQpU0Tz5s1Fvnz5BCDU1dVFrVq1xLBhw8TOnTvFkydPZPo8EydOFIULFxYxMTEyWS8s6qvos/mG0J3kIfQmnxC6kzx++0/P6fv/1h63XYRFfZWJ/azE8uXLRc6cOcXXr1nz2ZctWyYAcerUKXH48GHRuHFjAYjy5cuLZcuWiejoaGW7mKl58eKFKF++vDAwMBCvXr365TVhYWFi2LBhQlNTUxQsWFBMnz5dREVF/XZNFxcXUaRIEXm5rBDc3d0FIPbt25fqe+zt7UW5cuVEUlKSHD1TkRo+ffokpk2bJnLnzi2KFi0qVq9eLeLj4/94z+3bt0XhwoVF/fr1xcePHxXkqQohhFAFMypkzsaNGwWQ4c2qu7u7yJs3r6hZs6YICwuTkXfywdfXVxQuXFjUrl37j1/SaWXv3r0CEFevXpXZmmlBKpUKLy8v0aZNGwEIbW1tsXTpUvHp0yeRkJAg7t69K9atWyf69u0rKlSoIAABiOLFi4tOnTqJ+fPni4sXL4ovX77I1c9Hjx4JQOzZs0em6z6O+CRmHH0gmi7yEuX+J4gpN8lDNF3kJWYcfSDGOS8WOXPmFC9fvpSp/axAYGBgSjCQFfn48aOoVKmSUFdXF4Bo3LixOHTokEwOY/4VQkJCRJkyZUTVqlXFmzdvfvj5gAEDRI4cOUSRIkXE7NmzU7XJc3NzE4DMD4cURXR0tChdurRo3759mr4HL1++LABx8eJFOXqn4k8kJCSIDRs2iBIlSghNTU0xadKkVP3NJgcy9erVUwUySkAVzKiQOWvWrBE5cuRI9/1SqVTMmTNHAMLa2lruG2FZcffuXaGlpSWMjY3F27dvM7xefHy8MDAwEO3atZOBd2kjISFB7Nu3T9SpU0cAwsjISKxZs0YcPnxYTJ48WZiZmYm8efMKQGhoaIjatWuL4cOHi927d4unT58qsTbVEwAA0WpJREFUJYvUpEkT0aJFC7mt/yU2QTwI/yjuPH8vHoR/FF9iE1Jei46OFkWKFBHDhw+Xm/3MilQqFWXLlhVjxoxRtitp4tmzZ2LMmDGiQIECQkNDQ+TMmVOYmZkpPQOaVXn06JEoUaKEMDY2Fjdv3hS2trZCXV1dFC9eXCxYsEB8+vQp1WvdvHlTAOL27dty9Fh+DB06VOTLl088f/48TfdJpVKhp6cn+vfvLyfPVPwOqVQqTpw4IapWrSoA0adPHxEaGpqqe+/cuSMKFy4s6tatm2UD8KyOKphRIXNWrFghcufOna57v337Jnr06CEAMXPmzCyXbvf39xfFixcX1apVE5GRkRlaa9OmTQIQ9+7dk5F3f+fr169i9erVonz58gIQlSpVEi1atBAGBgYpWZcSJUoIS0tLsWDBAnH58uVMU160fft2AYiQkBCl2J8zZ84/m52xs7MT1apVU7Ybf0UqlYpr164Ja2troaamJgoXLiwmTZokXrx4IQ4ePCgAsW3bNmW7mWVxd3cXOXPmTPmcWLZsWbo+H969eycAceDAATl4KV+uXbsmJBKJWLFiRbrunzVrlsiXL1+WOcTLDty9e1e0aNFCAKJZs2bi1q1babq3SJEiqkBGyaiCGRUyZ/HixaJAgQJpvu/ly5eiTp06Infu3GL//v1y8EwxBAYGipIlS4oqVar8tob8b8TExIiyZcuKHj16yNi7XxMYGCh69OghcuXKlZJtSf7funXrihEjRog9e/aIZ8+eZdqT6y9fvoj8+fOLadOmKcV+dHS0KFy48D+ZnUkuCwoPD1e2K78kPj5e7N27V9SrV08AomLFimLt2rU/bRj79+8v8uXLJ548eaIkT7Mm9+7dE9bW1kIikYgSJUqIXLlyicaNG6f7oEMqlYqCBQuK+fPny9hT+RIbGyuqVq0q6tevn+4yxWfPnglA7NixQ8beqfhfXrx4IWxtbYVEIhGVK1cWx44dS9P3W3IgU6dOHVUgo2RUwYwKmTN//vw0N2/6+PiIUqVKibJly2bZ0oL/8ujRI1GmTBlRoUIF8eLFizTfv3TpUqGuri4ePXokc9/i4+PF7du3xerVq0XHjh1FgQIFUrIuefPmFa1atRILFy4UV65cEd++fZO5fXkycOBAUbZsWaX1O/yr2Zm3b98KiUQitm/frmxXfuD9+/diwYIFQltbWwCiefPm4vjx47/N+H769Eno6emJhg0bioSEhF9eo+L/8PX1FR07dhSA0NPTE5s2bRJxcXHi2rVrIm/evMLCwkLExsama+2aNWuKgQMHythj+eLs7Cw0NDTE/fv3M7SOqampXEtm/3Wio6PFlClTRK5cuUSxYsXEunXr0vx+v3fvnihSpIioXbu2eP/+vZw8VZFaVMGMCpkze/ZsUaJEiVRfv2fPHpErVy7RoEGDn9SxsjJPnjwROjo6Qk9PL9W1t0J831AVLVpUODg4yMSPiIgIceTIETFx4kTRtGlTkSdPHgEINTU1AYhcuXKJrl27irt372barEtq8fHxUWoz+r+cnaldu7bo1auXst0QQggRHBwshg8fLvLmzSty5swp+vXrl+pyzWvXrgk1NTXh7OwsZy+zLteuXROtW7dOyXK5urr+tBn08vISuXLlEh07dvyrCtSvsLa2zlIb+qCgIJEzZ07h5OSU4bW2bt0qJBJJmntuVPyZhIQEsXbtWlGsWDGRK1cuMWXKlHSpFd67d09oaWmpAplMhCqYUSFzpk+fLsqUKfPX65KSksTkyZMFIGxsbGQmq5uZePbsmShXrpzQ1dVNdS+Hs7Oz0NTUTJeCW3x8vPD19RWrVq0SvXr1Sul9AUTp0qWFiYmJ0NfXT5GeXbduXZbLvvwJqVQqqlWrJqytrZXmw5w5c4SmpuY/l51xcnISxYoVU1qfm1QqFRcuXBAdO3YUEolEFCtWTEyfPj1dByTTp08X6urq4saNG3LwNGuS/Ptt3ry5AIShoaHYu3fvH7OgJ0+eFDly5BDdu3dPc7Z0woQJonz58hl1WyEkJSWJJk2aCAMDA5l8nn769EnkyZNHzJkzRwbeqZBKpeLYsWOicuXKQiKRCFtb23QrpN6/f19oaWmJWrVqqQKZTIQqmFEhcyZPnizKlSv3x2s+ffokOnXqJCQSiVi4cGGWzwj8ibCwMKGvry/Kli0rgoOD/3jtu3fvRIECBYSjo2Oq1n79+rU4fPiwmDBhgmjSpInInTu3AESOHDlE/fr1xejRo8Xu3bvF0qVLRbVq1QQg6tatKw4ePJhtpWeXLVsmcuTI8YNErCJJzs6MGDFCKfaVxYULFwQg7ty5o1C7cXFxwtXVVRgbG6dssjdv3pyhw5H4+HhRr149YWBgID5//ixDb7MeUqlUnDlzJmX+jrGxsTh48GCqg9bDhw8LdXV10a9fvzQFuhs2bBDq6urpyuoomg0bNghAeHl5yWxNGxsbUaFChWz93agIbt26JZo1a5ZSZpqRzyc/Pz9RtGhRUbNmTZmOYFCRcVTBjAqZM378eGFgYPDb1589eyaqV68u8uXLJ44fP65Az5THy5cvRaVKlUSpUqXEHb+A30r8TpgwQeTLl++XG/G4uDhx8+ZNsWLFCtGzZ09Rrly5lKxLmTJlhLW1tViyZIm4fv26iImJEdHR0WLx4sWibNmyAhDt2rUTFy9ezPZfjm/fvhU5cuQQS5cuVZoP/2J2JjY2VuTNm1dhTdtv374Vs2fPFiVLlhSAaN26tfD09JTZ3/fjx49F3rx5ZVbumdWQSqXi+PHjon79+imHIGltkE5m9+7dQiKRiCFDhqT6/rNnzwog04sxvHr1ShQsWFDmcsrnzp0TQKYdFp3Zef78uejTp48ARNWqVcWJEycy9NmgCmQyN6pgRoXMcXR0FFWqVPnla5cvXxZFixYVenp64sGDBwr2THk8jvgkxu31EeVGbBc6E4//PHxxoZcYu8dH5C1tkKLG9erVK3Ho0CExbtw40bhx4xSlsZw5c4qGDRsKR0dHsX///p8EBl69eiUmTpwoChYsKHLkyCH69ev3T/2uhRCia9euwtDQUGmB27+anWnXrp3c+xwCAwPFwIEDRa5cuUSuXLnEwIEDRWBgoFxsJcuju7u7y2X9zEhSUpI4dOiQqFmzpgCEiYmJOH36dIbfS1u2bBGAGDt2bKrWCgkJEYDw9PTMkF15Y21tLYoXLy7zDW5SUpLQ1tYWgwYNkum62Z2PHz+KSZMmCU1NTVGiRAmxYcOGDIt5+Pv7i6JFiwpjY2NVIJNJUQUzKmTO8OHDRfXq1X/6+ebNm0WOHDmEqampTIZKZgXCor6KPptvCN1JHkJv8okfgpif/k08JnQneYhqI9YLnSo1U7Iu2traolu3bmLp0qXC29v7t+pAQUFBwt7eXuTMmVPkz59fjB8//p/KDPyX06dPC0CpPQ+zZ8/+57IzK1asEDlz5pT57KHkUqfkpvNSpUqJOXPmyP1zRCqVik6dOgktLa10y6xnFRITE4Wbm1tKOaqZmZnw8vKS6YHAqlWrBCCmT5/+12sTEhKEhoaGWLdunczsy5qjR48KQOzdu1cu60+ZMkUULFgwW/U1yov4+HixatUqUbRoUZE7d24xbdq0NA1q/R3+/v6iWLFiwtjYWLx7904GnqqQB2qoUCFjEhMTyZEjxw//39HREQcHB+zs7PD09KRo0aJK9FAxuPmGYb7sEtdDogBIkoo/3yD5/nb8nLsU6h1nMm7dYV6+fElYWBj79u3D0dGRBg0aoKmpmXKLEIKrV6/SqVMnqlSpwqlTp5gzZw4vXrxg4cKFlClTRm7Pl5kxNzdHR0eHLVu2KM2HESNGkCdPHhYsWKA0HxSNhYUF8fHxXL58WSbrxcTEsHnzZqpXr06rVq2IjIxkx44dhIaGMmXKFLl/jkgkEjZv3kyOHDmws7NDiL+8h7MgiYmJ7Ny5E0NDQ3r06EGZMmW4evUqXl5emJmZIZFIZGZr+PDhLFiwAGdn57++LzQ0NNDV1SUkJERm9mXJp0+fGDp0KG3btqV79+5ysdG3b1+io6M5duyYXNbPDgghOHr0KNWqVWPkyJF07NiR4OBgnJ2dyZ8/f4bWDggIoHnz5pQuXZpz586hpaUlI69VyBpVMKNC5iQkJKChoQHAx48fad++PatWrWL16tWsW7eOnDlzKtlD+bP6QjCTDvsTlyj9exDzv6ipI5Woc+B5Ttwff/vlJUlJSbi7u9OoUSOaNGnCkydP2LZtG8+ePWP8+PEULFhQBk+RdVFXV6d///64ubnx9etXpfhQsGBBxowZw8aNG3n16pVSfFA0lSpVomzZsnh6emZonYiICKZPn46Ojg4DBw5EX1+fCxcucPv2bWxsbBT6GVK0aFG2bdvG6dOnWbNmjcLsypv4+Hi2bNlC5cqV6du3LxUrVsTHx4fTp09jYmIiN7sTJkxgxowZTJo0iVWrVv3xWj09PZ4+fSo3XzLClClT+PjxI2vXrpVpwPdfKlasSMOGDXF1dZXL+lkdX19fmjVrhqWlJbq6uty9e5ctW7bI5BAvMDCQ5s2bU6pUKVUgkwVQBTMqZE5yZubx48c0aNAg5Qty2LBhcvvQz0ys87zPYs/HMllrsedj9vmGpfz/2NhYNm7cSNWqVencuTM5c+bEw8MDf39/+vXr908Eiqmlf//+fPnyhQMHDijNh38tOyORSLCwsEh3MHP//n369euHrq4uS5cupUePHjx+/JijR4/SrFkzpX1+tG7dmuHDhzN+/HgCAwOV4oOsiIuLY926dVSoUAEHBweMjY25e/cux44do169egrxYcaMGYwbN46RI0eyefPm316nr6+fKTMz3t7erFmzhjlz5qCrqytXW7a2tpw5c4bXr1/L1U5WIjQ0lF69elGvXj3ev3/P6dOn8fT0pEaNGjJZPzAwEDMzM0qWLMn58+f/iUqSrI4qmFEhcxISEvj8+TP169cH4ObNm5ibmyvZK/kQFxeHt7c3y5Yto1u3bmhXNmae51OZlqNMPxaAf8grXFxcKFeuHIMHD6Z69ercuHGDS5cu0a5dO9TUVG/l/0VXVxdzc3OllpolZ2c2bNjwz2RnLCwsCAgIIDw8PFXXS6VSPDw8aNGiBcbGxnh5eaWUSq5atQoDAwM5e5w6Fi5cSPny5enduzdxcXHKdifNfPv2jRUrVqCnp8ewYcNo1KgR/v7+HDx4EGNjY4X6IpFIWLhwIUOHDmXgwIHs3r37l9clZ2YyU3lffHw8AwYMoHbt2owYMULu9rp3706OHDnYtWuX3G1ldj5+/MiECROoVKkSFy5cYPPmzdy7d49WrVrJzEZQUBDNmzenRIkSqkAmC6HaAamQKUIIgoKC8PPzo0GDBty4cYMKFSoo2y2Z8eLFC/bv34+joyMNGzakQIECNGrUiMmTJxMREUFpy7Fo5Mgp0xPk+IREWk3bzuzZs7GysuLRo0ccPHgwJVhU8Xvs7e25evUqDx8+VJoP/1p2pkWLFkgkEs6ePfvH675+/cratWupUqUKHTp04OvXr7i5ufH06VPGjx9P4cKFFeRx6sidOzd79uwhICCA6dOnK9udVPPlyxcWLVpE+fLlGTt2LObm5gQFBbF3716qVaumNL8kEgmrVq3C1tYWW1tbDh8+/NM1enp6fP78maioKCV4+GsWLVrEw4cP2bx5M+rq6nK3V6hQISwtLXF1dc1UQZ0iiY+PZ+XKlejr67NmzRomT55McHAw9vb2Mv1vEBQUhJmZGcWLF1cFMlkMifhX3x0qZE58fDxDRoxm55HTlC6rw/Gj7ugVy09eTQ1lu5YuYmNjuXPnDt7e3nh7e3Pjxo2U0+by5cvTsGFDGjRoQMOGDalRowah72NpuVw2jc+/Yl/fatSvIt+ShuxGXFwcpUuXxt7enoULFyrNj9mzZ+Pi4kJISAilS5dWmh+Kom7dulSoUIE9e/b89NrLly9ZvXo1GzduJDo6mi5duqQcDmQFFi1axMSJE/Hy8qJZs2bKdue3REdHs3r1apYtW8anT5+wtbXFyckJPT09Zbv2A0lJSfTu3ZvDhw9z5MgR2rZtm/LavXv3qFmzJjdu3MgUhzePHz/GyMiI0aNHM3/+fIXZPXXqFG3btuXWrVvUrl1bYXaVjRACd3d3Jk6cSEhICHZ2djg7O1OqVCmZ23r48CHNmjWjWLFieHl5UaxYMZnbUCE/VMGMigwTHPmZzZceccj7IQmahX7ISkgAnSJ5MKtUnN71dahQImPqIvJCCMGLFy9SAhdvb2/u3r1LQkICuXPnpm7duinBS4MGDShZsuRPa8w8FsBOn+dpb/hPBepqEmzq6zKzo6HM187ujBo1Cjc3N16+fPmDyp4iiY6Oply5cvTt25cVK1YoxQdFMmXKFDZt2kRERERKCaSvry/Lli3jwIED5MmTBwcHB0aMGEG5cuWU62waSUpKwtzcnCdPnuDn55fpMkgfPnxgxYoVrFixgpiYGOzt7Zk4cSI6OjrKdu23JCQk0LVrV86cOcOJEydo3rw58F0xrGDBguzZs4eePXsq1UepVErz5s15+fIl/v7+5M6dW2G2ExMT0dbWpmvXrqxcuVJhdpWJj48PY8eO5dq1a7Rp04aFCxfKLZP48OFDzMzM0NLSwsvLi+LFi8vFjgr5oQpmVKSbF++/MdndnytP3oE0CdR+n+5VV5OQJBU0MSjKXKvqaBfJo0BPfyYmJuaHrIu3t3dKg6Wenh4NGzZMCV6MjIxStQk2XXSB/8feecfluP9//HkX2Q4yQ2VknDJDdjuyyazIMY8tM9mjkJQ9j5ERmdkk3WWeZJMRmoQoq6F5//7wqy/HatyrXM/H4zzO+X7vz/V5v++6u6/r9XmviLjvdx+TBlrqxQmYYiyz/Qsqd+7coVGjRhw+fJgePXoozI/fKToTEBCAkZERQUFBREZG4u7uzsWLF6lRowYTJkxgyJAheW6bqkgiIyNp2LAhnTp1+m70SRG8fv0ad3d31qxZQ1paGiNHjmTq1Kn55rOWnJxM9+7duXjxImfOnMnqqFahQgUmTJjArFmzFOrfli1bGDZsGL6+vpiamsrd/rRp09i6dSvR0dEFutFLaGgojo6OeHl50bBhQ1xdXTE3N5eZvUePHmFkZCQImXyOIGYEcsXeoEjmHg0mNS2dDLJfH6KqIqKQioj53XTp31w+J4USiYSIiAj+/fffLOFy69YtUlNTKV68eFbUpVWrVhgYGFCpUqUc24hPTqPBvDPI8o9JBNyb1yHfpu0pkubNm1O5cmWOHTumMB9+p+jMmzdvqFq1KiVKlODt27e0a9cOe3t7unXrJpc6A3mwd+9eBgwYwK5du7CxsfnumoTkNMJjE0hJy0CtkAra6iWk/vf78uVLXF1dWb9+PSKRiDFjxjB58uR8+VCWmJhIp06duHnzJufOnaNZs2YYGBigq6vL1q1bFebXy5cvqV+/Pt27d2f79u0K8SE4OBg9PT0OHTpEz549FeKDLImLi8PJyYnVq1dToUIFFi1axKBBg2T6ffHo0SOMjY0pW7YsYrE4X/7NCHxGEDMCOWaN+PH/tx6WQA6EzH+ZYlGHscbSbw6QlJTEtWvXvhIvL1++BD63+swULq1ataJBgwZZM3HyQnD0ezqvvpjnfX7FiXFt0dX4vWfI5IYNGzYwZswYoqKiFHpSvXDhQpydnQkNDZVJ3reiCQ8PZ9WqVfzzzz/Ex8dTsWJFTpw4UWDz/G1tbTl27Bh37tzJatH7+NVHdgdGIn4UQ2Rc4lcHHNJMu3327BkuLi5s3rwZNTU1xo8fz8SJE/P9PIyPHz9iYWFBSEgI/v7+LF68mOfPnxMQEKAwn/r164dYLObBgwcK/fk2a9aMatWq4e3trTAfpE1ycjLr1q1j4cKFpKSk4ODggL29PSVKlJCp3ZCQEIyMjChbtix+fn65OsQUUB4EMSOQI/YGReJw6K7U9lvaqwH98hChkUgkhIeHZxXoZ0Zd0tLSKFGiBC1atMgq0m/ZsqXMivpuRr6l5/rLMtn7Sw6Pak0TTeXK0c8PvH//nipVqjB79mxmzJihUD+0tbWxs7NjxYoVCvNDmkgkEi5fvoy7uzuHDx/mjz/+4O+//6Zo0aI4OTkRFxcn8wcTRfH+/XsaNmyItrY2HgeOM/vofS48eZOVVvsj8pJ2Gx4ezpIlS9i2bRslSpTA3t6ecePGUaZMGSm8I+Xg3bt3mJiY8Pz5c3r16sXx48eJiopSiC/Hjx+na9eu7N69G2tra4X4kMnq1auZNGkS0dHR+b5AXSKRcODAARwcHAgPD2f48OHMmzfvu/Wo0ubx48cYGRnxxx9/IBaLBSFTABDEjEC2iYpLxNTNn5S0DJBS6+EihVTwtTfM9s08MTGRa9eufdVh7NWrVwDo6Oh81WFMT09PKlGX7CBEZpSfQYMGceXKFUJCQhQ6vLWgRGdSU1M5ePAgbm5uBAUFUadOHSZOnMigQYMoUaIEDx8+pH79+pw8eRJLS0tFuyszzp8/T+fxzlS0HAMqqjlqAJKTtNsnT57g7OzMzp07KVu2LJMnT2b06NH5uvboZ7x58wZDQ0Oio6N5//49iYmJFC1aVK4+fPz4EV1dXXR1dTl58qTChz6/efMGDQ0Nli1bxoQJExTqS164fPkyU6ZM4cqVK3Tu3BkXFxf+/PNPudjOFDKlS5dGLBbLRTwJyB5hzoxAthnrcYnk1DSpCRmAtAwJjoe/H+mRSCSEhoaye/duxo4di76+PqVLl8bQ0JBFixbx8eNHhg0bxrFjx3j9+jUhISF4eHgwatQoGjduLDchA6CtXiIPCXfZQyKR4HfES8ZWCi5Dhw7lyZMnnD8vu/bZ2WH8+PEULVo0386defv2LS4uLtSsWZMBAwZQunRpjh8/zoMHDxg1alRWFKZu3bpUr14dHx8fBXssW+6kV0Hdchxpkp9HY75HeoaE5LQMHA7dZY348XfXPHjwAFtbW+rWrcupU6dwcXEhLCyM6dOnF1ghA1C+fHl8fX0pUaIEEomEf//9V+4+zJo1i9jY2Kx6JEVTvnx5unTpgoeHh6JdyRVPnjyhT58+tGnThqSkJHx9fTl+/LjchMyTJ08wNjYWhEwBRIjMCGSLNTsO4PpAdq0ofe3bU6WEyjdRl5iYGADq1KnzVYcxPT09pSsklnU3sxIZCdx36cfq1asZO3aszOwUVCQSSdbnaMeOHQr1ZcGCBSxevDhfRWceP37MypUr2bZtG2lpadjY2DBx4kQaNmz4w2uGDRvGlStXCA4OlqOn8kOWabd37txh0aJFHDhwgKpVq+Lg4MDQoUPlHp1QNIGBgbRs2RINDQ1u3Lght5SgwMBAWrVqhaurK5MmTZKLzexw5MgRevTowZ07d2jQoIGi3ckWsbGxLFy4kHXr1lGpUiWcnJywtbXNatsuD548eYKRkRElS5ZELBbnm+9dgewhiBmBn5KRkcH8+fNZffklpfW7gEj6Xz4iSQZqkYE83beE9PR0SpYsiYGBwVcdxvJDUaus58zYGmgSH7ANNzc33NzcsLe3l7qdgs6SJUtYsGABL1684I8/FJeu9+7dO2rUqKH0tTMSiQR/f3/c3d05fvw45cuXZ/To0YwaNSpbD5X79u2jX79+PHv2jKpVq8rBY/kRFZeImXsAyWkZUtuzSCEV3M3Lsdl9Cd7e3mhrazNjxgzs7OwoUqSI1OzkJzIyMihatCjFihVDU1MTf39/md8PUlNT0dfXR01NjX///VeuUf5fkZKSQtWqVbGzs8PV1VXR7vyU5ORkVq9ezaJFi8jIyGDGjBlMnDhRrjN6AJ4+fYqRkRElSpQQhEwBRUgzE/ghCQkJ9O3blwULFlCteQeZCBkAiUgFlap6rFu3jjt37vDu3Tt8fX1ZuHAhnTp1yhdCBsDGQFMmQgY+p6MMbKmFq6srDg4OTJo0Kd+mKSkSOzs7UlJS2LNnj0L9KFOmDPb29mzcuDFrvpEykZycjIeHB02bNsXExITQ0FA2b95MZGQk8+bNy/bpuKmpKSKRiLNnz8rYY/njePguaVL+e09OTeOvdT4EBwezbds2QkJCGDFixG8rZABUVFSoWbMmPXr04NWrV1hYWPDu3TuZ2nR1deX+/fts3rxZqYQMgJqaGjY2NuzatYu0tDRFu/NdJBIJe/fupV69ejg4OGBtbc2TJ0+YMWOGwoRM8eLFBSFTgBHEjMB3iYyMpG3btpw+fZq9B735KJHtzfRToVLY2A2hQYMGSpc+ll10KpWiXe3yqKpIN7daVUVEu9rlqV2xFCKRCGdnZ+bMmYODgwMLFy6Uqq2CTpUqVejUqRNbtmxRtCtZtTMuLi6KdiWL169fs3DhQrS1tRk8eDBVqlTBx8eHu3fv5irFSV1dnWbNmhW4upnHrz5y4ckb6R9eiFQoVqMpxwKCGDx4cLaG9f4O1KpVi7dv33L27FnCwsLo1KkT8fHxMrH1+PFj5s+fz6RJk2jSpIlMbOQVOzs7Xr16xZkzZxTtyjdcuHCBli1bMmDAABo2bMi9e/dYt26dQma4hIaGYmxsLAiZ3wBBzAh8w+XLl2nevDlv377l8uXL6LU0kukwSPg8sSY8NkHGVmSPc88GFJKymCmkIsK55/9yo0UiEfPnz2fhwoXMmTOH2bNnI2SLZp+hQ4dy7do1bt++rVA/MqMzGzZsUHh05v79+4wYMQJNTU0WL15M9+7duX//PidPnsTc3DxPxc/m5uacPXuWjAzppWMpmt2BkVI/tMhEVUXEnqBnMtk7v1KzZk1CQ0Np1KgRZ86c4d69e3Tr1o2kpCSp2pFIJIwcOZKqVasyb948qe4tTRo3bkyDBg2UqhFASEgIvXr1on379qSnpyMWizly5Aj16tVTiD+hoaEYGRlRtGhRxGKxQueLCcgeQcwIfIWHhwfGxsbUrVuXoKAgGjZs+LkVsxyQlx1ZUr1cceZ305Xqngu66X63dfWsWbNYunQpixYtYsaMGYKgySadOnWiUqVKv310RiKRcObMGTp27Iiuri7Hjx9n9uzZREVFsWHDBurXry8VOxYWFrx584Zbt25JZT9lQPwoRqYppeKQGJnsnV+pVasWoaGhSCQSmjdvzsmTJwkMDKRXr14kJydLzc727dsRi8Vs2LCB4sWzP/tH3ohEIuzs7Dhy5Ahv375VqC9v3rxh/Pjx6Orqcv36dXbt2sXVq1cxMjJSmE9hYWEYGxtTpEgRQcj8JghiRgCA9PR0pk6dyuDBgxk4cCC+vr5ZQ7nUCsnnYyIvO7Kmf3NNJpvXAcizwJhqUfenQ0WnTZuGm5sbS5cuZfLkyYKgyQaFCxfGzs6OXbt28enTJ4X6UqZMGSZOnCjX6ExSUhKbN29GT0+Pjh07EhMTw86dOwkPD8fR0VHqNWqtWrWiRIkSBSbVLD45jUgZdi0EiIxNJCFZOeshFEHNmjVJSkri5cuXALRt25YjR44gFosZMGCAVGpHXr16xeTJkxk4cCDm5uZ53k/W2NjYkJ6ejpeXYtr1f/r0CRcXF2rVqoWHhweLFi3i4cOH2NjYyLVL2X8JDw/HyMiIwoULIxaLC1zjEYHvUzCeHgXyxIcPH+jWrRtubm6sWLGCzZs3o6amlvW6PGaoiP7fTkGhVOQlYk+uorAKOU5HUVURUaSQCkt7NWCMce1frre3t2fNmjW4u7szbty4ApXOIyuGDBnC27dv8fb2VrQrTJgwgSJFisg8OvPy5UvmzJmDpqYmI0eOREdHB39/f65fv46tre1Xf/PSRE1NDWNj4wIjZiJiE4S0WzlTq1Yt4HMxdyZmZmYcPHiQY8eOYWdnR3p6ep5sTJw4EVVVVdzc3PK0j7yoXLkyHTt2ZPv27XK1m5GRgaenJ3Xr1mXmzJkMGjSIJ0+eMH36dLkX9/+XL4WMv78/1apVU6g/AvJDEDO/OU+ePKFly5ZcunSJkydPMmHChG/y40sUKYTmd9KcpImmenFKFFGurjG5JTw8nPHjx2PVpAriKSa0rvn5pPtXoibz9dY11fG1N/xpROa/jBkzho0bN7J27VpGjRolCJpfULduXdq2basUqWayrp25ffs2gwcPRktLCzc3NwYMGEBISAje3t4YGhrKZRighYUFFy9eJCEh/z+gC2m38qdGjRrA5zqIL+ncuTN79uxh7969jBw5MtffeydPnmTv3r24u7tTvnz5PPsrL+zs7AgMDOTRo0dysRcQEICBgQE2NjY0bdqUe/fusXr16qwsDkUSERGBsbExqqqqiMViQcj8Zghi5jdGLBZjYGBAWloagYGBdOjQ4YdrjetWlGnBq3Ed+Xc6kQUZGRkMHjyYsmXLsnLlSqqXK87OoQacndiegQZaaKkXh/+c64oALfXiDDTQwte+PTuHGny3RuZXjBgxgq1bt7J582aGDh2a55PKgs7QoUPx9fUlPDxc0a5IPTqTkZHBsWPHMDExoXHjxojFYpycnHj27BmrVq2idu1fR/ykiYWFBampqZw/f16udmWBkHYrf4oXL06VKlW+ETMAvXv3xsPDg61btzJx4sQcp9rGx8czatQozM3NsbGxkZbLcqFr166UKVMmqxFAQnIawdHvuRn5luDo91JLVXz48CHdu3fHyMgIkUjE+fPnOXz4MHXr1pXK/nklIiICIyMjVFRU8Pf3p3r16op2SUDOFIyjcIEcs379esaPH4+RkRH79u2jbNmyP10/oEU1tl8Jl4kv6RkSbFtmPwqhzLi7u3P+/Hn8/Py+GsqoU6kU87rpMg9dFi52YfW2vZzxPYdaIRW01UtILSr1119/ZdWEpKamsn37dqWbk6As9OnTh/Hjx7Nt2zbmz5+vUF8yozNLlixh+vTpVK5cOVf7JCQksH37dlauXMnjx49p2bIlXl5e9OrVS6Gfgzp16qCpqYmPjw+WlpYK80MaZKbdyjLVrKCl3UqDmjVrfpVm9iW2trYkJiYycuRIihUrxpIlS7IdcZw9ezavX79GLBbLJUopTYoWLUoXm+HsepDMxWV+RMUlffW5FAGa5YpjXLciNgaa6FQqlaP9Y2JimD9/Phs3bqRatWrs2bOHvn37KrQm5r9ERkZibGyMSCRCLBYLQuY3RXk+kQJyITU1lTFjxjB69GhGjx7NqVOnfilkHj16xLA+XUgKu4FIIt3Uhy9nqOR37t27h6OjI/b29j/t5JKRkkThhFc00SyLrsYfUk+vs7W1xdPTk71792Jra0tqaqpU9y8olChRgv79+7Nt2zaliGLlJTrz7Nkzpk+fTrVq1ZgwYQJNmjThypUrXLlyhb59+ypc0IpEIiwsLApE3YyQdqsYMjua/YgRI0bg7u6Oi4tLtudvBQUFsWrVKhYsWEDNmjWl5apciIpLZOCWQC6UMkRUx5DI/wgZ+Cy4I+IS2RkYgfmK8wzcEkhUNppXJCUlsXjxYmrXrs3u3btZsmQJDx8+pH///konZDLvtf7+/mhqFoxDUYGcozyfSgGZExcXR8eOHdm0aRMbN25k5cqVP33ISU1NZcmSJTRq1IiXL1+yZlAb1ApL9wb73xkq+ZWUlBRsbW3R0dHBycnpp2uTk5NlPtG7X79+7Nu3j4MHDzJgwABSUlJkai+/MnToUKKiovD19VW0K1nRmfXr12d1bfoVV69eZcCAAWhra7NhwwaGDh3K06dP8fLyomXLljL2OGeYm5tz//59nj3L/zNU2tQsg0hGsZmClHYrTX4Wmclk4sSJODk5MXfuXFxdXX+6NjU1lWHDhtGoUSMmTpwoRU9lz96gSMzcA7gcGguASOXng6Yz24hfDo3FzD2AvUGR312XkZHBzp07qVu3LnPmzGHIkCE8ffqUKVOm5HhgrqyJiorC2NgYiUSCWCwWhMxvjiBmfhMePHhAixYtuH37Nr6+vowYMeKn62/evImBgQEzZ85k/Pjx3L59mz6dTOQ2QyW/MW/ePO7fv8/OnTt/+aUvDzED0KtXLw4dOsSxY8fo06ePVOcxFBRatGiBrq6uUjQCgOxFZ9LT0zl48CBt27bFwMCAq1ev4ubmxrNnz3B1dUVLS0uOHmcfU1NTRCIRZ8+eVbQruebt27csWLCATVMHIpFRj8eClHYrTWrVqsWrV69+2UTC0dGRmTNnMnXqVNatW/fDdW5ubty7d4/NmzcrPHKZE9aIH+Nw6C7JaRk5nnWUniEhOS0Dh0N3WSN+/NVrfn5+NGvWjEGDBtGiRQvu37/PihUrpN6qXRpERUVhZGRERkYG/v7+SvudJyA/BDHzG3Dq1ClatmxJsWLFCAoKwtDQ8IdrP336xMyZM2nevHlWYwAXF5esAWL9m2syxaJOnvzJLND81QyV/MLly5dZunQp8+bNo0mTJr9cn5ycLLM2uP+la9eueHt7c+bMGXr27KnwuSrKhkgkYtiwYXh7e/PmzRtFu5M1d+Z70ZkPHz7g7u5O7dq16d27N6qqqhw+fJiQkBDGjx9PqVLKnaqprq5Os2bN8mWq2Zs3b5g5cyba2tosXrwYm66mNK9eUupNUQpS2q20yUwD+1mqWSYLFy7E3t6eMWPGsG3btm9ef/LkCfPmzcPe3h59fX2p+yor9gZF4uoTIpW9XH1C8AqK5P79+3Tp0gVTU1PU1NS4ePEiBw4cQEdHRyp2pM2zZ88wNjYmPT0dsVgsCBkBQBAzBRqJRMLy5cvp0qULhoaGXL58OavF5fe4dOkSjRs3ZtmyZcydO5dr167RrFmzb9aNNdZhSa8GFCmkkqsZKqpk8P7sOlr98SHH70nZiI+PZ+DAgRgYGDBt2rRsXSOvyEwmlpaWHD9+HH9/f7p160ZiomwH/uU3bG1tAdi1a5eCPfnMxIkTv4rOhIWFYW9vT7Vq1Zg2bRpt27bl2rVrBAQE0KNHD1RVf55iokxYWFhw9uzZfNM6/OXLl0yZMgUtLS1WrlzJyJEjCQ8PZ+XKlbj1b04hKYuZgpJ2KwtyImZEIhHLly9n5MiRDBs2jL1792a9JpFI+Pvvv6lcubLCG3/khKi4ROYeDZbqnjMO3qJxWzPu37+Pl5cXV65coU2bNlK1IU2ePXuGkZERaWlp+Pv7o62trWiXBJQEQcwUUJKTkxkyZAhTpkxh2rRpHD58+Icntx8/fmTcuHG0a9eOsmXLcuvWLWbPnv3T6EH/5pr42hvmaoaKz4R21OIVvXr1IjY2NpfvUDmYPHkyr169YseOHdlOVUhJSZGrmIHPA+ZOnjzJ5cuX6dy5M/Hx8XK1r8yUL1+eHj16sGXLlhy3dZUFZcqUYcKECaxdu5bOnTtTu3ZtduzYwbhx44iIiGDnzp356jT5SywsLIiNjeXmzZuKduWnREVFMW7cOLS1tdm8eTOTJk0iPDwcFxcXKlWqBED1csWFtFs5UqlSJYoXL/7LuplMRCIR69atw8bGBltb26wBuTt27ODcuXNs2LCBEiXyT8c4x8N3ScthWtmvSJeAwdgVPHjwgL59+yp1N7fnz59jbGxMamqqIGQEviH/JIoKZJtXrz4LhevXr7Nz586sk+fvcebMGUaMGMGbN29wd3dn7Nix2T7pzZyh8vjVR3YHRiIOiSEyNvHb1pDqxTGuUxHblppZ6RMHDx5EX18fa2trTp48ma9OlzM5ceIEmzZtYsOGDTma2yHvyEwmRkZGnD59GktLSywtLTl58qTSpybJi6FDh9KxY0eCgoJo0aKFwvxITU3lwIEDHD16lJSUFK5cucLatWsZNGhQVqpnfqZly5aULFmSs2fPKqUgCw0NZcmSJWzfvp1SpUoxa9Ysxo4dS5kyZb67vn9zTd7EJ0sl9aegpN3KCpFIRM2aNbMVmclERUWFrVu38unTJ/r168eOHTuYNGkSNjY2P52rpmw8fvWRC0+knwYrUlElKrUEUe9TqF1R/vek7PL8+XOMjIxISUkRhIzAdxFJlOEoUkBq3Lp1i27dupGamoq3tzcGBgbfXRcXF8ekSZPw8PDAzMyMTZs2/TQFLbskJKcRHptASlrGL2eonD17lo4dOzJjxgwWLVqUZ9vy5M2bN+jp6aGvr8/x48dzdKLVs2dPUlJSOHHihAw9/DH//vsvHTp04M8//+T06dNfzcP5XUlPT6dGjRpYWlqyceNGIGef5bzy9u1bNm3axJo1a3j27BlmZmZUrFiRw4cPExoamuu5M8pIt27diI+Px8/PT9GuZPHo0SMWL17Mrl27UFdXZ/LkyYwaNSrbYn9vUCRzjwaTliHJUVG2iggKq6qwoJuuIGSyQY8ePUhOTubUqVM5ui4lJQUrKytOnTpFiRIlePLkiVJMrc8u844GszMwIscF/9lBVUXEQAMt5kk5yigtMiMynz59wt/fP9+10BaQD0KaWQHi0KFDtGnThgoVKhAUFPRDIXPgwAHq16+Pt7c3W7ZswcfHRypCBj7PYNDV+CNbM1TMzc1xcnLCycmJI0eOSMW+PMjMuU5LS+Off/7JcWheUZGZTFq2bImvry8PHz7EzMyMt2/fKswXZUFVVZW//voLr9PnmXnoFobLxOjNO0Pn1Rfpuf4ynVdfRG/eGQyXiZl3NJjHrz5KxW5ISAhjxoyhWrVqzJkzBwsLC+7cucPZs2dZu3YtampqLFu2TCq2lAULCwsuXrz4y65U8uDevXsMGDCA+vXrc/bsWZYvX05YWBjTpk3LUdQyt2m3olchnB7fRhAy2SSnkZlM1NTUGDZsGOnp6SQnJ2c7VU1ZED+KkYmQgc8dzsQhMTLZO69ER0cLQkYgWwhipgAgkUhYuHAhVlZWdOnShQsXLlCtWrVv1r148QIrKyv69OlD69atuX//PkOGDFFonuz06dPp2bMngwYNIiREOl1aZM2uXbs4ePAgGzdupEqVKjm+XtFiBqB58+b4+fkRGhqKiYmJUnTyUiRRcYkEVzCmjLUrnkHPiIhLlNoAuv8ikUjw8/Oja9eu1KtXj/379zN16lQiIyPZsmULDRp8LgDPrJ3JydyZ/IC5uTmpqakEBAQozIcbN27Qq1cvGjRowJUrV1i3bh2hoaFMmDAh1+l8mWm3Zye2Z6CBFlrqxb9p3iwCtNSLM9BAi1UdyhO2bTLH93rk+f38LtSqVYuwsLAcD7lNSEhgwoQJGBsb07x5czp27MiNGzdk5KV0iU9OIzIX3zM5ITI2kYTkNJnayCmZQiYpKQmxWCwIGYGfIqSZ5XMSExMZMmQIXl5eLFiwgFmzZn0jTiQSCdu3b2fSpEmoqamxdu1arKyslKbY78OHD7Ro0QJVVVUCAwMpWbKkol36IZGRkTRo0IDu3buzY8eOXO3Rtm1batWqhYeH4h9i7ty5g5mZGZUrV8bX15eKFX+/YX25TRFSVRFRSEXE/G669M/GyXpycjJ79uxhxYoV3L59Gz09Pezt7bG2tv7hbKK3b9+ira3NsGHDWL58ebZ9U2YkEgna2tr07NmTFStWyNX2v//+y8KFCzl58iS1a9fG0dERW1tbChcuLBN7v0pVHD58OAcOHODx48eUL19eJj4UJE6dOkWnTp2IiIjI0ZDEyZMns379eu7evUuFChUwMzMjNDSUgIAAdHWVM70qk+Do93RefVHmdk6Ma4uuhnKkHL948QIjIyMSExPx9/enVq1ainZJQMkRIjP5mGfPntG+fXuOHTvGgQMHmD179jcCJSwsDAsLC4YMGUK3bt24f/8+vXv3VhohA1C6dGkOHTpEZGQkQ4cOVYqOUt8jIyODwYMH88cff7B69epc76MMkZlMGjZsiL+/PzExMRgZGfHixQtFuyRXZDWA7ktev37NwoUL0dLS4q+//qJq1aqcPXuWO3fuMGTIkJ8OWS1btuwP587kV0QiERYWFnKdNxMQEICZmRmtWrUiPDyc3bt38+DBA/766y+ZCRn4ddqtk5MTGRkZzJkzR2Y+FCQyH2pzkmp27do1VqxYwbx586hVqxalS5fm9OnTVKtWDVNTUx4//vHfrjKQkiafNubysvMrXrx4gbGxMQkJCYjFYkHICGQLQczkUwIDA2nevDkxMTFcunQJKyurr15PT09n1apV6OnpERISwqlTp/Dw8FDKab4Af/75J9u2bWPfvn24u7sr2p3vsnLlSsRiMdu3b89T0bwyiRn4/LMPCAjg/fv3GBkZ8fz5c0W7JBdkMYDuS4KDgxk+fDjVq1dn8eLF9OzZkwcPHnDixAnMzMyyfaAwceJEChcuXKBqZywsLHjw4AFRUVEysyGRSPDx8aF9+/YYGRnx5s0b9u/fz927d7G2tlaKqe8VK1Zk7ty5bNy4kdu3byvaHaVHS0sLkUiU7ZqX1NRUhg8fTsOGDZk0aVLW/1+uXDl8fHwoW7YspqamhIeHy8jj3CORSHj27BmXLsgnHVOtkOIfB1++fImJiQnx8fH4+/vnqEuowO+N4j+9Ajlm9+7dGBoaUqNGDYKCgmjcuPFXr9+/f5927doxYcIEhgwZwr179+jYsaNinM0BvXv3ZurUqUybNg1/f39Fu/MVwcHBzJgxg4kTJ2JiYpKnvRQxZ+ZX1K1bl4CAAJKSkjA0NCQyMvLXF+VjZDGAbs7RYCJjEzh9+jQdOnRAT0+PkydPMnfuXKKioli/fj316tXL8b5fRmdevXolVZ8VhampKSKRiLNnz0p9b4lEwrFjx2jZsiUdOnTg06dPHD16lJs3b9K7d29UVJTrtjd27Fh0dHSYOHGi0kallYUiRYpQvXr1bEdmVqxYwZ07d9i8efM34rVixYqcO3cONTU1TE1NFXqIkylcjhw5wpw5c+jUqROVK1emevXqjP9rgMw/FyJAW12xM3devnyJsbExHz58QCwWC0JGIEco17e6wE/JyMhgxowZ2NraMmDAAMRicdYAN/h8CrVo0SKaNGlCXFwcFy5cYPXq1flqloizszPt27enb9++PHv2TNHuAJ/Fx8CBA6lZsybOzs553k/ZIjOZ1K5dm4CAANLT0zE0NFTK00ppIYsBdClp6RhN24ilpSVv3rxh165dhIWFMWPGjDxHRAtadKZcuXI0b95cqmImIyODAwcO0KRJE7p164aamhpnzpwhMDCQrl27KlVq7Zeoqanh7u6Ov78/Bw8eVLQ7Sk/NmjWzFZkJDQ1l7ty5TJgwgWbNmn13jYaGBufOnSM1NRUzMzNiYmTf1UsikfD8+XOOHj3KnDlz6Ny5M1WqVKF69er06NEjqzX8iBEj8Pb2JjL0MVoyFhqa6sVl1nY+O7x69QoTExM+fPiAv78/Ojo6CvNFIH+i+Di7QLb4+PEjtra2HDt2DFdXVyZNmvTVzfn69esMGTKE4OBgpk2bxpw5c36ai6+sFCpUCC8vL/T19enduzcBAQEKf/BfsGABd+/eJTAwkGLFiuV5P2UVMwA1atQgICAAExMT2rdvXyBzlmU1gE6CCEnFunie8KO/pZFUH54zozPLli1j6tSpXx1i5FcsLCxYv349GRkZeYqWpKWlsW/fPpycnLh//z6mpqb4+/tjaGgoRW9li6WlJZ07d2bKlCl07txZKt8zBZWaNWty+/5DgqPf/7CxgkQiYeTIkVSsWJEFCxb8dD8tLS38/Pxo164d5ubmiMViypUrJzV/o6OjuXbtGtevX+f69etcu3YtK8JaoUIFmjVrxvDhw9HX10dfX59q1ap9891hUve9TOfMGNdRXOOXV69eYWxszLt37wQhI5BrhG5m+YCwsDC6detGZGQke/bsoVOnTlmvJSUlMW/ePFxdXWnYsCFbt26lSZMmCvRWOgQFBdG2bVuGDBnC+vXrFebHlStXaNu2LfPnz2fWrFlS2VNdXZ1p06Yxffp0qewnC54/f56Vu+zn50fdunUV7ZLUyK8D6DI7mw0fPhxXV1ep7y9vzp8/j6GhIdeuXUNfXz/H16emprJr1y6cnZ158uQJnTp1YtasWbRq1UoG3sqekJAQ9PT0mD17NrNnz1a0O0rH41cf2R0YyaF/H/E+Q+2rB34RoFmuOMZ1K2JjoMm/Pt4MGjSIkydPYmlpma3979+/n5W+7evrS+nSpXPsY3R0dJZgyRQvmY07KlSokCVYmjVr9kPh8qP3br7ifI79yS6+9u2pXVH+GRyZEZm3b9/i7+9PnTp15O6DQMFAEDNKzvnz5+nVqxdlypTh6NGj/Pnnn1mvBQQEMGzYMKKiopg7dy5TpkyRaWceebN582ZGjBjB1q1b+euvv+RuPyEhgcaNG1O+fHkuXLggtYLhkiVLsnDhQuzt7aWyn6x4+fIlpqamxMXFce7cua8+e/kZw2ViImQ4t0FLvTgBU4xlsvecOXNwdXUlLCws30dnUlJSUFdXx9HRkRkzZmT7uuTkZLZt28aSJUuIiIigZ8+ezJo1i6ZNm8rQW/kwdepU1q5dy6NHj6hevbqi3VEKouIScTx8lwtP3qCqIvrpIUTm62nP7tJC9JRDOzbnyNbNmzcxMTFBT0+P06dPU6LEj9O7MoXLlxGXTOFSvnz5r0SLvr4+1atXz1O0duCWQC6Hxkr1EEZVRUTrmursHPr9AduyJCYmBmNjY96+fYtYLC5QB2YC8kcQM0rM5s2bGT16NO3bt2ffvn1ZefcfPnxg+vTpbNiwgTZt2vDPP//kqrA4PzBs2DB27drFpUuXcnV6mxdGjRrFjh07uHXrllRD34ULF2blypWMHj1aanvKipiYGMzMzHj58iXnzp3LGuiYX4lPTqPBvDPfDMSUJiLg3rwOMslBj4uLo0aNGgUmOtOtWzc+fvyIWCz+5drExEQ2b96Mi4sLL168oF+/fjg6Oub7z+SXfPjwgTp16mBiYoKnp6ei3VE4uZ0BJclIp2jhQszvrpetGVBf8u+//2Jubo6BgQHHjx+naNGivHjx4puIS2Yb+0zh8mXUJa/C5XtExSVi5h5AshRbKBcppIKvvSHVy+VuUGxuiYmJwcTEhNjYWPz9/QUhI5BnhAYASkhaWhoTJkxgxIgRjBgxgtOnT2cJmRMnTqCrq8uuXbtYs2YN58+fL7BCBmDNmjU0aNAAKysrYmNj5Wb31KlTbNiwAVdXV6kKmYyMDNLS0pS2Zua/VKxYEbFYTLVq1TA2NubmzZuKdilPRMQmyFTIAEiA8NgEmexdrlw5JkyYwLp16wpEZzMLCwsuXbpEfHz8D9d8/PgRFxcXatSoweTJkzE3N+fBgwfs2bOnQAkZ+Dxza/HixezZs4eLF2U/KFGZycsMKJGKKsnpkl/OgPoe2traODo6EhAQgJaWFhoaGmhoaNC1a1fWrFlDSkoKf/31FwcPHiQ8PJyYmBhOnz6Nk5MTvXr1QlNTUybNJqqXK858KaevLuimK3ch8/r1a0xNTYmNjRUiMgJSQ4jMKBlv376lX79++Pn5sXr1akaNGgXAmzdvmDhxIrt376ZDhw5s3LgRLS0tBXsrHyIjI9HX16dJkyacOnUKVVVVmdqLjY1FT0+Pxo0bc/LkSanemD59+kSxYsXYuXMntra2UttX1rx9+xYLCwuePHmCj48PzZs3V7RLueJm5Ft6rr8sczuHR7WmiWZZmeydGZ0ZMWJEvu9uFhISQt26dTl+/DidO3f+6rV3796xZs0a3N3d+fjxI4MHD8bBwYGaNWsqyFv5kJGRgYGBAenp6QQFBcn8+04Z2RsUicOhu1Lbb2mvBvT7ToTm5cuXX6WJXb9+nejoaABKlSpFfHw8derUYcGCBRgYGMhMqOSENeLHUpmPNdWiLmOM5dv++PXr15iYmPD69Wv8/f0L9EGsgHwRIjNKxKNHj2jZsiXXrl3Dx8eHUaNGIZFI2Lt3L/Xr1+fkyZN4eHhw6tSp30bIAGhqarJ3717OnTsn80nZEomEUaNGkZKSwpYtW6R+40pOTgbIN5GZTMqWLYuvry/16tXDzMyMK1euKNqlXCGvwXCytJMZnVm7dq1cWsnKEh0dHbS0tPDx8cn6/2JjY5k9ezZaWlosWrQIa2trnj59yqZNmwq8kAFQUVFh1apV3Lx5k23btinaHbkjqxlQN0MiOXnyJAsWLKB79+5Uq1aNKlWq0KVLF1auXMmnT58YNGgQBw4cICwsjPfv3+Pt7c3Tp085duyYTFLHcsNYYx2W9GpAkUIqqKrkzB9VFRFFCqmwtFcDhQgZU1NTXr9+jVgsFoSMgFQRIjNKgo+PD3379kVDQ4OjR49Su3Ztnj9/zujRozl69Ch9+vRh9erV+b7oNy8sXboUBwcHDh8+TI8ePWRiY/fu3dja2uLl5UXfvn2lvn9MTAyVKlXiyJEjdOvWTer7y5oPHz7QuXNnbt26xalTp2jbtq2iXcoRCclp6OXjmplMClJ0ZsSIEVy8eBGxWMzy5ctZt25d1qHC5MmTqVKliqJdVAgDBw7kzJkzhISEUKZMGUW7IzdkUeguyUjnU8RtYrzmULZs2a8K8/X19dHW1v6hUPHy8sLa2pphw4axYcMGpRA0kLvGCO1ql8e5ZwO5p5a9efMGExMTXr16hVgsLjDNZASUB0HMKBiJRMKqVauYNGkSHTt2xNPTk9KlS/PPP/8wZcoUihcvzrp16+jZs6eiXVU4EomEPn364OPjQ1BQkNRzbaOiomjQoAGdO3dm9+7dUt37SxuamppZU+LzI/Hx8XTt2pWrV69y4sQJjIyMFO1StkhOTub06dNMu5BIcuGct13NLrLsZvYls2fPZvny5YSHh1OxouLmROSVjRs38vfff1OkSBHU1NQYO3Ys9vb2VKhQQdGuKZTnz59Tt25dRowYgZubm6LdkQuybkG8vU8tDJvUzbEg8fDwYPDgwUyYMAF3d3elETTwv5bV4pAYImMTvzqoEfF5IKZxnYrYttRUSPvlN2/eYGpqysuXLwUhIyAzhDQzBZKSksKIESOYOHEikyZN4ujRo1l/+CNGjKB3797cv39fEDL/j0gkYtu2bVStWpVevXr9tGg4p2RkZPDXX39RsmRJ1qxZI7V9/0tmmpmamprMbMiakiVLcuLECVq3bk2nTp3w9fVVtEs/JD09HbFYzPDhw6lcufLniF50MCIZxWbkOYDO3t6eQoUK5dvITHh4OH///Tfjxo0DoEOHDkRERODs7PzbCxmAqlWr4ujoyOrVq3n48KGi3ZELuwMjc5w6lV1UVUT4P0vPlRCxs7Nj3bp1rFy5UmrzxqSFTqVSzOumS8AUY+7N68AfV9bR6uNFToxry715HQiYYsy8broKETKxsbGYmZnx4sUL/Pz8BCEjIDMEMaMgXr9+jZmZGTt27GD79u0sWbKEFStW0KBBA8LCwjh79ixbtmyhbFnZFBHnV0qVKsXhw4eJjIxkyJAhSCuwuGbNGs6dO8f27dtl+jPPrzUz/6V48eIcO3YMIyMjunTpwunTpxXtUhYSiYTr168zefJkNDU1MTEx4dy5c4wZM4bg4GCOu09HgmwemNIzJNi2zFkr2NySX2tnHj9+zJAhQ9DR0eHgwYMsWLAAfX19ihUrJnzf/YdJkyZRvXp17O3tpfZdp8yIH8XIZJgtfP7bFIfk/u9k1KhRuLq64uzsjJOTkxQ9kx4lihQiNSaMKmrJ6Gr8IdNU118RGxuLqakp0dHRiMVidHWlP0hYQCATQczkgoTkNIKj33Mz8i3B0e9JSE7L0fV3796lRYsWPHr0CLFYjL6+Pq1atWLq1KmMHDmSe/fuYWZmJiPv8z/16tVj+/bt7N+/n+XLl+d5vwcPHjB9+nTGjRsn8597QREzAEWLFuXw4cNYWFjQvXt3jh07plB/Hj9+zIIFC6hfvz7NmjVj165dWFlZceXKFZ4+fcqiRYv4888/0alUina1y0v9BFhVRUS72uXlegKan6IzwcHB2NjYUK9ePU6fPo2Liwvh4eE4ODhgaWnJ2bNnSU9PV7SbSkXRokVZvnw5p0+f5sSJE4p2R2a8fv0a7+OniIiTTUvzTCJjE3N8v/6SyZMns2DBAmbNmoW7u7sUPZMeSUlJFC1aVKE+ZEZkoqOj8fPzE4SMgMwRamaySVZe6qMYIuO+k5darjjGdStiY6CJTqUfP8wcOXIEW1tbateuzf79+9m1axfOzs7Url2bLVu20KpVK5m/l4KCg4MDy5Ytw9fXF2Pj3NUopKam0qpVK+Lj47lx4wbFi8u2MPLKlSu0bt2au3fvoqenJ1Nb8iIlJYUBAwZw9OhRvLy86NWrl9xsv3jxAi8vLzw9PQkKCqJUqVL06tULa2trTExMKFTo+yeTBWkA3ezZs3FzcyMsLEwpa2du3ryJk5MTBw8epHr16jg4ODBkyJCvHrguXLhA+/btCQoKolmzZgr0VvmQSCSYm5sTERFBcHBwvk5Rhc81FP9thxwZGUnhijXQGLJa5vZPjGuLrsYfub5eIpHg6OjIkiVL2LBhAyNHjpSid3mncuXKjBkzhtmzZyvEflxcHGZmZkRFRSEWiwvMfU5AuREiM78gKi6RgVsCMV9xnp2BEUT8R8jA5yF5EXGJ7AyMwHzFeQZuCSQqLvHrNRIJixcvpmfPnnTo0AF3d3d69OiBk5MTDg4O3Lx5UxAyOWTRokUYGxvTr18/oqKicr3HrVu32Llzp8yFDHx+8IeCEZnJRE1Njb1792JlZUXfvn3x8vKSqb13796xdetWzMzMqFatGtOnT6dq1ars37+fV69esX37diwsLH4oZKDgDKCDz9EZVVVVXF1d5W77Z1y9epWuXbvStGlTbt68yebNm3ny5AmjR4/+5uS4ZcuWlCxZ8qsWzQKfEYlErFixgrCwMFauXKlod3JEbGwsPj4+ODs7Y2Vlhba2NhUqVKBjx464uLjw7t07+vXrx969ezl4+IhcfErJ4wGGSCTC2dmZ8ePHM2rUKHbs2CElz6RDUlISxYoVU4jtL4WMn5+fIGQE5IbiEirzAXuDIpl7NJi0/8/h/VUub+brl0NjMXMPYH43Xfo31yQpKYlhw4bh6emJg4MDnz59wsTEhKZNm3L9+nUaNmwo8/dSEClUqBB79uxBX1+f3r17c/78+RyJhKtXr+Lk5MTs2bPlNgSyIKWZfUnhwoXZtWsXhQsXxtramtTUVKkOBf306RMnTpzA09OTEydOkJKSgrGxMZs2baJXr165qrXo31yTN/HJeRxAJwFE6KaH0rdZpzzsk3sya2fc3NyYMmWKwqMzFy5cYOHChZw9e5a6deuyY8cOBgwY8FNxWbhwYUxMTPDx8cHR0VGO3uYP9PT0GDVqFAsXLmTgwIFUrlxZ0S59Q2xsbFbEJTPqEhERAUDp0qXR19enT58+6Ovr06xZM2rWrImKyv/OU4Oj3wORMvdTGjOgRCIR7u7uJCYm8tdff1GsWDH69OkjBe/yTuZgZnkTFxeHubk5kZGR+Pn50aBBA7n7IPD7IoiZH5CXKbvpGRLSMyQ4HLpLaPQbvJ3+5t69e8yePZvdu3cTHR3N0qVLs/LdBXJPhQoVOHjwIG3btmX8+PFs3LgxW9clJiYycOBAmjZtKteHp4IqZuCzuNy+fTuFCxdm0KBBpKWlMXjw4Fzvl5aWhlgsxtPTk0OHDvHhwweaNWvG4sWL6devHxoaGnn2eayxDuVLFsk6tMhJ8bGqiohCKioYlnjF5hnjWVTqncJSO+zt7Vm5ciWurq64uLjI3b5EIuHcuXMsXLiQ8+fP06BBA7y8vLCyssr2BHsLCwvs7e2Jj4+nZMmSMvY4/zF//nw8PT1xdHRk69atCvUlLi7uqzSx69evEx4eDnwWLk2bNqV3795Z81xq1ar1lXD5HtrqJRCBzGdAaauXkMpeKioqbNiwgaSkJKytrSlatChdu3aVyt65JT09nZSUFLmLmbdv32alQvr5+QkHtAJyR3iS/g57gyLzeFr7Pzb9+4K0otp06FCVhQsXYmhoyOnTp9HR0ZHK/gLQvHlz1q1bx7BhwzAwMGDIkCG/vGbatGlERUVx9OhRChcuLAcvP1MQWjP/DFVVVf755x8KFy7MX3/9ldV+PLtIJBKCgoLYvXs3Xl5evHr1Ch0dHezt7bG2tqZOnTpS97l/c03a1Cqf4wF0rWuqZw2g00p/zqxZsyhdujQTJkyQuo+/oly5cowfPx53d3e5RmckEgknT55k0aJF/Pvvv+jr6+Pt7U3Xrl1/+fD6XywsLEhNTSUgIIDOnTvLyOP8S7ly5Vi4cCFjxoxh1KhRcosmZwqXLyMumcKlVKlS6OvrY2VllTWAsnbt2jn+3cPnTlya5YoT8Z8UbWmiqV5cqh2+VFVV2b59O0lJSfTu3Zvjx49jbm4utf1zyqdPnwDkKma+FDLnzp0ThIyAQhDEzH+Iiktk7tFg6W0okaBqMACx5xQ2bNjA8OHDc/VFL/Bzhg4dSmBgIKNHj6Zhw4Y/LSL28fFh7dq1rF69WuqDN39FQY7MZJJ5YqmmpsbIkSNJTU1lzJgxP73mwYMH7NmzB09PT54+fUqVKlWwtrbG2toafX19mQ+pq16uODuHGuR6AJ2joyMfPnxg4sSJlCpVKluCWtrY29uzatUquURnMjIy8Pb2ZtGiRdy8eZPWrVtz6tQpOnTokOvfVe3atdHS0sLHx0cQMz9gxIgRbNiwgfHjx3P58mWp/128ffv2m+L8sLAw4LNwadq0Kb169cqKuORWuPyI9jrl2R0YQYYMWqfLagZUZrpzz5496d69O6dPn6Z9+/ZSt5MdkpKSAPmJmXfv3mFhYUFYWBh+fn40atRILnYFBP6L0M3sPwzcEsjl0Fjp9rqXZNBcszT7RxtKb0+Bb0hOTqZdu3a8evWK69evU758+W/WxMXF0aBBA3R1dTl9+rTcheXWrVsZOnQoaWlp2U6/ya9IJBImT56Mu7s7bm5u2Nvbf/X6s2fP2Lt3L56enty8eZM//viD3r17Y21tjaGhocJ/PgnJaYTHJpCSloFaIRW01Uv89FRXIpEwevRoNm3axN69exWSQ5/ZMlZWnc3S09PZv38/Tk5O3Lt3D2NjY2bNmoWxsbFUHqxHjBjBhQsXePDggRS8LZiIxWJMTEzYuXNnnurS3r59y40bN75KFQsNDQU+D8bNjLRk/qOjoyOz78ukpCS2bNnCso07EXWZIxMbAL727WXWOj0pKYkuXbpw9epVzp07R4sWLWRi52dERUWhqanJqVOn6Nixo0xtvXv3DnNzc0JDQzl37hyNGzeWqT0BgZ8hRGa+4PGrj1x48kb6G4tUCIqK50nMR4VM4f1dKFKkCAcPHqRp06YMGDCA06dPf/NAPHr0aJKSkti2bZtCImTJycmoqqoq/EFdHohEIpYvX07hwoWZNGkSqampDBs2jAMHDuDp6cn58+dRU1Oja9euzJ49m06dOilVxKpEkUI5auEqEolYu3YtHz9+xMbGhhIlStCpk3ybAmRGZ5YvX87SpUultm9qaiqenp44OzsTEhJCx44d2bBhA23atJGaDficarZ582YiIyPR1JTP8NH8hrGxMVZWVkyfPp0ePXpkq74oU7h8GXX5Urg0bdqU7t27Z0VcZClcvuT9+/esX78ed3d33rx5w4ABA4ivXIS7MSlSPVBUVRHRuqa6TO+/xYoV48iRI3To0IEOHTogFovl/oAvr8hMZkTm6dOngpARUAqEfKcv2B0YKfVBepmoqojY9a/sO7X87lSvXh0vLy/8/PyYNWvWV6/t2bMHLy8v1q1bR9WqVRXiX3JyslI9sMsakUjEnDlz6NmzJ9OnT6dixYqMGjWKIkWKsG3bNmJiYti/fz89e/YsED8XFRUVtm3bRqdOnbCysiIgIECu9tXV1Rk/fjxr1qzh9evXed4vOTmZTZs2UbduXQYPHkz9+vW5evUqp06dkrqQATAxMUFFRYWzZ89Kfe+ChKurK3FxcSxevPib1969e4efnx8uLi7069eP2rVrU65cOczMzFi4cCEvXryge/fu7Nq1iwcPHvD+/XsCAgJwc3PD2tqaunXrylzIxMTEMHPmTDQ1NZk7dy69evXi8ePH7Nq1i9UDW1NIyvfhQioinHvKvrtWyZIlOXnyJLVq1cLc3FzuEUZ51My8f/+eDh068OTJE86dO0eTJk1kZktAILsIaWZfYLhMLNPiQy314gRMyd1wR4GcsWzZMqZNm8ahQ4fo2bMnz549o0GDBnTs2JE9e/YozC8XFxeWLFlCXFycwnyQB6mpqfj6+uLp6cnhw4dJSEigWrVqPHv2jEmTJuHq6irzOhhF8unTp69STuRVrA2fW+TWqFGDUaNG5To6k5SUxD///IOLiwvPnz+nd+/ezJw5Uy458S1btkRLS0vm84ryO3PmzGHp0qVs2bKFFy9eZEVcnj59Cnx+sG7SpElWmlizZs3Q0dFRaFQ4MjISV1dX/vnnH1RVVRk1ahT29vZUqVLlq3V7gyJxOHRXanaX9mpAv+byi/TFxsZiZGREbGws58+fp3bt2nKxGxgYSMuWLblz545MWiO/f/8eCwsLHj9+jK+vL02bNpW6DQGB3CCkmf0/8clpRMpQyABExiaSkJwm1W4qAt9nypQpXL16FTs7O+rWrcvEiRMpXrw4a9euVahfBTkyk5GRwZUrV/D09GTfvn28efOGevXq4eDgwIABA6hVqxZLlixhxowZFClSBCcnpwIraIoWLYq3tzcWFhZ07NiRgIAAuQ2QU1dXZ9y4caxYsYIpU6ZQoUKFbF8bHx/Phg0bcHV15fXr11hbW+Po6Ej9+vVl6PHXWFhYsHbtWtLT03+LdMzs8v79+6xUsWvXrnHt2jVSUlIYOHAgJUqUoEmTJnTt2jVLvNSpU0dpfn4PHz5k6dKl7Nq1i9KlS+Pg4MDYsWMpV67cd9dLZwbUZ6Za1JWrkIHPf4O+vr60b98eU1NTLly4IJe0SVmmmWVGZEJCQjh37pwgZASUCuGp+v+JiE2QaX97+Nw/Pzw2IUd5+AK5QyQSsXXrVgwMDDAxMeHVq1ecOXPmhzdPeZGcnFzg2jLfu3cPT09PPD09iYiIoFq1avz1119YW1vTqFGjrwSLg4MDampqTJ48mZSUFJYtW1ZgBU3JkiU5ceIExsbGmJubc+HCBbmd0E6aNCmrs1l2ojPv379n7dq1uLm58f79e+zs7HBwcJCbv19iYWHBwoULuXnz5k+7EhZkvhQumeLlyZMnABQvXpymTZvSpUsXUlJSWL9+PYcOHcLCwkLBXn/L9evXWbx4MYcOHUJDQwMXFxeGDx+erTqfvM+AErGgm67chUwmlSpV4ty5c1mC5vz5899EoKSNrMTMhw8f6NixI48ePRIiMgJKiSBm/p+UtIwCZUfgcyvR5cuX06lTp6wcZkVTUCIzERERWa2U7969S7ly5ejTpw/W1ta0bdv2pzn3kyZNQk1NjXHjxpGSksLKlSsLrKApW7YsPj4+tG/fHjMzMy5evEi1atVkbjezduZX0Zm4uDhWrlzJqlWrSExMZNiwYUybNg0tLS2Z+/gjDAwMKFWqFD4+PtRv0DhHHeXyIx8+fPgq4nL9+nUeP34MfBYuTZo0oVOnTlnF+XXr1s2KuEgkEu7cucOkSZO4deuWUgxhlkgknD9/HmdnZ3x8fKhduzabNm1i4MCBOf7uk8YMKEVSrVo1zp07R7t27TAzM8Pf3z9HkdKckilmihYtKrU9P3z4QIcOHXj48CFnz55FX19fansLCEgLoWbm/wmOfk/n1RdlbufEuLZCZEZOpKam0qZNG549e8aLFy9wcXFh6tSpCvVp/PjxiMVi7t6VXj64vHjz5g379u3D09OTS5cuUaxYMXr06IG1tTUWFhY5jjht3LiRv//+m7///pu1a9cW6PlLUVFRtG3blmLFinH+/Hm5DLWMjY1FW1ub0aNHfxOdiYmJwc3NLSud6++//2bKlCloaGjI3K9f8fjVRwbMXse7ElVJL1r221k/5YpjXLciNgaa6FTKX90hP3z4wM2bN79qhxwS8jmVqnjx4jRu3DhLtOjr61OvXr1fporduHGDZs2asWrVKsaOHSuPt/FdJBIJJ06cwNnZmStXrtCwYUMcHR3p3bu3VNLdMmdAHQl6Qlyq6lcHID+bAaUMhISE0L59e6pUqYKfnx9ly5aViZ3du3dja2tLQkICxYvnXchlRmTu37+Pr6/vbxspFVB+FH+MoyRoq5dABDJPNateRnonJgI/x9nZmRs3bnD58mUOHz6Mg4MD+vr6mJiYKMyn/BaZiY+P58iRI3h6euLj44NEIqFDhw7s2rWL7t27Zytd5EeMHDmSwoULM2zYMFJSUti0aZPS5PhLm+rVq2ed0Ga2bS1TpoxMbWZGZ1auXJkVnYmOjmbZsmVs3LgRVVVVxowZw6RJk+Qirn5FVFxi1gm8SF0XyXcGJ0qAiLhEdgZGsP1KOO1ql1eKE/jvkSlcvkwV+69w6dChAzNnzsy2cPkeTZs2ZejQocyZM4cBAwagrq4u7bfyU9LS0ti/fz+LFy/m7t27tGnThhMnTmBpaSnViKtOpVLM66ZL2tU97NjnxcmL1/JNxK5OnTr4+vpiaGiIpaUlZ8+epVQp6QsuaUZmPn78iKWlJffv3+fs2bOCkBFQaoTIzBfIuptZalw0CV5Tad++PSYmJpiYmKCnp1egT6QVRVBQEK1atWLmzJnMnz+f9PR0OnbsyK1bt7h+/brCZlgMHjyYx48fc+nSJYXYzw4pKSmcOXMGT09Pjhw5QlJSEm3btsXa2prevXtLPU1i165d2NnZYWNjw7Zt2wqsoAG4e/cuhoaG1K9fHx8fH0qUKCFTe5nRGVtbW0QiEVu2bKFYsWJMmDCBCRMmKLyGLJO9QZF5qo2Y302X/gqqjYDPD37fi7hIJBKKFSv23YiLNFPCYmJi0NHRwcbGhnXr1klt35+RnJyMh4cHLi4uPH36lI4dO+Lo6Ei7du1katfGxoaoqCjOnz8vUzuy4Pr165iYmNC4cWNOnTollejJl6xZs4YpU6ZktWjOLR8/fqRjx47cu3ePs2fPKmQAqIBAThDEzBfMOxrMzsAIqQ7rykRVRYSZlhrVY/5FLBZz6dIlkpOTUVdXx9jYGGNjY0xMTKhbt26BrR+QF4mJiTRt2pRSpUpx+fJlChcuDHxOk9LX16dSpUqcP39eqnnF2cXa2pqXL1/i5+cnd9s/IyMjgwsXLuDp6cn+/ft5+/YtDRo0wNramv79+6OtrS1T+3v37sXW1pY+ffqwc+dOpcj9lxVXr17F1NSUli1bcuzYMZl+Dp88eYKVlRV37tyhbNmyTJkyhTFjxvDHH8qT6rpG/FgqXaumWNRhrLGOFDz6OZnC5b8Rly+FS2YrZFkIlx/h5ubG1KlTuXnzJg0bNpSZnfj4eDZu3IibmxsvXrygd+/eODg4yK0ovE2bNtSuXRsPDw+52JM2ly9fxsLCgjZt2nD06FGpRuqXLVuGs7Mzb9++zfUemRGZu3fvCkJGIN8giJkvePzqI+YrZHfas39wQ5rXrQ58nkNx5coVxGIxfn5+BAYGkpaWRpUqVbKEjbGxMTVq1BDETQ4ZP348mzdv5ubNm9SrV++r165fv06bNm0YNGgQmzZtkrtvVlZWJCYmcurUKbnb/i8SiYTbt2/j6enJnj17ePbsGVpaWlhbWzNgwACZzCn4GQcPHqR///50796dPXv2ZInQgoi/vz+WlpZ07NiR/fv3S/1h98GDBzg7O+Pp6Ym6ujofPnxg9OjRuLm5SdVOXlH2eSLx8fFZwiUz6vLo0SMkEglFixb9JuJSv359hQnxlJQUGjZsmFWXIe37RmxsLKtXr2b16tV8+PCBQYMGMW3aNOrWrStVO79CQ0OD4cOHM3/+fLnalSZisZhOnTphYWHBgQMHpPZdt2DBAjZs2EB0dHSurv/48SOdOnXizp07+Pj4YGBgIBW/BARkjSBm/sPALYFcDo2VanRGhITkyDvEH1vClClTsLe3/yZfNj4+nkuXLuHn54dYLOb69etkZGSgpaX1lbiRRyek/MzZs2exsLBg5cqVjB8//rtrtm7dytChQ/nnn38YOnSoXP3r0qULqqqqHDlyRK52v+Tp06dZncgePHhA+fLl6devH9bW1rRq1Uqh4vno0aP07t2bTp064eXlla/qi3LKiRMn6NGjB/3798fDw0Mq6aa3b9/GycmJAwcOULVqVaZPn87QoUNZuHAhq1atIiwsTKbdlHJCVFwiZu4BJEuxw2ORQir42hvmqoYmPj6eW7dufZUq9vDhw6+Ey5cDKBUpXH7EqVOn6NSpE/v376d3795S2TM6Oho3Nzc2bNhARkYGw4cPZ/LkyQpJ1f306RPFihVj27ZtDB48WO72pcmpU6fo3r07PXv2xNPTUyrptTNmzMDLy4vQ0NAcXxsfH4+lpSW3b9/Gx8eHli1b5tkfAQF5IYiZ/yCrG6yXXQN2rndn7dq1lCpVCkdHR0aNGvXDFJN3795x4cKFLHFz+/ZtAHR0dLKEjbGxsVIU7ioLmalR9erVw8fH56cPhyNHjsTDw4MLFy7IdTq7ubk5ZcuWZd++fXKzCfDq1Su8vLzw9PQkMDCQEiVK0LNnT2xsbDA1NVWqKMjJkyfp1asXpqamHDx4UCHpgPLCy8uLAQMGZHV0y62QDAoKYtGiRRw9ehRtbW1mzJiBnZ1dlhh88+YNNWrUYMyYMSxZskSabyHXyOLgSFVFROua6uwc+vMT5Uzh8mWq2JfCpVGjRl9FXP7880+lEy4/okuXLty7d48HDx7kad7I06dPcXFxYfv27RQrVoyxY8cyYcIEhYrhR48eUa9ePfz9/TE0NFSYH9Li8OHD9OnTB1tbW7Zu3ZrnA42JEydy9uxZgoODc3RdfHw8nTp14tatW4KQEciXCGLmO8gy9SEqKoqFCxeydetWqlSpwty5cxk8ePAvb5Rv3rwhICAAPz8//Pz8ePjwIQC6urpZzQQMDQ1l1vIxP2BjY8OJEye4e/cu1atX/+na5ORk2rdvz4sXL7h+/brcbtDt27dHS0uLnTt3ytzWhw8fOHz4MJ6envj6+qKqqoqlpSXW1tZ07dpV6sWn0sTHx4fu3bvTvn17vL29ZTLRWlnYsmULw4YNY/r06TkWGpcuXWLhwoWcOXOGOnXq4OjoiLW19XfFqaOjI6tWrSI8PJzy5ctLy/1cIeuUXl/79lnteRMSErKES2bU5eHDh2RkZFCkSJHvRlyUSdznlJCQEPT09Jg9ezazZ8/O8fV37txhyZIleHl5Ub58eSZNmsSoUaMoXbq0DLzNGadPn8bS0pLw8HCFzkKSJp6entja2ub5QAM+H9Jlfs6zS3x8PJ07d+bmzZucOXOGVq1a5dq+gICiEMTMD5BWUepUi7qMMf52inZISAhz585l79696OjosHDhQvr06ZPtk5kXL15k1duIxWJCQ0MRiUQ0adIkKy2tXbt2Mmn/qIzs27ePfv36sWvXLmxsbLJ1zbNnz2jatCkNGzbk9OnTcjl5NTAwoEGDBvzzzz8y2f/Tp0+cOnUKT09Pjh07RkpKCoaGhlhbW2NlZaU03auyg5+fH127dqVly5YcPXpU5p2/FMmKFSuwt7fH2dmZGTNm/HStRCJBLBazcOFC/P390dXVZdasWfTp0+enqSqZ0ZmxY8eyePFiab+FHCHLZisqImhY7B1/PPHJirhkCpdGjRp9VZz/559/5mvh8iOmTp3K2rVrefTo0S8PdjK5cuUKzs7OHD9+HC0tLaZNm8Zff/2lVAcJGzZsYNy4cSQlJeWbSFl2yEx9njRpEq6urrkWNIMGDSIsLIwLFy5ka31CQgKdOnXixo0bnDlzhtatW+fKroCAohHEzE/Ia7vQBd10f1mMeuvWLWbOnMnJkydp3LgxTk5OuerPHxERkSVu/Pz8eP78OaqqqjRv3jwrLa1169ZKfRqfW6Kjo9HT08PMzAwvL68c/ezEYjFmZmZMnTpVLuk3jRs3pk2bNqxdu1Zqe6anpxMQEMDu3bs5ePAg79+/p0mTJtjY2NCvX798XWd14cIFOnXqRJMmTThx4kSBFufz589n3rx5rF69+rvDDyUSCadPn2bRokVcvnyZJk2aMHv2bLp3757tQxBlic7Iug1+2tsXaNzY9FWqmK6uboEULt/jw4cP1KlTBxMTEzw9PX+4TiKRcPbsWZydnQkICKB+/frMmDGD/v37K+XPavr06ezfvz9XNSHKzpo1axg3bhyzZ89mwYIFudqjb9++vHv3Dh8fn1+uTUhIoHPnzly/fl0QMgL5HkHM/IIvB7mpqoh+KmoyX8/NILeLFy/i6OjIhQsXaNu2Lc7Ozrnu1y+RSHjy5MlX4ub169eoqanRqlWrrMiNgYFBjqe2KxsSiQRLS0vu3LnD3bt3czUwztXVlalTp3LgwAGsrKxk4OX/qF+/PpaWlnnuKiWRSLh+/Tqenp7s3buXFy9eUKtWraxOZPXr15eSx4rn8uXLWFpaoqury6lTp5SqrbA0kUgkTJkyBTc3N7Zv346dnR3wuW320aNHWbRoEdevX6dly5bMnj07V4ceyhCdiU9Oo8G8MzIdUCwC7s3roNSDFGXNtm3bGDJkSNY95UsyMjI4fPgwixcv5vr16zRv3hxHR0e6deum1HPP+vXrx5s3bzh37pyiXZEJLi4uTJ8+ncWLF+Pg4JDj67t27YqKisovG8wkJCTQpUsXrl27xunTp2nTpk1uXRYQUAoEMZNNHr/6yO7ASMQhMUTGJn51IxYBmurFMa5TEduWmlm52jlFIpFw5swZHB0duXnzJpaWljg5OdGkSZM8+S6RSLh//35WSpq/vz9v376lWLFitG3bNkvc6Ovr57vQ/fr16xk9ejQnT57E0tIyV3tIJBL69evHqVOnuHr1qkyFQK1atejbt2+uHyRDQkLw9PTE09OTx48fU6lSJfr374+1tTXNmzcvsG28r169SocOHahTpw6nT58usLVhEomEESNGsHXrVry8vMjIyGDRokVZwzZnz56NiYlJnn7Pio7OBEe/p/PqizK3c2JcW3Q1CqbwzQ4ZGRkYGBiQnp5OUFAQqqqqpKamsnv3bpYuXcrDhw8xMTFhxowZmJqa5ovvjhYtWtCgQQO2bNmiaFdkxrx585g/f/5PO3J+SUJyGuGxCaSkZTB+zGgqlVDhwN7dP1yfmJhIly5duHr1KqdPn/5G6AoI5EcEMZMLvvzyUCukgrZ6CameAGZkZHDw4EFmzZpFSEgIffv2ZeHChdSpU0cq+6enp3P79u2syM358+eJj4+nVKlStG/fPistrVGjRkp9Svf48WMaN27MoEGDWL9+fZ72io+Px8DAgIyMDK5evSqzdKZq1aoxdOjQHM1IiI6OZu/evXh6enL9+nVKlSqFlZUV1tbWGBsb5zsBmltu3LiBubk52tra+Pj45CoKlx9ITk7G0NCQwMBAACwsLJg1a5bUJqu/efMGbW1txo0bJ5fozKdPn3j27BlRUVFERkZyPTyWo59kP5vk8KjWNNEsmKI3u1y5coXWrVuzZs0aMjIycHV1JTIyku7duzNjxox8N0ekYsWKjB8/nlmzZinaFZkhkUiYNm0arq6ubN68mWHDhn2zJutw9VEMkXFfH64ikaClXgLjuhWxMdBEp9L/7mWCkBEoqAhiRolJS0vDw8OD+fPnEx0dzeDBg5k7d262CzqzS2pqKtevX88SNxcvXuTTp0+UK1cOQ0PDrG5p9evXV5rTu7S0NNq2bUtsbCy3bt2SSnH4o0ePaN68ORYWFuzfv18m77VChQrY29vj6Oj403Vv377l0KFDeHp6IhaLKVy4MF26dMHa2ppOnTopVVGuPLlz5w6mpqZoaGjg6+urNDNTpEFKSgo7duxg8eLFhIaGUrFiRd6/f4+vr6/UHzpmzJjB6tWr8xydSU9P5+XLl1lC5Xv/jomJ+eqainWaUqxX7moCcsLvHpkBeP/+PYaGhty5cwcVFRX69++Pg4MDenp6inYtx2QeuO3cuRNbW1tFuyNTJBIJY8eOZf369ezcuTOrqU1e0t7Vi35OQwsMDOTUqVNSOxwREFAGBDGTD/j06RMbNmzAyckpa5K3o6OjzB7kkpOTCQwMzEpLu3LlCqmpqVSqVClrvo2JiQm1atVSmLhZtGgRc+fO5dKlS1Ltie/t7U3Pnj1ZunQp06ZNk9q+mZQuXZq5c+cyefLkb15LSkri+PHjeHp6cvLkSdLS0jAxMcHa2pqePXtSpkwZqfuTHwkODsbU1BR1dXXOnTtH5cqVFe1Snvj06RNbtmxh6dKlREVFYWVlxaxZs6hbt25WpyGxWEzTpk2lZjM70RmJRMK7d+9+KFIiIyN5/vw5aWlpWdeUKFGC6tWro6mp+d1/V6tWDYmqGnpCzYxMiYmJYcWKFaxdu5bk5GTS09MZNGhQvk7PCg4ORk9Pj4sXL/4WNR4ZGRkMHTqUnTt3sm/fPlKqN8tTQ6LSj8/w4Pg/nDx5kvbt28vQcwEB+SOImXzEx48fWbFiBa6urmRkZGBvb8/kyZNlXhCdmJjIpUuXsiI3165dIz09nerVq2cJG2NjY7lNhM4sgp4+fTqLFi2S+v4zZ85kyZIl+Pj4YGpqKtW9ixQpwvLly7O6VaWlpXHu3Dk8PT05fPgwHz9+pHnz5lhbW9OvXz+qVKkiVfsFhUePHmFiYkKpUqXw8/NDQ0ND0S7lmISEBDZu3MiyZcuIiYmhf//+ODo6oqurm7Xm48ePmJmZ8fTpU86fP8+ff/4pNftTp05l/fr17Nixgw8fPnxXrCQkJGStL1SoEFWrVs0SJt8TK2XLls3WAYesu5lpqRcnYIqxzPZXViIiInB1deWff/6hUKFCjBo1Cnt7e7Zt28bcuXO5e/cu9erVU7SbueL48eN07dqV58+f58u/99yQnp6OjY0NPs9VKN02eyMHvotEAiIRvXXUcB1iLj0HBQSUBEHM5ENiY2NZunQpq1evpnjx4syYMYMxY8bILfXow4cPXLhwIStyc+vWLSQSCbVq1coSNsbGxjI5MU9KSqJp06YUK1aMf//9Vybd2NLT07NOxK9fvy41kSaRSFBRUWHjxo00bNgQT09PvLy8iImJoU6dOtjY2DBgwAB0dHSkYq+g8+TJE0xMTChSpAh+fn5ST7+UFR8+fGDt2rW4ubnx7t07Bg4cyIwZM374e4+Li8PIyIjY2FguXrxIjRo1fmkjPT2dFy9e/DSq8ubNm6+uqVix4k+jKpUrV/7pHJucIMs5M6oqIgYaaDGvm+6vFxcQHjx4wNKlS9m9ezd//PEHEyZMYOzYsVmNMj59+sSff/5J3bp1OXnypNKkC+eE1atXM3XqVBITE5W6llPa7P43jJlH7kttvy+HeAsIFBQEMZOPef78OYsWLeKff/6hYsWKzJkzhyFDhsh9PkBcXBwBAQFZ4iY4OBj43IY4U9wYGRlJpWB74sSJbNiwgRs3bkj1lPq/xMbGoq+vT4UKFbhw4QJFixbN8563bt2iSZMmVKhQgdevX6OhocGAAQOwtramSZMm+fIBQ9GEhYVhbGyMiooKfn5+aGtrK9qlH/L27VtWrVrFypUrSUhIYMiQIUyfPj1bPr98+ZJ27dqRnp7O+fPnKVas2E/rVJ4/f056enrW9SVLlvyuSDlz5gxHjx7l4cOHcp1H9PjVR8xXnJfZ/r727XPdVTI/ce3aNRYvXszhw4fR0NBgypQpDB8+/Ls1hIcPH6ZXr14cP36czp07K8DbvDF58mSOHz/Oo0ePFO2K3IiKS8TMPYDktAyp7VmkkAq+9oY5Gh0hIKDsCGKmAPDkyRPmzZuHp6cnNWvWZMGCBfTv319hp1cvX77E398/Ky3tyZMniEQiGjVqlJWW1q5duxynx507dw4zMzPc3Nywt7eXkff/48aNG7Rp0wZbW1s2b96cqz2ioqLYu3cvu3fv5vbt2wCYmJgwe/Zs2rVrJ7WT7t+ZyMhIjI2NSUtLw8/Pj1q1ainapa94/fo17u7urFmzhtTUVEaMGMHUqVN/KB4SExOJior6RqSEhITw77//kp6ezpdf24UKFcpK+/pRZOWPP/74rljOrJ0ZP348zs7OMvsZfI+BWwK5HBor1eiMqoqI1jXV2Tk0f3XpygkSiYSAgAAWL16Mj48PtWvXxsHBAVtbW4oUKfLT68zNzYmMjOTevXv5bsZYr169SEhI4MyZM4p2RW4IfyMCAtlDEDMFiDt37jB79myOHj1KgwYNcHJyokuXLgo/8Y+KikIsFmeJm8jISFRUVGjWrFlW5KZNmzY/7Uj27t07GjRogI6ODr6+vnITatu3b+evv/5i06ZNDB8+PFvXxMbGcuDAATw9PTl//jxFixalW7dudOnShUGDBnHo0CF69uwpY89/L54/f46xsTGJiYn4+fllq425rFusv3jxAldXVzZs2IBIJGL06NFMmDCB9PT0r4TKf0VLbGzsV/tUqlQpS5iUKlWKgwcPUrFiRTZt2sSff/5JpUqV8vT34ODgwNq1awkLC5Pr3Bnh1DlnSCQSjh8/zuLFi7ly5QqNGjVixowZ9O7dO9uHIvfu3aNx48YsWbKEKVOmyNhj6dK0aVNatGjBhg0bFO2KXBCilwIC2UcQMwWQK1eu4OjoiL+/P61atcLZ2RkjIyNFuwV8viGHhoZmCRuxWMzLly8pXLgwBgYGWeKmZcuWX6V2DRw4kKNHj3L37l25NRrIZNSoUWzdupULFy7QokWL765JSEjg6NGjeHp6cvr06axTUGtra3r06EGpUqV4/vw51apV48SJE3Tq1Emu7+F34MWLF5iYmPD+/XvOnTv33eGnP5vPIAI0yxX/7nyG7CCRSIiNjeXq1ausXbsWHx8fVFRUqF27NiVKlODFixdER0eTkfG/h/dSpUqhqan50+5f/z1tv337NkZGRujp6XHmzBmKF8/bg/vr16+pUaOGQqIze4MicTh0V2r7FcR6gLS0NPbt28eSJUu4e/cubdq0wdHREUtLy1wdVI0bNw4PDw9CQkLyVSfAsmXLMn36dBwcHBTtilwQ6soEBLKPIGYKKBKJBF9fXxwdHbl27RoWFhY4OTnRrFkzRbv2FRKJhIcPH2YJG7FYTFxcHEWLFqV169aYmJgAMGvWLHbs2MHAgQPl7mPmEMPo6GiuX7+e1RI7NTUVHx8fPD098fb2JjExkVatWmFtbU2fPn2oVKnSV/uEhoZSq1YtfH19pd4lTeAzr169wszMjJiYGM6dO5c1TyMv8xkyT/kTEhJ+WqcSERFBcnJy1j4qKipUr14dbW3tH4qV3HYivHLlCubm5rRt25YjR478NL0oOygqOgOwRvwYV5+QPO8z1aIuY4xrS8Ej5SA5ORkPDw+WLl1KaGgoHTt2xNHRMc/zQeLi4tDR0aF79+5s3bpVSt7Klnfv3lG2bFn27t1Lv379FO2OXBA6/gkIZB9BzBRwJBIJhw8fZtasWTx48AArKysWLlz43VNrZSAjI4O7d+/i5+eHn58fAQEBfPz4kUKFCmFubp5Vc9O4cWO51ps8e/YMfX199PT0mD17Nvv27WPfvn3Exsby559/YmNjQ//+/alZs+Z3r09ITkN87R5WffqxdfMmepi1+W1nYMiaN2/eYG5uTlRUFL6+vjxMLZer+QwiSQZIMigafIyYK4eJi4v76vUqVapQvXp1ypQpQ1RUFI8ePaJUqVIMHDiQCRMmULNmTZmmQ547d47OnTvTpUsX9u7dS6FCuf88KTI6A58jNHmZobGgm26BicjEx8ezceNGli9fzsuXL+nduzczZsygSZMmUrOxbt06xowZw9WrV2nevLnU9pUVmc1T/v33XwwMCn6tR3xyGg2EWUwCAtlGEDO/Cenp6ezatYu5c+cSFRXFoEGDmDdvHlpaWop27YdIJBIsLS0JCgpizJgxBAYGcvHiRRITEylTpgyGhoZZaWm6uroye3CUSCTcvXuXJUuWsGfPHgCqV6+OtbU11tbWNGjQ4LvpHrJMaRL4OXFxcVhYWPDsDz2KtuiT+43+fz5DM7UX9NApmhVVqVq1Ko8ePcLJyYl9+/ZRpUoVpk2bxvDhw/Oc9pUTjh49Sq9evRg4cCBbtmyRSu1MeHi4VDoP5pScRM8kGemIVFS/iZ7lZ2JjY1m9ejWrVq3i48ePDBo0iGnTplG3bl2p20pLS6Np06aULFmSS5cuKbyu8ldkdmJ79eoVFStWVLQ7Mic4+j2dV1+UuZ0T49qiqyHbOXUCAvJAEDO/GcnJyWzatIlFixbx9u1b/v77b2bOnPlNSpQysHHjRv7++++vWommpKRw9erVrLS0y5cvk5KSQoUKFTAyMsLExAQTExN0dHTyfIMOCwtjz549eHp6EhwcTLly5ahbty5XrlzBy8uLvn37fvc6aaQ0CWSfjx8/freQ/k5CSeJqW0rNTmY9xvXr11m0aBHe3t5oaWnh4ODAX3/9ledUr9zi6emJra0tY8eOZeXKlbn+3GdGZyZMmICTk5OUvcw+WYcAITFExn7/ECDs8nGMqqmyY7WLotyUGs+fP8fNzY2NGzeSkZHBiBEjmDx5ssznJonFYkxMTNi5cye2trYytZVX3N3dmTVrFvHx8UovvKTBzci39Fx/WeZ2Do9qTRPNsjK3IyAgawQx85uSkJDAypUrcXFxITU1lYkTJzJ16lTKlCmjaNeAz+2mGzVqhK2tLRs3bvzhuqSkJK5cuZKVlhYUFERaWhoaGhpZURsTE5Nszx+JiYlh//79eHp6cvnyZYoXL06PHj2wtrbG3NycwoULM2DAAE6cOMHVq1e/SdfLa7rM/G669C8g6TLSIDU1lefPn/90+OO7d++y1otEIqpUqYJGnYa8afE3EpEqSOnhp7AKaAXv4NyRfdSuXRtHR0dsbW3lPtfpe2zatImRI0cyc+ZMFi1alOt9FB2d+S8/6jg3Y8YMNm7cyIsXLxQmIvPKkydPcHFxwcPDg2LFijFu3DjGjx+fVZMnD3r37s2VK1d49OgRJUuWlJvdnDJ+/HjOnTuXNcOsoCNEZgQEcoYgZn5z3r59i4uLCytXrqRIkSJMnz6d8ePHyzVV5r+kpaXRvn17YmJiuHXrVo5ush8/fuTixYtZ3dJu3LiBRCKhRo0aWcLG2NgYDQ2Nr67x9vbG09OTs2fPIhKJ6NixI9bW1nTr1u2bltHx8fG0bNmStLQ0rl69SunSpQHpFTJPsajDWOPvT4MvSGRkZPD69esfCpWoqChevHjx1UyVsmXL/nRKvYaGBmpqajKZzyBJT0M1NpRFppXo27dvnmpUZIGrqytTp05l6dKlTJs2LVd7vH79Gm1tbSZOnKjQ6MyvePjwIfXr12f//v307t1b0e7kiDt37rBkyRK8vLyoUKECkyZN4u+//876HpEn4eHh1KtXj8mTJyv177tbt25kZGRw/PhxRbsiFxKS09ATamYEBLKNIGYEgM9tbZ2cnNi0aRPq6urMnj2bYcOGSXWwWnbnejg7OzN79mwuXLhA69at82Tz7du3nD9/Pkvc3L37uQ1snTp1qFmzJu/fv+fWrVskJSXRrl07bGxs6N279y9PpR8/fkyzZs0wNTXl4MGDeF2LElrM/ocPHz78tPtXVFQUKSkpWeuLFCnyQ5GS+e/sCNvfeT7D7NmzWbRoEevXr+fvv//O1R7Tp09n3bp1ShOd+REGBgZUrFiRY8eOKdqVbHH58mUWL17M8ePH0dbWZtq0aQwePJhixYop1K/Zs2ezbNky7t+//8MGJoqmQYMGGBoasmbNGkW7IhfCw8Ppuuk6HyVFf704lwjdzAQKEoKYEfiKsLAw5s2bx86dO9HW1mb+/PlYW1vnunNYTovgb968SYsWLZg6darUuyplZGRw9OhR1qxZw8WLF79qo1u3bl06duyIsbExhoaG2Uq3O3r0KN27d2eGkysHk/78rYb/paSk8OzZs5+Klffv32etV1FRoUqVKj8VK+XLl5dKPvzvPJ9BIpEwceJEVq9ezc6dO7GxscnxHvklOrNu3TrGjx/P8+fPlbLmDz7/Pnx8fFi8eDEBAQH8+eefzJgxg379+ilFeiJ8TjmuV68ezZs359ChQ4p25xskEgmlS5dm3rx5TJ48WdHuyIyPHz9y4MABPDw8CAgIoKLlGIo17AAi6Te2UfbvMQGBnCKIGYHvEhwczOzZszl8+DC6urosWrSI7t27Z/thMzdF8K1rluPmpqkUTUsgMDBQKlEhiUTCzZs38fT0ZO/evTx//pwaNWpgbW3NgAEDKFOmTNZ8Gz8/P8LDw1FRUaFp06ZZaWlt27b9YURg1qxZ/POkKMVrNkGaz86qKiJa11Rn51D5tyHNyMggJibmp3Uqr169+ir9q1y5cj8UKpnpX/J6ePvd5zNkZGQwbNgwduzYwcGDB+nevXuO98gP0Zm4uDiqVKnCkiVLsLe3V7Q7X5Gens7hw4dZvHgxN27coEWLFjg6OtK1a1eZtuvOLXv27MHa2lopZ2C9efOGChUqcODAAaysrBTtjlRJT0/Hz8+PHTt2cOjQIZKSkjA2NsbOzo5G7TrQfdM1mdlW5gizgEBOEcSMwE+5evUqM2fOxNfXlxYtWuDs7PzLm11ui+BFkgwy0lIZ37YKk3u0ypPfT548wdPTE09PTx49ekSFChXo168f1tbWtGzZ8oeiLCws7CtxEx0dTaFChWjRokWWuGnVqlVWasjDF+/puEp2hZqyuOG8f//+p0Ll2bNnpKamZq0vVqzYT1O/qlev/k1dkaIQ5jN8Jj09nf79+3P06FFOnDiBmZlZjq7PjM7Y29vnqaGArOnTpw8hISHcvn1b0a4AnyOWu3fvZunSpTx69AhTU1NmzJiBiYmJUnfhkkgktGvXjnfv3nHr1i2lqge7du0azZs35/r16zRt2lTR7kiFhw8f4uHhwa5du3j27Bl16tTBzs4OW1tbNDX/l14si9o/RR6UCQjICkHMCGQLPz8/ZsyYwdWrVzE1NcXJyem7w8vyXgQvAUS5KoJ/+fIlXl5eeHp6cvXqVUqWLEmvXr2wtrbG1NQ0xzdoiURCSEhIlrARi8W8efOGIkWK0KpVK0xMTIiq2JJzESmky+CvKDepAMnJyTx79uynYuXjx49Z61VUVKhatepPxYq6urrCHsQkEgkpKSkkJSWRlJREYmLid/87839HfEhn5+tqMvcrP3QBSklJoXv37pw/f56zZ8/muP5s+vTprF+/nrCwMKWNzhw/fpyuXbty8+ZNGjdurDA/EhMT2bJlC8uWLSMqKooePXowY8YMWrRooTCfcsqNGzdo1qwZq1atYuzYsYp2J4v9+/fTt29fYmNjKVeunKLdyTWxsbHs3bsXDw8PgoKCKFu2LP3792fQoEEYGBh89zs2Ki4RM/eA3yqFWUAgNwhiRiDbSCQSjh49ysyZMwkODqZ79+4sWrQIPT094HNERt5F8O/fv+fQoUN4enri5+dHoUKF6NSpE9bW1nTp0kWqxbUZGRkEBwdnCRt/f39K9F9G4bIav744l3yZ0pSRkcHLly9/Wqfy6tWrr64vX778T4VKlSpVcizy0tPTvxERPxMYeVmXlJRERkb2b+QltfRQH7AkR+8nN+SX+QyJiYl07NiRO3fu4O/vn6MH/vwQnUlNTaVatWpYW1vj7u4ud/vv3r1j3bp1rFixgri4OAYMGICDgwO6uvmzFmH48OEcPHiQx48fK42AdXFxwcnJiXfv3il1dOt7pKamcvLkSXbs2MGxY8fIyMjA0tISOzs7unbtmq224oq4rwoI5DcEMSOQY9LT09mzZw9z5swhPDwcGxsb/p48kyEHw+RygvTp0ydOnDiBp6cnJ06cICUlBSMjI6ytrbGysqJsWfk8ZL5PTKbRQl/ZGpFIqPavO88jQnn+/PlX6V/FixfPEiVVq1alUqVKVKpUCXV1dcqWLcsff/yBRCKRusD4sgPZr1BRUaF48eIUK1aMYsWKZeu/c7uuWLFiPHj5UZjP8B8+fPiAqakpERERXLhwIUcT5adNm8aGDRuUOjozadIkdu3axfPnz+VWl/Xq1StWrFjBunXrSE5OZsiQIUydOpUaNWrIxb6siImJQUdHB1tbW9auXatodwAYPXo0ly5dUppUwl+RWafp4eGBp6cnb968oXHjxtjZ2TFgwIBcNauQVtv/qRZ1GWNcO8/7CAgoG4KYEcg1KSkpbNmyhQULFiAxGkMxrUZIpNh55cvc3vT0dMRiMZ6enhw8eJAPHz6gr6+PtbU1/fr1o2rVqlKzm13kNdgs9dgCCn18iaqqKhKJhIyMDNLS0vj06ROfPn0iKSmJnPwZFy1aNM/CIbvrChcuLNfTVGE+w/eJjY2lffv2fPjwgYsXL6KlpZWt62JiYqhRo4ZSR2du375N48aNOXLkCN26dZOprYiICJYtW8aWLVsoVKgQo0ePxt7ensqVK8vUrjxxc3Nj6tSp3Lx5k4YNGyraHSwtLSlSpAje3t6KduWnvHjxgl27duHh4UFwcDCVKlXC1taWQYMGSeXnmNeBzAu66QoRGYECiyBmBPLMnYjXdNtwVWb7m366zMm9W3n58iW1a9fGxsaGAQMG5OiEWRbcjHxLz/WXZW6ndthhyhMvFYFRtGhRpeymJE1+925mPyI6Opp27dohEom4cOECVapUydZ1+SE606RJE2rWrMnBgwdlsv+DBw9YsmQJnp6e/PHHH0ycOJExY8bILQosT1JSUmjYsCFVqlTBz89P4ald9evXp0OHDqxYsUKhfnyPpKQkvL292bFjBz4+PhQuXJju3btjZ2eHhYWF1Bsp5KZLaLva5XHu2UCokREo0OSfo0UBpeXQ7ZhffrHmFklGOmdCk+jfvz82Njbo6+sr/OaaiVoh+YiClW7L801KkzJgXLeiTOfMGNepKPV95YGGhga+vr60a9cOCwsLAgICslVQPWXKFNauXYu7u7vSRmfs7OyYNm0asbGxUhVcQUFBLF68GG9vbzQ0NHB1dWXYsGFK071PFqipqeHu7k6nTp04dOiQQtshSyQSwsPDlSp9TyKRcOnSJTw8PNi3bx8fPnygdevWrF+/nj59+shU4FYvV5ydQw3+N78tJIaI2K8PbkSApnpxjOtUxLalptB+WeC3QIjMCOQZmZ+ElytOwFTlOwkXUpqUk8evPmK+4rzM9s/v8xkePnxIu3btqFGjBufOnaNUqV+/l8zoTHh4uFJ2lIqJiaFq1aqsWLGCMWPG5GkviUSCv78/ixcv5uzZs+jo6ODg4ICtra1UZl/lF7p06cK9e/d48OCBVBup5ISXL19SpUoVuaQQ/oqwsDB27NjBjh07CA0NRUtLi0GDBjFw4EB0dHLWeVOanDjjS89BIzA2Nedd3Bv8jngJ9wuB346CnW8iIHPik9OIlKGQAYiMSyQhOU2mNnJDiSKF0JRx6F5TvbhwY8ohOpVK0a52eVRVpBvBk6SnUSE9lkqKea6TGvXq1cPHx4eQkBC6du1KUlLSL6+ZMmUK6enpCukYlh0qVqyIpaUlHh4eud4jIyODo0eP0rp1a0xMTHj9+jVeXl48ePCAIUOG/FZCBj7XzkRHR7N8+XKF+RAWFgaAtra2Qux/+PCBLVu2YGhoSM2aNXF1daV9+/aIxWJCQ0NZsGCBQoUMQCHSSY0Jo9D7Z/yR8VG4Xwj8lghiRiBPRMQmyDQyAZ8nz4THJsjYSu4wrltR6g/NmeTnlCZF49yzAYWk/HsppKrCgx1z0NXV5ejRo1LdW940adKEEydOEBQURJ8+fX7Zoa5ixYqMGTOGlStXEhcXJycvc4adnR1BQUHcv38/R9elpaXh6elJo0aN6N69O4ULF+bkyZPcuHGDvn37oqqqKiOPlZs6deowYcIEFi9ezLNnzxTiQ3h4OCBfMZOeno6Pjw82NjZUrlyZ4cOHo6amxs6dO3n58iXbtm3DyMhIaWoP09PTgc8zxhQVQRMQUDTK8dcokG9JkWIrZmWwk1NsDDRlUpsBkJ4hwbal0H0mN1QvV5z5ORg2mh2cezXi7hUxurq6dO/eHSsrK54/fy5VG/KkTZs2eHt7c/bsWQYOHJj1UPQjlD0606VLF8qVK5ft6MynT5/YuHEjdevWxcbGhurVq3P+/HnOnz+PpaWl0tTmKZLZs2dTsmRJpk2bphD7YWFhlCtXjtKlS8vc1v3795k+fTqampp06NCBGzduMGfOHCIiIjh79iy2trZKWSsliBkBAUHMCOQReRXBy8tOTpFVSpOqioh2tcvn69oMRdO/uSZTLOpIZa+pFnXp11yTGjVqcPLkSfbu3culS5eoX78+a9as+aUQUFbMzc3Zu3cvBw8eZOTIkT9t8V2xYkVGjx6ttNGZIkWKMGDAAHbt2vXT38fHjx9xdXWlRo0ajBo1imbNmnHjxg1OnjxJu3bt5Oix8lO6dGkWL17Mnj17uHhR9m3o/0tYWJhMi//fvHnD6tWrad68Obq6umzevJkePXoQGBjI/fv3cXBwoHr16jKzLw0EMSMgIIgZgTyirV4CWZ9fiv7fjrIik5QmFRHOPRtIdc/fkbHGOizp1YAihVRyLDhVVUQUKaTC0l4Nvho0JxKJ6NevHw8ePMDa2ppx48bRunXrfDPU77/07NmTbdu2sWXLFiZNmvRTQTN16lTS0tKUNjpjZ2dHdHQ0vr7fDrONjY1l7ty5aGlp4ejoSOfOnXn48CFeXl40adJEAd7mDwYPHkyzZs2YMGGC3EV7eHi41FPMUlJS8Pb2pmfPnmhoaDBp0iQ0NDQ4ePAgL168YO3atbRo0SLfROYEMSMgIIgZgTwiFMHLJqVpQTddYS6AlOjfXBNfe0Na1/zcsvdXoibz9dY11fG1N/zhoLmyZcuyYcMGLl68SHx8PPr6+kybNo2EBOWs7/oZAwcOZO3ataxYsYL58+f/cJ2y1840a9aM+vXrf5Vq9vz5cyZNmoSWlhaurq7Y2dkRGhrKP//8Q5060oncFWRUVFRYtWoVN27cYNu2bXK1La3IjEQi4dq1a4wbNw4NDQ169uyZNQD1+fPnHDlyhF69elGkSBEpeC1f0tI+N8f59OkTRYsWVbA3AgKKQRAzAnlGpkXwIvJFEbwsUpoEpEfmfIazE9sz0EALLfXi30QURXweiDnQQAtf+/bsHGqQLUHZpk0bbt68yYIFC1i1ahV6enqcOnVKJu9DlowePZrFixczf/583NzcfrguMzqjjEMMRSIRdnZ2HD58mJs3bzJ8+HBq1KjBtm3bmDRpEhEREbi7u1OtWjVFu5qvaNWqFTY2Njg6OvL+/Xu52ExPTycyMjJPkZnnz5/j4uKCnp4ezZs358CBA/z111/cuXOHGzduMGHCBCpWVP77y8/IjMx8+vRJiMwI/LYo73G3QL7BxkCT7VfCZbJ3ugSi/D15rj+aqlWrysSGtBhrrEP5kkWYezSYtAxJjhoDqKqIKKQiYkE3XUHIyBCdSqWY102XeeiSkJxGeGwCKWkZqBVSQVu9RK4jgGpqajg6OtK3b1/+/vtvOnXqRL9+/VixYgWVK1eW8ruQHQ4ODnz48IHJkydTunRphg0b9s2aL6MzEydOVLq5M02aNCE5ORl9fX0qVqyIk5MTI0eOlEsReUFm6dKleHt7s2DBArm0a46OjiY1NTXHkZnExES8vb3x8PDA19cXNTU1evTowfLlyzEzM6NQoYL12COIGQEBITIjIAVkVgQvgiqidxzato6aNWsycuRIQkNDpWpD2sgqpUlA+pQoUghdjT9oolkWXY0/pJLKWLt2bc6ePcuOHTs4d+4c9evXZ9OmTWRkKGc3vu/h5OTEmDFjGDFiBF5eXt9dM3XqVFJTU5UqOnPp0iW6dOlChw4dKFKkCDVr1iQ8PJypU6cKQkYKVK1aFUdHR1atWsXDhw9lbi+zLXN2xExGRgbnz59n6NChVK5cGRsbGxISEtiwYQMvX75kz549dOzYscAJGfifmElKShLEjMBviyBmBKSCrOZ67JvyObd5/vz5HD58mDp16jBw4MAcz5KQJ9lJaQIJqXHRNC7xIUcpTQLKj0gkYuDAgTx8+JCePXsycuRI2rdvT3BwsKJdyxYikYhVq1Zha2uLra0tJ06c+GaNsnQ2k0gknDlzBkNDQ9q2bUt4eDg7d+5k8+bNPH36VGHzUQoqkyZNonr16kyaNEnmtjIHZmppaf1wzdOnT5k3bx61a9fG0NAQPz8/7O3tefLkCRcvXmT48OH88ccfMvdVkQhiRkBAEDMCUkKWRfClS5fGwcGB8PBw3N3d8ff3R1dXFysrK65fvy5Vm9IkM6UpYIox9+Z14MS4thwe1ZoT49oSPK8jPQvf5ti8QbwJzR8PuQI5Q11dna1btyIWi3n9+jWNGzdm5syZJCUlKdq1X6KiosLWrVvp2rUrVlZWiMXib9b8KDqTkJxGcPR7bka+JTj6PQnJaVL3Lz09nQMHDqCvr0/Hjh359OkT3t7e3LlzB1tbW6ysrChdunS2Z84IZI+iRYuyfPlyTp069V2RK03CwsKoVKkSxYt/fcjz/v17/vnnH9q1a0ft2rVxc3PD2NiYgIAAnj59yvz586lVq5ZMfVMmMsVMWlqaIGYEfltEkp/14RQQyCFrxI9x9QnJ8z5TLep+1Q73S1JSUti5cydLlizhyZMndOjQgZkzZ+a7GREpKSkYGxsTERHB9evXqVSpkqJdEpARycnJLF68mMWLF6Opqcn69esxMzNTtFu/JDk5ma5du3LlyhV8fX0xMDD46vUpU6awefNmzgUFc/zhe8SPYoiMS+TLm4oI0CxXHOO6FbEx0ESnUu5nJ6WkpLB7926WLFlCSEgIpqamODo6Ymxs/E0r3eHDh+Pj40NYWJjSTGsvCEgkEszNzYmMjOTevXuoqanJxM5ff/3FgwcP+Pfff0lPT+fs2bN4eHjg7e1NcnIyZmZm2NnZ0bNnz28Ez+/EunXrGD9+POnp6Xh6ejJgwABFuyQgIHeEb3gBqSKLuR7/RU1NjaFDh/LgwQM8PT15/vw57du3p3379pw5c+anczKUCTU1Nfbv309aWhr9+/fParEpUPAoUqQI8+bN4/bt21StWhVzc3MGDhzI69evFe3aTylSpAiHDx+mUaNGWFpacufOna9etxkxnuKdp9N72212BkYQ8R8hAyABIuIS2RkYgfmK8wzcEkhUXGKO/EhMTGTVqlXUrl2bIUOG8OeffxIYGIivry8mJibfnQkyePBgIiMjCQgIyOG7FvgZIpGIFStWEBoayqpVq2RmJywsjHLlyjFt2jSqV6+OpaUlt2/fZt68eURFReHj44ONjc1vLWTgc2QmsxZIiMwI/K4IYkZA6sirCL5QoUIMGDCA27dv4+3tzadPn+jYsSPNmzfn8OHD+aLoWkNDg/3793Px4kUcHBwU7Y6AjKlXrx5isZgtW7Zw4sQJ6tWrx7Zt25RagJcoUYITJ06gra2NhYUFjx8/BmBvUCT9dwZTVPPzcNdfde/LfP1yaCxm7gHsDYr8pe13797h5OSElpYWkyZNwsjIiHv37nH48GFatGjx02tbt25N7dq1hVQzGaCnp8eoUaNYsGABL1++lOrer1+/ZtWqVVy5coVTp06xdetWrKysCAoKIjg4mOnTpyt9Z0t5kp6enhV5FMSMwO+KIGYEZIIs53r8FxUVFbp3705gYCA+Pj6ULFmSXr160bBhQ3bv3q30EY927drh6urK8uXL2bdvn6LdEZAxIpGIIUOG8PDhQzp16sSQIUMwNjbm0aNHinbth/zxxx+cOXOGsmXLYmZmxqLD13A4dJfktAwkopzdRtIzJCSnZeBw6C5rxI+/u+bVq1c4ODigqanJwoUL6du3L48fP2bHjh3o6mavNk8kEjFo0CAOHDhAfHx8jnwU+DXz58+ncOHCzJw5M897JScnc+jQIbp3746GhgaTJk0iJSWFv//+m+joaFavXk2zZs2+G4H73RHEjICAUDMjIEekOdfjV1y6dAknJydOnTpFzZo1mT59OnZ2dko74VkikWBra8uRI0cIDAzM9gObQP7n7NmzjBo1iqioKGbMmMGMGTOU9nP6/Plz2gyaCi1spLbn0l4NsqKx4eHhuLq6smXLFgoXLszo0aOZOHFirmf1hIeHU6NGDbZv346dnZ3UfBb4zLp16xg7dixXr16lWbNmObpWIpEQFBTEjh072LNnD3Fxcejr62NnZ0fr1q1p1qwZZ86cwcLCQkbeFwxcXFxYtGgRHz9+5OrVqzRv3lzRLgkIyB1BzAgUaG7evImzszMHDx5EQ0ODKVOmMHz4cEqUKKFo174hISGBVq1a8enTJ4KCggp8S1GB/5GUlMSiRYtwcXGhVq1abNy4EUNDQ0W79Q1RcYmYuvmTnJYhtVPyIoVUWN9FA491buzevZuyZcsyYcIExowZQ9myZfO8f2ZzAD8/Pyl4K/AlaWlpNG3alJIlS3Lp0qVsfSaePXvGrl278PDw4OHDh2hoaGBra8ugQYOyDnH8/PwwNTUlJCQEHR0dWb+NfI2zszMuLi68f/+eu3fvoqenp2iXBATkjpBmJlCgadKkCfv37yc4OBhTU1OmTJmCtrY2zs7OvH//XtHufUWJEiU4dOgQMTEx2NnZ5YuaHwHpUKxYMZycnLh16xbq6uoYGRkxZMgQYmNjFe3aVzgevku6BKmm+ySnpjHA/Sh+fn4sX76c8PBwZs2aJRUhA58bAYjFYiIiIqSyn8D/KFSoECtXruTKlSt4enr+cF1CQgK7du3C3NwcTU1N5s+fT5MmTTh9+jSRkZEsXbr0q2h0eHg4IpEITU1hiPCv+DLNrGjRogr2RkBAMQhiRuC3oH79+nh4ePD48WN69+7N/Pnz0dTUZObMmUrVUap27drs2rWLI0eOsGTJEkW7IyBndHV1uXDhAhs2bODQoUPUq1ePXbt2KUWDgMevPnLhyZtfFvrnGJEKxWo05cy/t5kwYYLUo6ZWVlaUKFGCnTt3SnVfgc8YGxtjZWXFtGnTvqpNysjIwN/fnyFDhlC5cmUGDhxIcnIymzdv5tWrV3h6etKhQwdUVVW/2TMsLAwNDQ2lTbdUJoSaGQEBQcwI/GbUqFGD9evXExYWxvDhw1m5ciXa2trY29vz/PlzRbsHQJcuXZgzZw6zZs3izJkzinZHQM6oqKgwcuRIHj58iKmpKQMHDsTCwoInT54o1K/dgZE5breeXVRVROy7/kIme5csWRIrKys8PDyUQhQWRFxdXYmNjc2a/TVnzhxq1aqVNcxyypQpPH36lPPnzzN06FBKly790/3CwsKoUaOGnLzP36Snp2dFSgUxI/C7IogZgd8SDQ0NXF1diYiIYPLkyWzfvp2aNWsycuRIQkNDFe0ec+fOxdLSEmtra8LDwxXtjoACqFy5Mnv37uXkyZM8fvyYBg0a4OzsTEpKikL8ET+KkX5U5v9Jz5AgDomRyd4AdnZ2PHnyhMuXL8vMxu9MmTJlMDU1xdnZGR0dHVasWIGpqSnnz5/nyZMnzJ07l5o1a2Z7v/DwcLS1tWXncAFCEDMCAoKYEfjNUVdXZ8GCBURERDB//nwOHz5MnTp1GDhwIPfv31eYXyoqKuzatYsyZcrQq1cvkpKSFOaLgGKxtLQkODiYcePGMWfOHJo2bcqlS5fk6kN8chqRORx0mVMiYxNJSJZNG3UjIyM0NTWFmTNSJC0tjZMnT9K/f38qV67M6dOnUVNTo1mzZrx8+ZJ//vmHdu3a5aq+SojMZJ/09PSs/xZqZgR+VwQxIyAAlC5dGgcHB8LDw3Fzc8Pf3x9dXV2srKy4fv26QnwqW7Yshw4d4uHDh4waNUpIkfmNKVGiBC4uLly7do3ixYvTtm1b/v77b969eycX+xGxCcj60ycBwmMTZLK3iooKgwYNwsvLSzgYyCN3795lypQpVK9enc6dO3Pv3j0WLlxIVFQU27Zt49q1a1y5ciXX+ycnJxMdHS1EZrJJWloaIpGIokWLCnN4BH5bBDEjIPAFxYsXZ/z48Tx9+pTNmzdz+/ZtmjVrhqWlJRcuXJC7P40aNWLTpk14eHiwYcMGudsXUC4aN27MlStXWL16NZ6entSrVw8vLy+ZC92UNPl01pOlnUGDBvHhwweOHDkiMxsFlZiYGFasWEGTJk1o2LAh27dvp0+fPly7do27d+8ydepUNDQ06N+/P23atGHixIm5Hlac2XVOiMxkj8zIjJBiJvA7I4gZAYHvoKamxrBhw3j48CGenp48e/aM9u3b0759e86cOSPXKImtrS3jxo1jwoQJeTrxFCgYqKqqMnbsWO7fv0/r1q3p378/nTt3JiwsTKp20tLSePjwIQcPHmTbls1S3ftHqBWS3S1JR0eH1q1bC6lm2SQ5OZkDBw7QrVs3qlatyrRp06hRowbe3t5ER0ezatUq9PX1v4oGiEQiVq1aRXBwcK4PXzJrBIXITPbIrJkRUswEfmcEMSMg8BMKFSrEgAEDuH37Nt7e3nz69ImOHTvSvHlzDh8+LLdZMK6urrRo0YLevXvz6tUrudgUUG6qVavGoUOH8Pb25u7du+jq6rJs2TJSU1NztE9GRgZhYWEcO3aMxYsXY2trS+PGjSlRogT169end+/e7N28GmQs4EWAtrpsh9na2dnh4+NDdHS0TO3kVyQSCYGBgYwePZoqVarQp08fXr58yYoVK3jx4gWHDh2ie/fuqKmp/XCPpk2bMnToUObMmZOrOUlhYWGoqqpSvXr1vLyV34b09HQkEokQmRH4rRHEjIBANlBRUaF79+4EBgbi4+NDyZIl6dWrFw0bNmT37t25TqnILmpqauzfv5+MjAz69euX4wdWgYJL9+7duX//PiNGjMDBwYHmzZtz9erVb9ZJJBKeP3/OmTNnWL58OUOGDKFFixaULl2amjVr0q1bN5YuXUp4eDgtW7bE1dUVPz8/Xr16RUx0FFoyFhqa6sUpUaSQTG307dsXNTU1du3aJVM7+Y2oqCicnZ2pX78+LVu25OjRo4wYMYLg4GCuXr3KmDFjUFdXz/Z+Tk5OpKenM2fOnBz7Eh4eTrVq1ShUSLafhYKCkGYmIAAiiVBVLCCQKy5duoSTkxOnTp2iZs2aTJ8+HTs7O5kOert48SLGxsaMGzcONzc3mdkRyJ9cu3aNESNGcPPmTXr06EGbNm148uQJ9+7d4969e7x//x74XBumq6uLrq4uenp6Wf9oaGj8sIh43tFgdgZGyKQ9s6qKiIEGWszrpvvrxXlkwIAB3Llzh3v37v3WBdPx8fEcOnSIHTt24OfnR9GiRenVqxd2dnaYmJh8d5hlTnBzc2Pq1KncvHmThg0bZvu6/v378+rVK8RicZ7s/y4MHTqUY8eOoaWlRVBQkKLdERBQCIKYERDIIzdv3sTZ2ZmDBw+ioaHBlClTGD58uNQnmWeyevVqxo8fz549e+jfv79MbAjkD969e0dwcDDBwcFZgiU4OJiYmP/NbNHS0qJNmzbo6elliRdtbe2sqeHZ5fGrj5ivOC/tt5CFr317alcsJbP9Mzl9+jSWlpYEBQXRrFkzmdtTJjIyMggICMDDw4MDBw6QkJCAoaEhdnZ2WFlZ/XKYZU5ISUmhYcOGaGhocO7cuWwLRwMDA/7880+2bdsmNV8KMoMHD+bkyZPUq1eP8+dl9/cpIKDMCHFcAYE80qRJE/bv38+DBw9YsmQJU6ZMwcnJCXt7e8aMGcMff/whVXtjx44lMDCQoUOHZp2oCxRsEhISePDgwVeC5d69ezx79gz43BRAR0cHPT09Ro8eja6uLmXLlsXNzY2TJ0/SqFEjbG1t81SHoFOpFO1ql+dyaKxUozOqKiJa11SXi5ABMDc3p0qVKmzfvv23ETMhISHs2LGDnTt3EhkZSa1atZg2bRoDBw6UWdcwNTU13N3d6dSpE4cOHcLKyipb14WHh9O5c2eZ+FQQEWpmBASEyIyAgNQJCwvDxcWFrVu3UqxYMcaOHcvEiRMpX7681GwkJibSqlUrkpKSuHr1KmXKlJHa3gKKIzk5mUePHn0lWO7du0dYWFhWB72aNWt+FWXR09Ojbt26301vlEgkHDp0iHHjxvHhwwcWLVrEuHHjcp1CFBWXiJl7AMnSaqEskVCksCq+9oZUL1dcOntmg2nTprFlyxaio6NlmhaqSN6+fYuXlxc7duzgypUr/9fencfVnP1/AH/dW6QaawkhWwvSDAYtQ2RniPCdIhWTLcyXLCNZC6GxzAwSZnAjk7HURChNTfY0GLIXk5pBiyxpr3t/f/jqZyla7tLtvp6Phwd1b+ecS12f1+ec8z6oV68e7O3t4eLiAisrK7ktsRs2bBhu3LiBmzdvfvSCOzs7G5988gkCAgLg5OQkl/Epu3HjxiEiIgJffPEFy46TymKYIZKRhw8fYv369SUlSqdMmYJ58+ahefPmUmn/3r176NatG3r16oWQkJAKLxsixSkqKkJiYuJbgeX69etISEgo2dDbokWLtwKLqakpOnTogE8++aTC/T1//hyenp7YunUrunbtiu3bt6Nr166VGntQXDI8DsdX6mtL0/TB74jwXy7VJU4fc+PGDXTq1AmHDh3CqFGj5NavrBUVFSE8PBwikQihoaEoLCzEoEGD4OzsjBEjRijk7v3du3fRqVMnLF26FIsXL/7gc2/evAlTU1OcOnUKvXr1ktMIlZu9vT1OnjyJgQMHIigoSNHDIVIIhhkiGcvIyMAPP/yATZs2ITc3FxMmTMCCBQvQtm3bKrd97NgxDBs2DN7e3h+9UCD5E4vFSEpKem9Py61bt1BQUAAAaNy48VuB5fXvsphtu3DhQkmVqtmzZ8PLy6tS4WhzdALWRdyt/EAkEkAgwChDdeyca49WrVohLCwMLVq0qHybFdS9e3fo6+vXiLvZV69ehUgkQmBgINLS0tCpUye4uLjA0dERzZo1U/TwMH/+fPj5+eHOnTsf/DcOCwvDsGHDkJKSItfvBWU2ZswY/P777xg5ciT3GZHKYpghkpMXL17Az88PGzZsQGZmJsaOHYuFCxeiY8eOVWrXy8sLXl5eOHbsGAYPHiyl0VJFSCQSPHz48L09LTdu3EBOTg4AoH79+qWGFj09PbmOtbCwEBs3bsTy5cvRuHFjbNmyBcOGDatwO0FxyVgWegNFYkmF9tCoCYCiwgLUjg/BuT3rkJaWhqFDh6KoqAhhYWHo3LlzhcdSGZs3b4a7uzv+/fdfuf8bSENqaioCAwMREBCAq1evonHjxhg3bhxcXFzQuXPnalWp7cWLFzAyMkL//v0RGBhY5vO2bNkCd3d35ObmVrmamqoYOXIkoqOj4ejoCD8/P0UPh0ghGGaI5CwnJwc//fQTvvvuO/z777+ws7ODp6cnPv/880q1JxaLYWtri3PnzuHSpUsy29BLr6Snp78XWt4te9yxY8e3Sh6bmpqiefPm1eoC8/79+5g+fTrCw8MxZswY/PDDD9DX169QGymZOfAMjsfpxAyoCQUfDDWvH+9lqIvJXeriq6F9oa+vj6ioKOTm5mLYsGG4e/cuDhw4IJdQ/uTJEzRr1gy+vr6YPXu2zPuThry8PBw5cgQikQgnTpyAmpoahg8fDmdnZwwZMgS1atVS9BDLtHPnTri6uuLMmTP44osv3nosO78ISU+ysW7jDzgdHYWrZyNlfuZQTTF8+HBER0dj6tSpWL9+vaKHQ6QQDDNEClJQUICAgACsWbMG9+7dw+DBg+Hp6VmpteJPnz5Ft27dUK9ePZw9exZaWvLbTF1Tfazsce3atdG+ffv3ZlsqU/ZYUSQSCfbv349Zs2YhLy8Pq1evxtSpUyt8VzwhNQuBscmIvpuG5Cc5ePM/FQFeHYhpY6yH8RYGJVXL4uPj0bt3b3To0AERERGQSCQYO3Ysjh8/Dj8/P0yZMkV6L7QMo0aNwt9//40rV67IvK/KkkgkuHDhAkQiEfbv349nz57B3Nwczs7OcHBwQKNGjRQ9xHIRi8UwNzeHWCzGxYsXcT8j59X3zJ00JGeW8j3TSAs2JnpwNDeAURP5VLpTRkOHDsUff/yBOXPmYOXKlYoeDpFCMMwQKVhRURF+/fVX+Pj44MaNG+jVqxcWLVqEgQMHVuhO/rVr12BhYYH//Oc/2L17d5lf+/ouaEGRGLXVhWito63Sd0HfLXv8OrSUVvb4zQ35hoaGNeaU8qdPn2LBggXYsWMHzM3NsX379goddPiminx/Xbx4Ef369YOlpSWOHDkCdXV1zJo1C1u2bIGHhwdWrVol02AYGhqKESNG4OrVq5V+vbLy4MED7NmzBwEBAUhISECLFi3g5OQEZ2dntG/fXtHDq5Tz58/Deogdes3xw/1cjQrN5vnYmcm14p2yGDRoEGJiYrB48WLumySVxTBDVE2IxWIcOXIEq1atKjnQz9PTEyNGjCj3Bd2+ffvg6OiILVu2YPr06SWfL7lzrsJ3Qd8se/zmErF3yx6/GVg+VPa4Jjp9+jSmTp2KhIQEzJ07F0uXLpX5LN8ff/yBIUOGYMiQIfj111+hpqaG77//HnPnzoW9vT127dqFOnXqyKTvwsJCNG/eHE5OTtViic7Lly9x6NAhiEQiREdHQ0tLC6NHj4azszNsbGyUfh9JUFwyPA9eQTEEEAjL/1rUhAKoCwXwsjWFQ3cDGY5Q+fTv3x8xMTFYs2YN5s6dq+jhECkEwwxRNSORSBAZGYlVq1YhJiYGpqamWLhwIezt7cs1EzBr1ixs3boVMTExaGHyWaX2NCjzXdDXZY/f3dMiq7LHNU1BQQF8fX2xcuVK6OvrY+vWrRg0aJBM+wwLC8PIkSMxduxY7N69G0KhEIcPH4ajoyO6deuGkJAQ6OjoyKTv2bNnIygoCCkpKQrZcyIWixEdHQ2RSIRDhw4hJycHNjY2cHZ2xujRo1G3bs24uVDlCnj/M2+gMWbaGElhRDVDnz59cOrUKWzevPmtG1hEqoRhhqgaO3v2LFatWoXjx4+jbdu28PDwgLOz8wdnCgoLC9G3b1/8LdSHVi8XFEtQsWpTSnIXtLSyx9evX8ft27cVUva4prl79y7c3NwQFRWFsWPHYuPGjWjSpInM+tu/fz/Gjh0LNzc3bN68GQKBABcuXMDw4cPRsGFDHD9+HO3atZN6v1euXEHXrl1x5MiRSlV1q6w7d+5AJBJh7969SElJgaGhIVxcXODk5IRWrVrJbRzyIO2zidaOMoN9NX5vkqeePXvi7Nmz2LlzJyZOnKjo4RApBMMMkRK4cuUKfHx8cOjQIejr62PevHmYPHkytLW1S33+6tDL2Hb+Ucl5HpVVHe6Cvlv2+PWMS2llj9+dbVHGkrvViUQiwZ49ezBnzhwUFxfD19cXrq6uMtvH8tNPP2Hy5MlYuHAhfHx8ALw6HHbo0KHIzMxEaGgoLC0tpdqnRCLBZ599BhMTExw4cECqbb8rMzMT+/fvh0gkQmxsLOrXrw8HBwc4OzvD0tKyWlW7k5aUzBz03xiD/CKx1NrUUBci0r230s4eS5O5uTkuXryIX375BQ4ODooeDpFCMMwQKZFbt25hzZo1CAwMRMOGDeHu7o4ZM2agfv36Jc9R5rugFS17/Dq8VLeyxzVNRkYG5s+fj927d6Nnz57Ytm1blc9HKsvGjRsxZ84crF69Gh4eHgBelVEeOXIk/vzzT+zduxejR4+Wap8bNmzAwoUL8ejRI6lXByssLMSJEycgEolw5MgRFBcXY/DgwXB2doatra3M9gNVF04/x+Lc/ScVmh3+GDWhAFZtdbDH1VxqbSqrrl274sqVKwgJCcGIESMUPRwihWCYIVJCf//9N3x9fbFz505oampi5syZmD17NnKFWkpxF7S0ssfXr19Heno6gJpR9rgmen2eRVJSEhYsWIBFixbJ5GLcy8sLy5cvf6uQRV5eHiZOnIj9+/fju+++w5w5c6QWYFNTU9G8eXNs2rQJbm5uVW5PIpHgr7/+gkgkwr59+5Ceno5PP/0ULi4uGDduHJo2bSqFUVd/CalZGPD9KZm1H+luXVLqW1WZmZnh+vXrOHHihMz3thFVVwwzRErs4cOHWL9+Pfz9/QEApjO34omaDoql+FNdlbug2dnZuHnz5ntntahS2eOa5vV5NKtXr0br1q3h7++Pvn37SrUPiUSCuXPnYuPGjRCJRHB2dgbwap/UkiVL4OPjg+nTp+OHH34o+T6pasnxYcOGIT09HbGxsZUe9+PHjxEYGAiRSIT4+Hjo6enB0dERzs7O6Ny5c6XbVVbLQ29gT+wDqc7KvKYmFMDJvBWW25pKvW1l0qFDB9y+fRsxMTGwtrZW9HCIFIJhhqgGyMjIgNf323Gk+DOZ9fGhu6CVKXtsamoKExOTGr/Mpqa6desWpk6ditOnT8PZ2Rnr16+Hrq6u1NqXSCSYMmUKdu7ciYMHD8LOzq7ksR07dsDNzQ19RoxFj3Fzcfb+0yqXHD9w4AC++uor3Lp1q0LnuOTl5eG3336DSCRCeHg41NXVYWtrCxcXFwwaNEghFdKqi97fReNBZo7M2m+lo4WYeTYya18ZGBoa4t69e7h48SK6d++u6OEQKQTDDFENsTz0BvZcSJLqrMxrr++CLh5q8tGyx82bN3/rnBaWPa65xGIxdu3ahfnz50MoFGLdunVwcXGR2vKv4uJiODo6Ijg4GEeOHMHAgQMBvNpUPuWnaNx6CkBcDHzgzJLylhzPy8tDs2bNMG3aNKxevfqD45JIJDh37hxEIhF+/fVXPH/+HBYWFnBxcYG9vT0aNmxY6ddcU7zML4LZ8nDI8gJDAOD68kEqfehv69at8eDBA8THx6NTp06KHg6RQjDMENUQsr4LKszOwL/bppSUPdbV1YWZmdl7sy0se6x60tLSMGfOHAQGBsLGxgb+/v4wNjaWStsFBQUYNWoUoqKicPLkSaTUbolloTdQJJZIveT49OnTERoaigcPHpR6QGVSUhL27NmDgIAAJCYmomXLlnB2doaTkxNMTEwq/RprohsPn+PLTWdk3k/YNz1hql//40+soVq2bIl//vkHiYmJMildTqQMGGaIagB53AWFRIJvmiWh66emLHtMpYqIiICbmxv++ecfLFq0CAsWLPjgmUjllZubi6FDh+KmoCU0ze2r3F5ZJcdjY2NhYWGBiIgIDBgwAACQlZWFgwcPQiQSISYmBtra2hg9ejRcXFzQp08fFqQow5Xkp7Dbek7m/QS7WaGLgerOhDVr1gyPHz/Gv//+C319fUUPh0gh+C5MVAM8eJIt2yADAAIBen85BtbW1gwyVKqBAwfi+vXrmDt3LlasWIHOnTvj9OnTVW5XU1MTLt7bpBJkAGBdxF3sj0t+7/M9evSAiYkJdu3ahZMnT2L8+PFo0qQJXF1doaamBpFIhMePH0MkEqFv374MMh9QW10+fzfy6qe6er28V1NTU8EjIVIc1V1oSlSDFEixFPOHfGHdGwWP7kJDQwPa2trQ0tIq+b2sP3/s8dKeW6dOHZ4bo6Q0NTXh4+ODsWPHYurUqbC2toarqyt8fX0rfYZLSmYOVkfck+o4l4begFU73bf20Ny5cwdNmzZFUFAQfvnlFxgbG2Px4sUYP348DAx44nxFtNbRhgCQ+Z6Z1jqlHxysKoqKigCAhVRIpTHMENUA8ro7uXrVCjSQvEROTg6ys7Pf+v3NPz979qzMx8uzslUgEJQEnaoGo7L+XLt2bTn8jakuMzMznDlzBtu3b4eHhwdCQ0OxceNGjBs3rsJB1TM4HkVSLu9bJJbAMzge3480RFBQEEQiEeLi4lCvXj1IJBIsXLgQq1atYqiuJG0NdRg00pLpPj4DHS2V3vwPvCrCATDMkGpT7XcBohpCXndBp44bVaWLB4lEgvz8/I+GoY89/vLlS6Smppb6eF5eXrnGoq6uLrVgVNbjpW0iVyVCoRDTpk3DiBEjMGvWLIwfPx4BAQHw8/Mr92blhNQsnE7MkPrYisUSnE7MQEvTsSjISMaQIUNw4MABDBs2DLa2tjhz5gyDTBVkZ2ejfs4/kIjrQ/CBanOVpSYUwMaYy12Li4uhrq7O71VSaSwAQFRD8EyHV8RiMXJzc6sUlj72eGFhYbnGoqGhIbVgVNrjmpqaSnURExYWhhkzZiA1NRVLly7FvHnzPnoOiywPXoS4GJ21s7Btan80adKk5NOBgYEYP348K0RVwsuXL+Hn54d169YhS6iNJhM3y6yvD519pSq0tbUhkUiQkyO7936i6o4zM0Q1hI2JnkxP21aWu6BCoRDa2trQ1pbdWvrCwsIPBqDyhqWHDx+W+TXlvc8k7f1K7/65du3aUgtMX375Jfr06YNly5ZhyZIl2LdvH7Zv3w5LS8syvyb6TppsggwACNXwVEv/rSADAHZ2dqhbty4CAgLg5eUlm75rmKysLGzevBnr16/Hixcv8PXXX8PDwwNLfk/FuftPpPpvqCYUwKqtjsoHGeDVzIw0KgYSKTPOzBDVEAmpWRjw/SmZtc+7oPJT2nI8ac4s5eTkIDc3t1xjUVNTk0mhh3v37mHBggW4cuUK3Nzc4OPj894ZRYo8eNHV1RVRUVG4d+8eq5Z9wPPnz7Fp0yZs3LgRL1++hKurKzw8PEoKJqRk5qD/xhjkS7FIiYa6EJHuvcs8AFWVaGhooG7dusjIkP5STCJlwTBDVIM4/Rwrs7uge1zNpdYmKd67y/GkHZays7PLvRwPeFX0oWHDhtDR0SkJPWo6rZBs6ijDv4VXSjt48dSpU+jduzf++OMP9O7dW+ZjUDbPnj3Djz/+iI0bNyI3NxeTJ0/GggUL0KJFi/eeGxSXDI/D8VLre+0oM9iXcfCpqlFXV0fjxo3x6NEjRQ+FSGG4zIyoBvGxM0P/jTFSDTPqQgF87Myk1h5VD/JejvehMPTw4UMEBQXhxo0baNCgAczMzCAUCpFWLJ8776WVNu/Zsyfatm0LkUjEMPOGp0+f4vvvv8cPP/yA/Px8TJkyBd9++y2aN29e5tc4dDdAxst8rIu4W+X+5w80YZB5g0QiYWVGUnmcmSGqYXgXlJRVSEgIZs6ciadPn8Lb2xsDvpoIW7/zMu+3tJkZAPDy8sK6devw+PFjmYY+ZZCZmYmNGzfixx9/RGFhIaZNm4b58+ejWbNm5W4jKC4Zy0JvoEgsqdANFzWhAOpCAbxtTfle9A6hUIh27dohISFB0UMhUhguBCaqYRy6G2DeQGOptMW7oCRPI0eOxK1btzBp0iTMnz8fznZDZN7nhw5edHZ2xsuXL3H48GGZj6O6ysjIgKenJ1q1aoUNGzZg8uTJuH//PjZs2FChIAO8em+KdO8Nq7Y6rz4h+fA+GjXhq8ITVm11EOnem+9FpeDMDBFnZohqLN4FJWUWFxeHKVOmIM18Bmo1rNhFc4VkpeHETEu0b9++1Id79+6NWrVqITIyUnZjqIbS09Oxfv16bN78qrTyjBkzMHfuXOjpVb2q4e3bt/Fpz4FwWLIFDwo/QfKTnLeKPAjw6kBMG2M9jLcwYOGRMojFYqipqaFz5864cuWKoodDpDDcM0NUQzl0N8AX7XThGRyP04kZUBMKPhhqXj9u1VYHPnZmrBRECtW9e3fExcVh1KpfcDWnWDYHLwoAYXoCrKzmIiQkBNbW1u89x8XFBZMmTUJKSgpatmwp9TFUN6mpqVi3bh38/PwgFArxzTffYO7cudDV1ZVaH8HBwaidl4ltU/pDU1MT2flFSHqSjYIiMWqrC9FaR7tKh/OqiufZeail1waCxm1w4+Fz/r2RyuLMDJEKSEjNQmBsMqLvpvEuKCkVWZccP+zaBfOnOuHMmTPYtWsXxo0b99bjWVlZaNq0KRYtWgRPT0+ZjUPRHj9+DF9fX/j7+0NdXR3//e9/4e7uDh0dHan31aNHDxgYGODgwYNSb7umK3kvv5OG5MxS3ssbacHGRA+O5gYwasL3clINDDNEKoZ3QUnZyLrkeEFBAaZMmQKRSISVK1fC09PzrYNCnZycEBsbizt37kjtANHq4uHDh/D19cW2bdtQu3ZtzJo1C7Nnz0ajRo1k0t8///yDli1bYu/evXB0lH3Z7ZoiJTOnwrPsvQx1OctOKoFhhoiIqjWpH7wokUCjlhCR7n1KLvQkEglWrFiBZcuWwdXVFVu3bkWtWrUAAJGRkRgwYADOnTsHS0tL6YxBwf7991+sXbsW27dvh6amJmbPno1Zs2a9d3CptG3evBnu7u5IT0+XeV81RVX3P3rZmsKB+x+pBmM1MyIiqtZaNtKCl62p9BoUCJB/dg/S/r71xqcEWLp0KUQiEQICAvDll1/ixYsXAAAbGxu0aNECIpFIemNQkJSUFMyYMQNt27bF3r17sWjRIiQlJWHZsmVyCRfBwcHo27cvg0w5bY5OgMfheOQXiSs8M1ksliC/SAyPw/HYHM3SzVRzMcwQEVG1J82S45N7NIHey3v44osv8PPPP7/1mLOzM06cOIGLFy+iZ8+eSElJgZqaGpycnLB//37k5eVJZQzy9uDBA7i5uaFdu3YICgrCsmXLkJSUhCVLlqB+/ffP2JGFJ0+eICYmBnZ2dnLpT9kFxSVL5aBRAFgXcRf745Kl0hZRdcMwQ0RESmGmjRHWjDKDhrqw5AyS8lITCqChLsTaUWZYZNcNp0+fxoQJEzBp0iRMnjz5rZDSt29fnDt3Di9evICFhQX++usvuLi44NmzZwgNDZX2y5KppKQkTJkyBUZGRjh48CBWrFiBpKQkeHp6ol69enIdy9GjRyEWizFixAi59quMUjJzsCz0hlTbXBp6AymZOVJtk6g6YJghIiKl8e7Bix8LNWUdvKihoQF/f3/s2rULe/fuRc+ePZGUlFTydR07dsSFCxfQrFkz9OrVC/fv34eFhQV2794tk9clbffv34erqyuMjIwQEhKCVatW4e+//8aCBQtQt65iqlwFBwfD0tKywodtqiLP4HgUSbHgBQAUiSXwDI6XaptE1QHDDBERKZWWjbSwx9UcJ2dbw8m8FVrpaOHdSCORSKCWk4mhRp8g0t0ae1zNS63qNGHCBJw/fx6ZmZn4/PPPER4eXvJY06ZNERMTAxsbGwwfPhxt27ZFeHg4Hj16JONXWHmJiYmYOHEijI2NERYWhrVr1+Lvv//G/Pnz8cknnyhsXNnZ2QgPD+cSs3JISM3C6cQMqVbvA17toTmdmIHEtCyptkukaAwzRESklIya1MVyW1PEzLPB9eWDEPZNTwS7WSHsm57YMbAuakf4wH9yP+z8fjVyc3PLbKdz5864dOkSLCwsMGTIEKxYsQJi8avKadra2ggODoabmxv27dsHgUCAvXv3yuslltvdu3fh7OwMExMTnDhxAuvWrcP9+/cxZ84caGtrK3p4CA8PR15eHsNMOQTGJld4GWV5qQkF2HuBe2eoZmFpZiIiqpHy8/Ph6+uLlStXokWLFvD398eAAQPKfL5YLMbKlSuxfPlyDB06FHv27EHDhg0BvJrp+eGHH+Du7o769evj0aNH0NTUlNdLKdPt27excuVK/PLLL2jatCk8PDwwadKkajG2N40fPx7Xrl3DtWvXFD2Uaq/3d9F4IMO9La10tBAzz0Zm7RPJG2dmiIioRtLQ0MCSJUsQHx+PVq1aYeDAgXB0dERaWlqpzxcKhVi6dCmOHTuGc+fOoVu3bvjrr78AvCrdPHv2bHh6euL58+ewtLRERkaGHF/N227evImxY8eiY8eOiImJwY8//oh79+7hm2++qXZBpqCgAEePHuWsTDm8zC9Csow36Sc/yUF2fpFM+yCSJ4YZIiKq0YyNjfH7779j9+7dCA8PR/v27fHzzz+XLCV71+DBg3Hp0iU0aNAAlpaWCAgIKHnMy8sLOjo6uHv3LqysrJCYmCivlwEAuH79Ouzt7dGpUyecPXsWfn5+SExMxIwZM1CnTh25jqW8/vjjDzx//pxhphwePMmGrJfLSAAkPcmWcS9E8sMwQ0RENZ5AIICLiwtu374NW1tbTJo0CX369MGtW7dKfX6bNm1w9uxZjBs3Di4uLnBzc0N+fj7U1dUxYcIE1KlTBwKBABYWFog6dRY3Hj7HleSnuPHwuUzuel+7dg1jxoyBmZkZYmNj4e/vj8TEREybNg0aGhpS70+agoOD0bp1a3z22WeKHkq1UVRUhMePHyM+Ph5RUVEICgrCpk2bsNnPXy79FxSVHuSJlBH3zBARkcqJiorCtGnTkJSUhIULF2LhwoVlzmz89NNPmDlzJj777DMcPHgQz549w+c2X2LUgo34899cFNZpAIHg/zdsCwAYNNKCjYkeHM0NYNSk8qWQr1y5Am9vb4SEhKBNmzZYtGgRnJycULt27Uq3KU9isRgtWrSAg4MDNmzYoOjhyExhYSEyMjKQnp6OtLQ0pKenl/x69+P09HRkZma+10bt2rWhZ9IVal8ulvl4w77pCVN9+RyWSiRrDDNERKSS8vLy4OPjgzVr1qBNmzbw9/eHjU3pG6P//PNPjB49GrlCLXSbvh43MyWARAwIyl7goCYUoFgsQS9DXfjYmZVaGrosly5dgre3N0JDQ9GuXTssXrwYjo6OqFWrVoVfpyKdP38eVlZWOHXqFHr16qXo4ZRbQUEBMjIyyhVM0tLS8OzZs/faqFOnDho3blzyS09P74Mf161bFzkFxei0PFymS80EAK4vHwRtDXUZ9kIkPwwzRESk0m7evImpU6fizJkzcHFxwbp166Crq/ve836KuolV4QkQSwCBWvkvBNWEAqgLBfCyNYXD/w7tLEtcXBy8vLwQFhYGIyMjLF68GOPGjYO6unJeeH777bfYvXs3Hj16BDU1NYWNIz8/v9QQUtbHz58/f68NTU3NUkNIWUFFW1v7rRm78mI1M6KKUc53RyIiIil5XRFs586dmD9/Po4ePYoNGzbAycmp5GJ0c3QC1p38GxCqv3dA58cUiyUoFkvgcTgeGS/zMdPG6L3nxMbGwsvLC8ePH4eJiQn27t0Le3t7pQ0xwKty1sHBwRg5cqTUg0xeXl65g0l6ejpevHjxXhtaWlpvhRBjY2N88cUXZc6gyOu8nq5NNfDgycsPzvpVlppQABtjPam3S6RInJkhIiL6n9TUVMyZMwf79u1Dv379sHXrVlx6pgGPw/FS62PtKDPY/2+G5ty5c/Dy8kJERAQ6dOiAJUuW4KuvvlLoLIa0XL9+HWZmZjh27BiGDBnywefm5uaWO5ikp6cjK+v9U+w/+eSTcs2YvP6zllb5l/3JQ0JCAlatWoWg4zFo+vVmmfUT6W4NQ73K7+Miqm4YZoiIiN4RHh4ONzc3pL4sQlPXLSiWYvFPDXUhVvfSxtbvViAyMhKmpqZYsmQJxowZUyNCDADk5ORgyZIl2LZtGwIDA/Hs2bMP7jnJzn6/VHDdunXLDCWlfa66na9TXnfv3sXKlSsRGBiIJk2awMPDAxc0uyE26RmKxdK7RFMTCmDVVgd7XM2l1iZRdcAwQ0REVIqcnBzYeB/GY0ldCIRSXO4lESM36S80uXkAS5cuxahRoyAUVu+TErKzsz86Y/Lmxzk57+/5qFevXrk2wr/+VV3PzZGW27dvY+XKlfjll1/QrFkzeHh4YNKkSahTpw5SMnPQf2MM8qVYQllDXYhI994VKkRBpAyUdzEuERGRDP2bVYxUQUNUYg/3hwmE0GzTFQe/nwXjpvWk3PjHSSQSvHz5skJ7TnJzc99rp379+m8Fkc6dO5d8DADu7u5Ys2YNxo8fD11d3Wp/Ho683Lx5EytWrMD+/fvRvHlzbNq0CV9//fVb4a1lIy142ZpKdXmjt60pgwzVSAwzREREpQiMTS4pryxtakIB9l1MwXJb0yq3JZFIkJWVVe5gkpaWhvz8/Pfaadiw4VszI59//nmZS7t0dXU/eNbN999/Dw0NDUyfPh1163J/BvBqD9GKFStw4MABtGzZEn5+fpg4cWKZIc+huwEyXuZjXcTdKvc9f6BJyT4topqGYYaIiKgU0XfSZBJkgFcVzqLvpmE53g8zEokEL168KHN/SWkfFxQUvNdOo0aN3gohrVu3LnN5l66urlTPsDl8+DD69+/PIAMgPj4e3t7eOHjwIFq1agV/f39MmDChXAefzrQxgu4nGlgWegNF/6uKV16vS4J725oyyFCNxj0zRERE73iZXwQzGR9eCEgwLDcaT9MfvxdUCgsL33qmQCAoCSfl2XOiq6ursLLOaWlpaNq0KXbs2AFXV1eFjKE6uHr1Kry9vXH48GG0adMGixYtgpOTU7lCzLtSMnPgGRyP04kZH50trMphrUTKiDMzRERE73jwJFvGQQYABIiMvYqmGkVo3LgxDA0Ny6za1ahRI6U5cyY0NBQCgQC2traKHopCXLlyBd7e3ggJCUG7du2wc+dOjB8/vkozXy0baWGPqzkSUrMQGJuM6LtpSH6S89b3qACAgY4WbIz1MN7CgOWXSWVwZoaIiOgdV5Kfwm7rOZn349OnIcYNspJ5P/L05Zdf4uXLl4iJiVH0UOTq0qVL8PLywpEjR2BoaIjFixfD0dFRZiE0O78ISU+yUVAkRm11IVrraENbQzkCL5E0Ve9akERERApQW10+/z1OmzwJv/32m1z6kocXL14gMjISdnZ2ih6K3MTFxWHYsGHo1q0b7ty5g4CAANy6dQsuLi4ynU3T1lCHqX59dDFoCFP9+gwypLIYZoiIiN7RWkcb0q7I/C4BgP4Wn8HOzg4//vijjHuTj+PHj6OgoEAlwkxsbCyGDh2KHj164N69ewgMDMTNmzfh5OSkNEsCiWoChhkiIqJ3aGuow0DGG6f1tIQ4GBSIefPmYdasWZg9ezaKi4tl2qesBQcHo2vXrmjVqpWihyIz58+fx+DBg2FhYYEHDx7gl19+wfXr1zFu3DioqakpenhEKodhhoiIqBQ2JnpQE8pofkYixr0zodi0aRPWrl2LLVu2YNOmTRgzZgxycnJk06eM5efn49ixYzV2Vubs2bMYOHAgrKys8M8//2D//v2Ij4+Hg4MDQwyRAjHMEBERlcLR3EBm58xAIMToT/Uwe/ZsjB07Fs7Ozvjtt98QERGBPn36IDU1VTb9ytDvv/+OrKysGhdmTp06hX79+qFnz554/PgxDhw4gGvXruGrr76CUMjLKCJF408hERFRKYya1EUvQ12pz86oCQXoZaiLbd954cCBAwgLC4O5uTmMjIxw6tQppKSkwMLCArdu3ZJqv7IWHBwMIyMjdOzYUdFDkYo//vgDNjY26N27N548eYJDhw7hr7/+wpgxYxhiiKoR/jQSERGVwcfODOpSDjPqQgF87MwAAGPGjEFcXBwkEgm6d++OBw8eIDY2Ftra2rCyslKa8sbFxcX47bffYGdnB4FA1qUTZEcikSAqKgq9e/eGjY0Nnj9/juDgYFy+fBmjRo1iiCGqhvhTSUREVIaWjbTgZWsq1Ta9bU3fOpW9ffv2iI2NxeDBgzF69Ghs2bIFMTEx+PzzzzFgwAAEBgZKtX9ZOHfuHNLT05V2iZlEIkFkZCSsra3Rr18/ZGdnIzQ0FJcuXcLIkSMZYoiqMf50EhERfYBDdwPMG2gslbbmDzSBfXeD9z5ft25d7N+/H+vXr8f69evxn//8B7t27YKjoyPGjx+PlStXojqfcX348GE0a9YMPXr0UPRQKkQikSAiIgI9e/bEgAEDkJ+fj6NHjyIuLg7Dhw9X6lkmIlXBMENERPQRM22MsGaUGTTUhRXeQ6MmFEBDXYi1o8www8awzOcJBALMmTMHUVFRuHnzJiwtLTF58mR4e3tjyZIlmDRpEgoLC6v6UqROIpEgODhYqWYwJBIJTpw4ASsrKwwaNAjFxcU4duwYYmNj8eWXXzLEECkR5XjXISIiUjCH7gaIdO8Nq7Y6APDRUPP6cau2Ooh0713qjExprK2tcfnyZbRu3Rp9+vRBo0aNIBKJsGfPHgwdOhTPnz+v2guRsr/++gsPHjxQiiVmEokEYWFhsLCwwJAhQyAQCHDixAmcP3++5GMiUi4MM0REROXUspEW9ria4+RsaziZt0IrHS28e/krANBKRwtO5q0Q6W6NPa7mb+2RKQ99fX1ER0dj+vTpmDlzJk6ePImQkBDExcWhZ8+eSE5Oltprqqrg4GA0aNAAffr0UfRQyiSRSHDkyBH06NEDw4YNQ61atRAREYGzZ89i0KBBDDFESkwgqc6LcImIiKq57PwiJD3JRkGRGLXVhWitow1tDXWptR8UFARXV1e0a9cOa9euhZubGwoKChAWFoYuXbpIrZ/KMjMzQ5cuXRAQEKDoobxHIpEgNDQU3t7euHz5MqytrbFs2TLY2NgwwBDVEJyZISIiqgJtDXWY6tdHF4OGMNWvL9UgAwAODg64ePEi8vPzMXbsWHh5eaF58+bo1asXjh07JtW+KioxMRHXr1+vdkvMxGIxgoOD0bVrV4wcORL16tVDdHQ0YmJi0LdvXwYZohqEYYaIiKiaMzU1RVxcHPr27YsJEyagT58+6NevH4YPHw5/f3+FjSs4OBiampoYNGiQwsbwJrFYjEOHDqFLly4YNWoUGjVqhJiYGERHR1frZXBEVHkMM0REREqgXr16OHToEHx9fbFhwwZkZ2fD1dUVbm5u+PbbbyEWi+U+puDgYAwaNAhaWhXbEyRtYrEYv/76Kz777DOMGTMGenp6OH36NH7//XdYW1srdGxEJFsMM0REREpCIBBg/vz5OHnyJK5du4YTJ05g9uzZWLduHRwcHJCbmyu3sTx69Ajnz59X6BKz4uJiBAUFwczMDPb29tDX18fZs2dx8uRJ9OzZU2HjIiL5YZghIiJSMn379sXly5fRvHlz+Pn5YcqUKTh69Cj69++PjIwMuYzht99+g5qaGoYNGyaX/t5UXFyMffv2wczMDGPHjoWBgQHOnz+P8PBwWFlZyX08RKQ4DDNERERKqEWLFoiJicHkyZOxbds29O3bFwkJCbC0tERCQoLM+w8ODi45B0deioqKsHfvXpiamsLR0RFt27ZFbGwsjh8/DgsLC7mNg4iqD4YZIiIiJVW7dm1s3rwZAQEBiIqKgq6uLsRiMSwtLXH27FmZ9fvs2TNERUXJbYlZUVERAgIC0LFjRzg5OcHY2BhxcXE4evQoevToIZcxEFH1xDBDRESk5JycnHDhwgUUFBQgIyMD+vr66NevH3799VeZ9Hf06FEUFRVh5MiRMmn/tcLCQuzevRsdOnSAi4sLOnbsiEuXLiE0NBTdunWTad9EpBwYZoiIiGqATz/9FH/++Sd69+6N+Ph4GBkZwd7eHr6+vpD2+djBwcHo0aMHmjdvLtV2XyssLMTPP/+M9u3bY+LEiTAzM8OVK1cQEhKCrl27yqRPIlJODDNEREQ1RIMGDRASEoJVq1bh+vXraNeuHRYsWIDp06ejqKhIKn3k5ubixIkTMlliVlBQgB07dsDY2BiTJk1Cly5dcPXqVRw+fBidO3eWen9EpPwYZoiIiGoQoVAIT09PRERE4NmzZ9DR0cGOHTtga2uLrKysKrcfERGBnJwcjBo1SgqjfaWgoADbtm2DkZERpk6dih49eiA+Ph4HDx7Ep59+KrV+iKjmYZghIiKqgQYMGIDLly+jTZs2EAqFiI6OhrW1NR4+fFjuNrLzi3Dj4XNcSX6KGw+fIzu/CMHBwejYsSOMjY2rPMb8/Hxs3boVhoaGcHNzg5WVFeLj47F//3506tSpyu0TUc0nkEh7IS0RERFVG3l5eZg1axa2b98ObW1tNGjQAMePH4eZmVmpz09IzUJgbDKi76QhOTMHb14kCAAUP09FhwZibJ5lD6MmdSs9pp9++glr1qzBo0eP4ODggMWLF6NDhw6Vao+IVBfDDBERkQrYtWsXpk2bBjU1NaipqeHw4cMYMGBAyeMpmTnwDI7H6cQMqAkFKBaXfXkgFABiCdDLUBc+dmZo2UirXGPIzc3Fjh07sHbtWjx+/Bjjxo3D4sWLYWJiUuXXR0SqiWGGiIhIRVy+fBl2dnZ49OgRxGIxtm/fjq+//hpBcclYFnoDRWLJB0PMu9SEAqgLBfCyNYVDd4Myn5ebm4tt27bB19cXaWlpcHR0xKJFi6SyVI2IVBvDDBERkQrJzMzEuHHjEB4eDgCwXbgFVyWtqtzuvIHGmGlj9NbncnJy4O/vD19fX2RkZMDZ2Rmenp4wNDSscn9ERADDDBERkcoRi8Xw9vbG+uDz0Bn6X6m1u3aUGey7GyA7Oxtbt27Fd999h8zMTLi4uMDT0xNt27aVWl9ERADDDBERkUpKycxB3/XRKCiWQCAQSKVNDXUhxmjewvaNq/Hs2TNMnDgRCxcuRJs2baTSPhHRuxhmiIiIVJDTz7E4d/9JhfbIfIxEXIz85GsYWf8fLFy4EK1aVX35GhHRhzDMEBERqZiE1CwM+P6UzNqPdLeGoV7lyjYTEVUED80kIiJSMYGxyVATSmdp2bvUhALsvZAsk7aJiN7FMENERKRiou+kSXV52ZuKxRJE302TSdtERO9imCEiIlIhL/OLkJyZI9M+kp/kIDu/SKZ9EBEBDDNEREQq5cGTbMh6s6wEQNKTbBn3QkTEMENERKRSCorENaofIlJtDDNEREQqpLa6fP7rl1c/RKTa+E5DRESkQlrraEM2dcz+n+B//RARyRrDDBERkQrR1lCHQSMtmfZhoKMFbQ11mfZBRAQwzBAREakcGxM9mZ4zY2OsJ5O2iYjexTBDRESkYhzNDWR6zsx4CwOZtE1E9C6GGSIiIhVj1KQuehnqSn12Rk0oQC9DXRjq1ZVqu0REZWGYISIiUkE+dmZQl3KYURcK4GNnJtU2iYg+hGGGiIhIBbVspAUvW1Optulta4qWMi4uQET0JoYZIiIiFeXQ3QDzBhpLpa35A01g3517ZYhIvgQSiUQ2OwCJiIhIKQTFJWNZ6A0UiSUVKgygJhRAXSiAt60pgwwRKQTDDBERESElMweewfE4nZgBNaHgg6Hm9eO9DHXhY2fGpWVEpDAMM0RERFQiITULgbHJiL6bhuQnOXjzIkGAVwdi2hjrYbyFAauWEZHCMcwQERFRqbLzi5D0JBsFRWLUVheitY42tDXUFT0sIqISDDNERERERKSUWM2MiIiIiIiUEsMMEREREREpJYYZIiIiIiJSSgwzRERERESklBhmiIiIiIhIKTHMEBERERGRUmKYISIiIiIipcQwQ0RERERESolhhoiIiIiIlBLDDBERERERKSWGGSIiIiIiUkoMM0REREREpJQYZoiIiIiISCkxzBARERERkVJimCEiIiIiIqXEMENEREREREqJYYaIiIiIiJQSwwwRERERESklhhkiIiIiIlJKDDNERERERKSUGGaIiIiIiEgpMcwQEREREZFSYpghIiIiIiKlxDBDRERERERKiWGGiIiIiIiUEsMMEREREREpJYYZIiIiIiJSSgwzRERERESklBhmiIiIiIhIKTHMEBERERGRUmKYISIiIiIipcQwQ0RERERESolhhoiIiIiIlBLDDBERERERKSWGGSIiIiIiUkoMM0REREREpJQYZoiIiIiISCkxzBARERERkVJimCEiIiIiIqXEMENEREREREqJYYaIiIiIiJQSwwwRERERESklhhkiIiIiIlJKDDNERERERKSUGGaIiIiIiEgpMcwQEREREZFSYpghIiIiIiKl9H+Tp2V8J3wncQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Example: Visualize the first graph in the list\n",
        "graph = train_set[5]  # Replace with your data object\n",
        "G = to_networkx(graph,  to_undirected=True)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "nx.draw(G)\n",
        "plt.title(f\"Graph with y = {graph.y}\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_uGN1zsGyui"
      },
      "outputs": [],
      "source": [
        "class FourCycles(SymmetrySet):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.p = 4\n",
        "        self.hidden_units = 16\n",
        "        self.num_classes = 2\n",
        "        self.num_features = 1\n",
        "        self.num_nodes = 4 * self.p\n",
        "        self.graph_class = True\n",
        "\n",
        "    def gen_graph(self, p):\n",
        "        edge_index = None\n",
        "        for i in range(p):\n",
        "            e = torch.tensor([[i, p + i, 2 * p + i, 3 * p + i], [2 * p + i, 3 * p + i, i, p + i]], dtype=torch.long)\n",
        "            if edge_index is None:\n",
        "                edge_index = e\n",
        "            else:\n",
        "                edge_index = torch.cat([edge_index, e], dim=-1)\n",
        "        top = np.zeros((p * p,))\n",
        "        perm = np.random.permutation(range(p))\n",
        "        for i, t in enumerate(perm):\n",
        "            top[i * p + t] = 1\n",
        "        bottom = np.zeros((p * p,))\n",
        "        perm = np.random.permutation(range(p))\n",
        "        for i, t in enumerate(perm):\n",
        "            bottom[i * p + t] = 1\n",
        "        for i, bit in enumerate(top):\n",
        "            if bit:\n",
        "                e = torch.tensor([[i // p, p + i % p], [p + i % p, i // p]], dtype=torch.long)\n",
        "                edge_index = torch.cat([edge_index, e], dim=-1)\n",
        "        for i, bit in enumerate(bottom):\n",
        "            if bit:\n",
        "                e = torch.tensor([[2 * p + i // p, 3 * p + i % p], [3 * p + i % p, 2 * p + i // p]], dtype=torch.long)\n",
        "                edge_index = torch.cat([edge_index, e], dim=-1)\n",
        "        return Data(edge_index=edge_index, num_nodes=self.num_nodes), any(np.logical_and(top, bottom))\n",
        "\n",
        "    def makedata(self):\n",
        "        size = 25\n",
        "        p = self.p\n",
        "        trues = []\n",
        "        falses = []\n",
        "        while len(trues) < size or len(falses) < size:\n",
        "            data, label = self.gen_graph(p)\n",
        "            data = self.makefeatures(data)\n",
        "            data = self.addports(data)\n",
        "            data.y = int(label)\n",
        "            if label and len(trues) < size:\n",
        "                trues.append(data)\n",
        "            elif not label and len(falses) < size:\n",
        "                falses.append(data)\n",
        "        return trues + falses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8ognWc5G4wR",
        "outputId": "fd1cd338-b485-4f73-93af-73e521836829"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=1),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0),\n",
              " Data(edge_index=[2, 32], num_nodes=16, x=[16, 1], id=[16, 1], ports=[32, 1], y=0)]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fourcycles_dataset = FourCycles()\n",
        "train_set = fourcycles_dataset.makedata()\n",
        "train_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "4737x1PuHLB_",
        "outputId": "636a7b90-6e21-4415-95fd-db009cf00869"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApQAAAHzCAYAAACe1o1DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAr4ElEQVR4nO3df5RVd33o/c85czKTzIgYBgcTZaR1GPSOY81SLizvojqacJc/QjvRGGrwSdUarSaaRDCU1GCopUExpJrUX1WjCV34tDe01MflzY1rjHSloTxPTULolYFGOJhY5jITEphJh5yZ8/wRyU8GZth7D3POeb3WYgVy5nz33ix+vPmevb/fXLlcLgcAAJyi/Ok+AQAAKpugBAAgEUEJAEAighIAgEQEJQAAiQhKAAASEZQAACQiKAEASERQAgCQiKAEACARQQkAQCKCEgCARAQlAACJCEoAABIRlAAAJCIoAQBIRFACAJCIoAQAIBFBCQBAIoISAIBEBCUAAIkISgAAEhGUAAAkIigBAEhEUAIAkIigBAAgEUEJAEAighIAgEQEJQAAiQhKAAASEZQAACQiKAEASERQAgCQiKAEACARQQkAQCKCEgCARAQlAACJFE73CQATMzhcir39g3G0NBr1hXzMaW6Kpga/lQE4ffwtBBVg94HDsXFbMXp29UVxYCjKz3ktFxGtMxqja15LXLqgNebOmna6ThOAGpUrl8vlk38ZVJZqmcXbPzAUqzbviK17DkZdPhcjo2P/dj32+qK2mbG2uzNmz2icxDMFoJYJSqpGtc3ibdpejNVbdkZptHzCkHyhunwuCvlc3LCkI5bOb83wDAHgaYKSileNs3i39OyO9Xf1Jh5n+eL2uKJrbgpnBABjE5RUtGqcxdu0vRgr79yR2njrLuqMS6bYNQJQXQQlFasaZ/H2DwzF+RvuieHSaGpjNhTycffVb52ys7EAVD7rUFKRNm0vphKTERHr7+qNH2wvpjJWUqs274jSBGZax6M0Wo5Vm9Ob8QSAFxKUVJz9A0OxesvOVMe8fsvO2D8wlOqYE7X7wOHYuufghD66H4+R0XJs3XMw9vQdTnVcADhGUFJxqnUWb+O2YtTlc5mMXZfPxR33TY1ZWACqj6CkolTzLF7Prr7Ur+uYkdFy9PT2ZTI2AAhKKkq1zuIdGS5FMeOP3Iv9QzE4XMr0GADUpsrbOoSaNhmzeJ+PjlTHfeqpp+LIkSNx5MiROHz48HH/+/BjR6Mcr0n1uC9Ujoi9/YPRce70TI8DQO0RlFSMyZrF63vs8Sgf/c8x42+i/x0eHj7hMfP5fLzsNW+Mae9dk+m1RUQcTXE5IgA4RlBSMfb1D0bWi6aWI+JVrz0vnur75Zhfc9ZZZ8VLXvKSmDZt2vP+O3369HjlK1/5ov9/sv+eeeaZ8W+/fiLe/dV/yvjqIuoL7nIBIH2CkooxWbNra76wNv7LrMbjBmBTU1MUCun/tpnT3BS5iEyDOfeb4wBA2gQlFWOyZtcufPc7J/0+w6aGQrTOaIx9GX6k39rcGE0NfssDkD6ff1Exjs3iZel0zuJ1zWvJ9An2rvaWTMYGAEFJxTg2i5el0zmLd+mC1kyfYH9D4+OZjA0AgpKK8rZ5L8/sF+3pnsWbO2taLGqbmfosZT7KUdfXG++94L/F+973vti1a1eq4wOAoKQijI6Oxt/93d/F33/pM5HVozkjo+VYtrA1o9HHZ213ZxRSDsozCnXxkxv/KG677bbYvn17dHR0xOWXXx6PPPJIqscBoHYJSqa0UqkUt99+e7z+9a+Piy++OF7eMBIdzfnUZ/Hq8rlY1DYz2lqmpTruRM2e0Rg3LEl3YfU1SzpizsunxWWXXRa7du2KL33pS3HnnXdGW1tbXHvttfHYY4+lerzB4VLsfPTx+Hnxsdj56ON25wGoAblyuZz10n4wYcPDw3HbbbfFunXr4pe//GVceOGFsWrVqli4cGHsHxiK8zfcE8MpLiPUUMjH3Ve/NWZnfI/meN3SszvW39WbeJwVi+fFJ7vaXvT/n3jiiVi/fn3cdNNNccYZZ8S1114bn/rUp6Kx8dSuf/eBw7FxWzF6dvVFcWDoecsf5SKidUZjdM1riUsXtMbcWac32gFIn6BkQgaHS7G3fzCOlkajvpCPOc1NqT7EMjg4GN/85jdj/fr18etf/zouvvjiWLVqVfzO7/zO875u0/ZirLxzR2rHXXdRZ1wy//R+3P1Cm7YXY/WWnVEaLU/oYZ26fC4K+VysWdJx0ms6cOBA/Nmf/Vl84xvfiJaWlli9enV86EMfijPOOGNcx9o/MBSrNu+IrXsORl0+d8LzPPb6oraZsba7c8rEOwDJCUpOajJmnx5//PG49dZbY8OGDfHYY4/FBz/4wVi5cmXMmzdvzPdkPYs3FUxWsP37v/97XH/99fE3f/M30d7eHl/4whfife97X+RyY99akDR4b1jSEUunWMQDcGoEJWOajJg5ePBg3HzzzXHLLbfEf/7nf8aHP/zh+OxnPxtz5swZ1/snYxZvKngm6nv7oth/nKhvboyu9pZYtrA10X2g999/f/zJn/xJ/PjHP443v/nNceONN8Y73vGOF31dWjG/fHF7XNE1N/E4AJxegpLjynr26dFHH40vf/nL8fWvfz0iIv74j/84PvOZz8Q555wz4XOttY9ds77tICLipz/9aaxcuTK2bdsW559/ftx4443xpje9KSJq43YDACZGUPIiWc4+7d27N9atWxff+c534qyzzoorr7wyPv3pT8fMmTMTH2+yZvFqRblcjr//+7+PVatWxS9+8Yt4//vfH5+8dnV8dPO+qn4gCoCJE5Q8T1azT7/4xS/iL/7iL2Ljxo1x9tlnxzXXXBOf+MQnYvr0bPbMnoxZvFpRKpXi+9//fqxevTqe+m8fi7Pm/E6Uc+mtOFaXz8Vbfrs5bv/IgtTGBGByCUqekcVyPPV1Ea97+H/EP266Lc4555xYsWJFfPSjH42mptOzXzanbkfxYFz4tW2ZjX/31b9r5higQlnYnGes2rwjSinvJT18tBQPNPyX+PrXvx4PP/xwXHXVVWKyQv2P+w+kvqD8MXX5XNxxXzGTsQHInqAkIp6+/3DrnoMTegBnPHJ1hYhXvC7e/vt/EA0NDamOzeTq2dWX+q+PY0ZGy9HT25fJ2ABkT1ASEREbtxXNPjGmI8OlKA4MZXqMYv+QbRoBKpSgJCLMPnFi+/oHI+ubrcsRsbd/MOOjAJAFQYnZJ07qaIoPak2F4wCQLkGJ2SdOqr4wOX9UTNZxAEiXP70x+8RJzWluimzusH1W7jfHAaDyCErMPnFSTQ2FaM14J5vW5kaLzwNUKH/DY/aJcema15LpSgBd7S2ZjA1A9gQlZp8Yl0sXtGa6EsCyha2ZjA1A9gQlEWH2iZObO2taLGqbmfqvk7p8Lha1zbTtIkAFE5REhNknxmdtd2cUUg7KQj4Xa7s7Ux0TgMklKIkIs0+Mz+wZjXHDko5Ux1yzpCNmZ3zLBQDZEpQ8w+wT47F0fmssX9yeylgrFs+LS+abvQaodIKSZ5h9Yryu6JobN17UGQ2F/IRntevyuWgo5GPdRZ3xya62jM4QgMmUK5fLWW+SQoW5pWd3rL+rN/E4KxbPEwxVbv/AUKzavCO27jkYdfncCe/DPfb6oraZsba70z80AKqIoOS4Nm0vxuotO6M0Wp7Qwzp1+VwU8rlYs6TDR5k1ZPeBw7FxWzF6evti38HBiNyzs5a5eHrZqK72lli2sNX9tABVSFAyJrNPnIoZLefERz69Mj7wwf8r6gv5mNPcZA1SgCrnT3nGNHtGY9z+kQXPm30q9g/Fc7PS7BPPVS6X44mB/xOvaW6I81rPPt2nA8AkEZSc1NxZ0+LzSzri89ERg8OlWL3+lvj+HX8TPT/5X2afeJ6hoaEYGRmJl770paf7VACYREqACWlqKMSs+qfi6K97o+Pc6af7dJhinnjiiYiImD7drw2AWmLZICasUCjEyMjI6T4NpqDHH388IsIMJUCNEZRMWF1dXZRKpdN9GkxBx4LSDCVAbRGUTJgZSsZy7CNvM5QAtUVQMmFmKBmLGUqA2iQombBCoRDlcjlGR0dP96kwhQwOl6L3/wxF/Tnt8avBp38MQG3wlDcTMjhcir6n6qP+nPbY8avHom3WdMsG1bBn1ijd1RfFgaEoR3Occ9lNceGt9z69RumMxuia1xKXLmiNubOsUQpQreyUw0m9OBqeJRpqk12UAHguQcmYRAPHk3Sf9xuWdMRS+7wDVBVByXGJBo7nlp7dsf6u3sTjLF/cHld0zU3hjACYCgQlLyIaOJ5N24ux8s4dqY237qLOuMQ/OgCqgqe8eZ5N24upxGRExPq7euMH24upjMXptX9gKFZv2ZnqmNdv2Rn7B4ZSHROA00NQ8gzRwFhWbd4RpQnc+jAepdFyrNqc3ownAKePoOQZooHj2X3gcGzdc3BC99KOx8hoObbuORh7+g6nOi4Ak09QEhGigbFt3FaMunwuk7Hr8rm44z63RQBUOkFJRIgGxtazqy/1f2gcMzJajp7evkzGBmDyCEoiQjRwfEeGS1HM+B7YYv+QbRoBKpygRDQwpn39g5H1umLliNjbP5jxUQDIkqBENDCmo6XRqjoOANkQlIgGxlRfmJw/IibrOABkw5/iiAbGNKe5KbJ5VOtZud8cB4DK5W94RANjamooROuMxkyP0drcGE0NhUyPAUC2BCWigRPqmteS6ZJSXe0tmYwNwOQRlESEaGBsH/ivszNdUmrZwtZMxgZg8ghKIiLi0gWtmUbDpQtEQyX61a9+FZ/6w/fHk7/818iV032oqi6fi0VtM6OtZVqq4wIw+QQlERExd9a0WNQ2M/1ZyvJoPPnLf42PXnJhPPjgg+mOTWbK5XJ8//vfj9e//vWxY8eOWP/+N0X9GeneslDI52Jtd2eqYwJweghKnrG2uzMKKQdlwxmF2PAH/zX+4z/+I84777y48sor47HHHkv1GKTrwIED0d3dHZdddllceOGF8dBDD8UHL3pX3LCkI9XjrFnSEbMzvncXgMkhKHnG7BmNmUTDB37vv8eDDz4Y69ati9tuuy3a29vjW9/6VoyMjKR6LJL727/92+jo6Ih777037rzzzrj99tvj7LPPjoiIpfNbY/ni9lSOs2LxvLhkvtsgAKqFoOR5soqG+vr6WL58efT29sY73/nOuPzyy2PBggVx3333pXIskunv74+lS5fG+9///njb294WO3fujO7u7hd93RVdc+PGizqjoZCf8O0RdflcNBTyse6izvhkV1tapw7AFJArl8tZ77pHBdq0vRirt+yM0mh5Qg/r1OVzUcjnYs2SjhPOQN17771xxRVXxM9//vO47LLL4sYbb4xXvOIVaZw6E7Rly5a4/PLL4+jRo3HrrbfG0qVLI5c7cSzuHxiKVZt3xNY9B6Munzvhr5Fjry9qmxlruzt9zA1QhQQlY8o6GkZGRuKv//qv47rrroujR4/G5z//+bjyyivjjDPOSHTeg8Ol2Ns/GEdLo1FfyMec5qaqWgMzres7dOhQXHXVVfG9730v3vOe98Q3v/nNOOeccyY0xu4Dh2PjtmL09PZFsX/oeXvC5+Lp9Ue72lti2cJWT3MDVDFByUllHQ0DAwPxuc99Lr7+9a9He3t7fOUrX4kLLrjg1M5xV18UB45zjjMao2teS1y6oDXmzqq8sEn7+u666674yEc+Ek888UTcfPPN8Yd/+IcnnZU8mWoPeQDGJiiZkCyj4YEHHogrr7wytm7dGt3d3XHTTTfFnDlzTvieav/oNe3rO3z4cKxYsSK+8Y1vxPnnnx/f/va3o7XVwzEAJCMomVLK5XJs2rQpli9fHgMDA3HttdfGtddeG2edddaLvjbpfZ43LOmIpVP4SeO0r++ee+6JD33oQ9HX1xdf+tKX4uMf/3jiWUkAiBCUTFFHjhyJP//zP48vf/nLce6558ZNN90U3d3dzwTQLT27Y/1dvYmPs3xxe1zRNTfxOGlL8/o+vOCVsWrVqvjLv/zLWLRoUXz3u9+N17zmNSmcJQA8TVAype3evTuuuuqq+NGPfhTnn39+fOUrX4kHjjTFyjt3pHaMdRd1Tqk1ETdtL6Z6fYX/7wfxyNa/jbVr18anP/3pyOetFgZAugQlFeGHP/xhXHXVVfGrQ0/GuX/0tRjN1aU2dkMhH3df/dYpcU/l/oGhOH/DPTFcSmff7HK5HLnRUtx28WvibfNtcwhANkxVUBHe8573xEMPPRRv+vj6GEn5n0Cl0XKs2pzejGASqzbviNIE7pc8mVwuF3Vn1Me3HxxKbUwAeCFBScXY//hT8cjISyOXT292MiJiZLQcW/ccjD19h1Mdd6J2HzgcW/ccnNADOOMxVa4PgOolKKkYG7cVJ7zd33jV5XNxx33FTMYer2q/PgCql6CkYvTs6kt99u6YkdFy9PT2ZTL2eFX79QFQvQQlFeHIcCmKA9neB1jsH4rB4VKmxxhLtV8fANXNvmhUhH39g5H1cgTliFj0rovizCcPRl1dXeTz+airq0vt+yd6/bFoinK0Z359e/sHo+Pc6ZkeB4DaIyipCEdTWkbnZNraXxsvGT4YIyMjMTo6GiMjI8f9/lNPPXXSrzne98d6rTzj1dH4+6szv77J+nkEoLYISipCfWFy7s5Y/bnrTssM3s5HH493f/WfMj/OZP08AlBb/O1CRZjT3BRZ7zqd+81xTodqvz4AqpugpCI0NRSiNeOdbFqbG6Op4fRM2lf79QFQ3QQlFaNrXkum6zR2tbdkMvZ4Vfv1AVC9BCUV49IFrZmu07hsYWsmY49XtV8fANVLUFIx5s6aFovaZqY+i1eXz8WitpnR1jIt1XEnqtqvD4DqJSipKGu7O6OQcnAV8rlY292Z6pinqtqvD4DqJCipKLNnNMYNSzpSHXPNko6YnfEDMeNV7dcHQHUSlFScpfNbY/nidHaVWbF4Xlwyf2rdW1jt1wdA9cmVy+Wsd7SDTGzaXozVW3ZGabQ8oYdZ6vK5KORzsWZJx5SOrWq/PgCqh6Ckou0fGIpVm3fE1j0Hoy6fO2F4HXt9UdvMWNvdWREfA1f79QFQHQQlVWH3gcOxcVsxenr7otg/FM/9RZ2Lpxf17mpviWULWyvyaedqvz4AKpugpOoMDpdib/9gHC2NRn0hH3Oam6pqh5hqvz4AKo+gBAAgEU95AwCQiKAEACARQQkAQCKCEgCARAQlAACJCEoAABIRlAAAJCIoAQBIRFACAJCIoAQAIBFBCQBAIoISAIBEBCUAAIkISgAAEhGUAAAkIigBAEhEUAIAkIigBAAgEUEJAEAighIAgEQEJQAAiQhKAAASEZQAACQiKAEASERQAgCQiKAEACARQQkAQCKCEgCARAQlAACJCEoAABIRlAAAJCIoAQBIRFACAJCIoAQAIBFBCQBAIoISAIBEBCUAAIkISgAAEhGUAAAkIigBAEhEUAIAkIigBAAgEUEJAEAighIAgEQEJQAAiQhKAAASEZQAACQiKAEASERQAgCQiKAEACARQQkAQCKCEgCARAQlAACJCEoAABIRlAAAJCIoAQBIRFACAJCIoAQAIBFBCQBAIoISAIBEBCUAAIkISgAAEhGUAAAkIigBAEhEUAIAkIigBAAgEUEJAEAighIAgEQEJQAAiQhKAAASEZQAACQiKAEASERQAgCQiKAEACARQQkAQCKCEgCARAQlAACJCEoAABIRlAAAJCIoAQBIRFACAJCIoAQAIBFBCQBAIoISAIBEBCUAAIkISgAAEhGUAAAkIigBAEhEUAIAkIigBAAgEUEJAEAighIAgEQEJQAAiQhKAAASEZQAACQiKAEASERQAgCQiKAEACARQQkAQCKCEgCARAQlAACJCEoAABIRlAAAJCIoAQBIRFACAJCIoAQAIBFBCQBAIoISAIBEBCUAAIkISgAAEhGUAAAkIigBAEhEUAIAkIigBAAgEUEJAEAighIAgEQEJQAAiQhKAAASEZQAACQiKAEASERQAgCQiKAEACARQQkAQCKCEgCARAQlAACJCEoAABIRlAAAJCIoAQBIRFACAJCIoAQAIBFBCQBAIoISAIBEBCUAAIkISgAAEhGUAAAkIigBAEhEUAIAkIigBAAgEUEJAEAighIAgEQEJQAAiQhKAAASEZQAACQiKAEASERQAgCQiKAEACARQQkAQCKCEgCARAQlAACJCEoAABIRlAAAJCIoAQBIRFACAJCIoAQAIBFBCQBAIoISAIBEBCUAAIkISgAAEhGUAAAkIigBAEhEUAIAkIigBAAgEUEJAEAighIAgEQEJQAAiQhKAAASEZQAACQiKAEASERQAgCQiKAEACARQQkAQCKCEgCARAQlAACJCEoAABIRlAAAJCIoAQBIRFACAJCIoAQAIBFBCQBAIoISAIBEBCUAAIkISgAAEhGUAAAkIigBAEikcLpPAADgVAwOl2Jv/2AcLY1GfSEfc5qboqlB2pwOftYBgIqx+8Dh2LitGD27+qI4MBTl57yWi4jWGY3RNa8lLl3QGnNnTTtdp1lzcuVyuXzyLwMAOH32DwzFqs07Yuueg1GXz8XI6Nj5cuz1RW0zY213Z8ye0TiJZ1qbBCUAMKVt2l6M1Vt2Rmm0fMKQfKG6fC4K+VzcsKQjls5vzfAMEZQAwJR1S8/uWH9Xb+Jxli9ujyu65qZwRhyPp7wBgClp0/ZiKjEZEbH+rt74wfZiKmPxYoISAJhy9g8MxeotO1Md8/otO2P/wFCqY/I0QQkATDmrNu+I0gTulxyP0mg5Vm3ekeqYPE1QAgBTyu4Dh2PrnoMTegBnPEZGy7F1z8HY03c41XERlADAFLNxWzHq8rlMxq7L5+KO+9xLmTZBCQBMKT27+lKfnTxmZLQcPb19mYxdywQlADBlHBkuRTHjB2eK/UMxOFzK9Bi1RlACAFPGvv7ByHqB7HJE7O0fzPgotUVQAgBTxtHSaFUdp1YISgBgyqgvTE6aTNZxaoWfTQBgypjT3BTZPN/9rNxvjkN6BCUAMGU0NRSidUZjpsdobW6MpoZCpseoNYISAJhSuua1ZLoOZVd7SyZj1zJ5DgAkNjhcir39g3G0NBr1hXzMaW465VnAC377zLjtn7Nbh3LZwtbE46R5vdUgVy6Xs346HwCoQrsPHI6N24rRs6svigNDz1vuJxcRrTMao2teS1y6oDXmzpp20vFKpVL81V/9Vfzpn/5pTP+96+KMV3XEaJp3VI6OxJP7Hoh35P93fPGLX4xXvepVE3p72tdbTQQlADAh+weGYtXmHbF1z8Goy+dOuKvNsdcXtc2Mtd2dMXuM+yP/5V/+JT7+8Y/H/fffHx/72Mfik5+9Pt77nftjOMXlfRoK+fjY7L5Y97nPxuDgYFx33XVxzTXXRENDwwnfl8X1Vhv3UAIA47ZpezHO33BP3Ptwf0TESbdIPPb6vQ/3x/kb7olN25+/j/ahQ4fiE5/4RCxcuDDK5XL88z//c3zta1+L1//WOXHDko5Uz33Nko645vLLore3Ny6//PL43Oc+Fx0dHfHDH/5wzPekfb3VSlACAONyS8/uWHnnjhgujU54r+2R0XIMl0Zj5Z074pae3VEul2Pjxo3x2te+Nu64447YsGFDbN++PRYsWPDMe5bOb43li9tTOfcVi+fFJfOfvndy+vTpcdNNN8WDDz4Yc+bMiQsvvDDe/e53R29v7/Pek+b1VjsfeQMAJ7VpezFW3rkjtfFe/vBd8f/+31+Jiy++ODZs2BCvfOUrT3js1Vt2Rmm0PKGwq8vnopDPxZolHc/E5AuVy+XYvHlzXHPNNfHoo4/GNddcE9ddd138P794LNXrXXdR55jnUA0EJQBwQvsHhuL8Dfekdj9juVyO3MhT8YW3nBnLut857nPI8j7GJ598Mr74xS/GjTfeGDNa58ZZ710bpXJ6DwQ1FPJx99Vvrdp7KgUlAHBCH/z2trj34f4Jf+x7InX5iLf89sy4/SMLTv7Fz/HMk9a9fVHsP86T1s2N0dXeEssWtkZby8SftN63b1+858v/M544c1bk6tJbBqgun4u3/HbzhK+3UghKAGBMuw8cjgtu/llm49999e+eUvhFZLMW5FS+3qnMQzkAwJg2bitmumvNHfed+lPQTQ2F6Dh3epzXenZ0nDs9lYXFp/L1TmWCEgAYU8+uvlQ/6n6ukdFy9PT2ZTL2qaq1602LoAQAjuvIcCmKA0OZHqPYPxSDw6VMjzFetXa9aRKUAMBx7esfjKwftChHxN7+wYyPMj61dr1pEpQAwHEdTXHbw6lwnJOptetNk6AEAI6rvjA5mTBZxzmZWrveNFXfFQEAqZjT3BTZPO/8rNxvjjMV1Nr1pklQAgDH1dRQiNaMd3ZpbW5MZbmfNNTa9aZJUAIAY+qa15Lpuoxd7S2ZjH2qau160yIoAYAxXbqgNdN1GZctbD3l9w8Ol2Lno4/Hz4uPxc5HH09lOZ5L3vzKKXu9U1n1zbkCAKmZO2taLGqbmfpe3uXRkXjqVw/F5u/9W3z605+O+vr6cb3vmb28d/VFceA4e3nPaIyueS1x6YLWmDtr/Fsclsvl+PGPfxwrVqyIJ19/SZw1540RufTm3Y7t5V2N2y5GmKEEAE5ibXdnFFL+GPjM+jPi3S1PxMqVK+ONb3xj9PT0nPDr9w8MxQe/vS0uuPlncfu2fbHvBTEZ8fQaj/sGhuL2bfvigpt/Fh/89rbYP46Fyh944IFYvHhxvOtd74rm5ub4zh8vjoYz0p1zK+Rzsba7M9UxpxJBCQCc0OwZjXHDko5Ux1yzpCO+teHG+PnPfx7Nzc3x9re/PT7wgQ/Eo48++qKv3bS9GOdvuCfufbg/IuKkM6XHXr/34f44f8M9sWn78ffPfuSRR+LDH/5wnHfeeVEsFuMf/uEf4qc//Wm8+60LMrne2Rk/8HM6CUoA4KSWzm+N5YvbUxlrxeJ5ccn8p+8lfMMb3hA/+9nP4nvf+1785Cc/iXnz5sVNN90UTz31VERE3NKzO1beuSOGS6MT/sh9ZLQcw6XRWHnnjrilZ/cz///w4cNx/fXXx9y5c+Mf//Ef46tf/Wo89NBDsWTJksjlcpleb7XKlcvlrHcZAgCqxKbtxVi9ZWeURssTCry6fC4K+VysWdIxZlwdOnQorr/++rj11lvjda97XVy88ub47s7htE491v5+Rxx54K64/vrr49ChQ3H11VfHypUrY/r06WO+J8vrrSaCEgCYkP0DQ7Fq847Yuudg1OVzJwytY68vapsZa7s7x/Wx7/333x8f+8yq+PV5fxT5Qn1ELqX7N0eeike++fFYeuEF8YUvfCFe/epXj+ttWV9vNRCUAMApeeaJ696+KPYf54nr5sboam+JZQtbJ/x087Jv3xf37jkYo2nuXVMejTfMOjO2XH3BKb09y+utdIISAEhscLgUe/sH42hpNOoL+ZjT3HTKO8LsPnA4Lrj5Zymf4bPuvvp3EwdfmtdbDWr3ygGA1DQ1FKLj3LHvRZyIjduKJ/1o+VTV5XNxx33F+HzCp7jTvN5q4ClvAGBK6dnVl+luNT29fZmMXcsEJQAwZRwZLkVxHIuRJ1HsH0plm0aeJSgBgCljX//gi3bASVs5Ivb2D2Z8lNoiKAGAKeNoabSqjlMrBCUAMGXUFyYnTSbrOLXCzyYAMGXMaW5Kc+XJ48r95jikR1ACAFNGU0MhWjPeXaa1ubGm14zMgqAEAKaUrnktUZfPZp6yLp+LrvaWTMauZYISAJhSLl3Qmuk6lMsWtmYydi0TlADAlDJ31rRY1DYz9VnKunwuFrXNrLl9tieDoAQAppy13Z1RSDkoC/lcrO3uTHVMniYoAYApZ/aMxrgh4X7bL7RmSUfMzviBn1olKAGAKWnp/NZYvrg9lbFWLJ4Xl8x372RWcuVyOesdjgAATtmm7cVYvWVnlEbLE3pYpy6fi0I+F2uWdIjJjAlKAGDK2z8wFKs274itew5GXT53wrA89vqitpmxtrvTx9yTQFACABVj94HDsXFbMXp6+6LYPxTPjZhcPL1oeVd7Syxb2Opp7kkkKAGAijQ4XIq9/YNxtDQa9YV8zGlusgPOaSIoAQBIxFPeAAAkIigBAEhEUAIAkIigBAAgEUEJAEAighIAgEQEJQAAiQhKAAASEZQAACQiKAEASERQAgCQiKAEACARQQkAQCKF030CU8ngcCn29g/G0dJo1BfyMae5KZoa/BQBAJxIzdfS7gOHY+O2YvTs6oviwFCUn/NaLiJaZzRG17yWuHRBa8ydNe10nSYAwJSVK5fL5ZN/WfXZPzAUqzbviK17DkZdPhcjo2P/NBx7fVHbzFjb3RmzZzRO4pk+ywwqADAV1WRQbtpejNVbdkZptHzCkHyhunwuCvlc3LCkI5bOb83wDJ9lBhUAmOpqLihv6dkd6+/qTTzO8sXtcUXX3BTO6PgqcQYVAKhNNRWUm7YXY+WdO1Ibb91FnXFJBjOVlTSDCgBQM0G5f2Aozt9wTwyXRlMbs6GQj7uvfmuqM4KVMoMKAHBMzaxDuWrzjihNYLZvPEqj5Vi1Ob0Zz03bi6nEZETE+rt64wfbi6mMBQBwIjURlLsPHI6tew5O6OPj8RgZLcfWPQdjT9/hxGPtHxiK1Vt2pnBWz7p+y87YPzCU6pgAAC9UE0G5cVsx6vK5TMauy+fijvuSzwRWwgwqAMDx1ERQ9uzqS3128piR0XL09PYlGqMSZlABAMZS9UF5ZLgUxYw/9i32D8XgcOmU318JM6gAAGOp+qDc1z8YWT/GXo6In/zLg/Hoo4/Gk08+GRN9cH6qz6ACAJxI1e/bdzTFZYJO5OJL/iCO/vrpJ7Tr6+vj7LPPft63l73sZcf98ZnTXhb7JmkG1TaNAEAWqr4w6guTMwn73W9/K6aPHo5Dhw7FY4899rxvhw4dikceeSQeeuihZ358+PDT9zWe0fJbce6Hv5rpuZUjYm//YHScOz3T4wAAtanqg3JOc1PkIjL92DsXEb/39rdMaAawVCrFoUOH4t5dj8anfrg/u5P7jcmaqQUAak/V30PZ1FCI1oz3tm5tbpzwx8mFQiFmzpwZr/mtV2d0Vs83WTO1AEDtqYnK6JrXkulT1F3tLaf8/mMzqFnK/eY4AABZqImgvHRBa6ZPUS9b2HrK75+qM6gAAONVE0E5d9a0WNQ2M/VZyrp8Lha1zYy2lmmJxpnKM6gAACdTE0EZEbG2uzMKKUdbIZ+Ltd2diceZyjOoAAAnUzNBOXtGY9ywpCPVMdcs6YjZKXxcPdVnUAEATqRmgjIiYun81li+uD2VsVYsnheXzE9v5m8qz6ACAJxITQVlRMQVXXPjxos6o6GQn/CMYF0+Fw2FfKy7qDM+2dWW6nlN5RlUAIATyZUnuvF0ldg/MBSrNu+IrXsORl0+d8J7GI+9vqhtZqzt7sw00m7p2R3r7+pNPM6KxfNSj14AgOOp2aA8ZveBw7FxWzHueuhX8cgTT0Uu9+ysZS6eXnKnq70lli1snbR7ETdtL8bqLTujNFqe0MM6dflcFPK5WLOkI9WP4wEATqTmg/KYH/3oR/Ge339v3L3tgTi7+eVRX8jHnOam07Z+41SdQQUAeKGaX+16cLgUe/sH45/+96/irJbWePO8V8dLGxtO92nF7BmNcftHFsTd//Yf8eX/1Ru7+45E6ThRWcjnYm7LS2L54vZ4x+tecRrOFACodTU5Q3nsY+6eXX1RHBiK5/4E5CKidUZjdM1riUsXtMbcWadnyR0zlABApaipoKyUSEt6D+UNSzpiqXsoAYBJUjNBWSmRltZT3ssXt8cVXXNTOCMAgBOriaCslEjbtL0YK+/ckdp46y7q9LQ3AJC5ql/YfNP2YioxGRGx/q7e+MH2YipjvdD+gaFYvWVnqmNev2Vn7B8YSnVMAIAXquqgrKRIW7V5x3Gf4k6iNFqOVZvTm/EEADieqg7KSom03QcOx9Y9Byd0b+d4jIyWY+ueg7Gn73Cq4wIAPFfVBmUlRdrGbcUJ7ys+XnX5XNxxXzYf0wMARFRxUFZSpPXs6ks9fI8ZGS1HT29fJmMDAERUcVBWSqQdGS5FMeMHZ4r9QzE4XMr0GABA7arKrRcnM9Imutf36OhoHD16NIaHh+Po0aPx0COHIut1m8oRsbd/MDrOnZ7xkQCAWlSVQbmvf3BSIq37g5dH3eFfPy8Qj30b68cjIyPPG6f+nPY457KbMj7biKOl0cyPAQDUpqoMysmKp5HIxbTGxnjZy14WDQ0NUV9f/8y3E/34ud8/cPSMWPuv2a8tX1+o2rsbAIDTrCqDcrLi6Ss335T4Y+TB4VL8xb/+z0xnVHMRMae5KcMjAAC1rCqnreY0N0U2z3c/K61Ia2ooROuMxuQndAKtzY0TvtcTAGC8qjIoKy3Suua1ZLrEUVd7SyZjAwBEVGlQRlRWpF26oDXTJY6WLWzNZGwAgIgqDspKirS5s6bForaZqQdwXT4Xi9pmRlvLtFTHBQB4rqoNykqLtLXdnVFI+VwL+Vys7e5MdUwAgBeq2qCMqKxImz2jMW5Y0pHqmGuWdMTsjO8lBQCo6qCstEhbOr81li9uT2WsFYvnxSXz3TsJAGQvVy6Xs19V+zS7pWd3rL+rN/E4KxbPi092taVwRie2aXsxVm/ZGaXR8oTuA63L56KQz8WaJR1iEgCYNDURlBGVF2n7B4Zi1eYdsXXPwajL5054zsdeX9Q2M9Z2d/qYGwCYVDUTlBGVGWm7DxyOjduK0dPbF8X+oeftqJOLp9fD7GpviWULWz3NDQCcFjUVlMdUaqQNDpdib/9gHC2NRn0hH3Oam+yAAwCcdjUZlM8l0gAAkqn5oAQAIJmqXjYIAIDsCUoAABIRlAAAJCIoAQBIRFACAJCIoAQAIBFBCQBAIoISAIBEBCUAAIkISgAAEhGUAAAkIigBAEhEUAIAkIigBAAgEUEJAEAighIAgEQEJQAAiQhKAAASEZQAACQiKAEASERQAgCQiKAEACARQQkAQCKCEgCARAQlAACJCEoAABIRlAAAJCIoAQBIRFACAJCIoAQAIBFBCQBAIoISAIBEBCUAAIkISgAAEhGUAAAkIigBAEhEUAIAkMj/D36FiW5DdOjdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "graph = train_set[6]\n",
        "G = to_networkx(graph,to_undirected=True)\n",
        "nx.draw(G)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}